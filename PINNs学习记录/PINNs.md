py

# è€¦åˆPINNã€æ­£åé—®é¢˜ã€3Dé—®é¢˜çš„å­¦ä¹ ç ”ç©¶



## è®°å½•æ—¶é—´ï¼š2022-06-29 

### ä»€ä¹ˆæ˜¯PINNï¼Ÿ

Raissiç­‰äººåœ¨2018å¹´ 

 [Physics-informed neural networks: A deep learning 
framework for solving forward and inverse problems involving 
nonlinear partial differential equations](https://github.com/maziarraissi/PINNs)

ä¸­æå‡ºäº†PINNï¼Œé€šè¿‡åœ¨æŸè€—å‡½æ•°ä¸­ç»“åˆç‰©ç†åœºï¼ˆå³åå¾®åˆ†æ–¹ç¨‹ï¼‰å’Œè¾¹ç•Œæ¡ä»¶æ¥è§£å†³åå¾®åˆ†æ–¹ç¨‹ã€‚æŸå¤±æ˜¯åå¾®åˆ†æ–¹ç¨‹çš„å‡æ–¹è¯¯å·®å’Œåœ¨åˆ†å¸ƒåœ¨åŸŸä¸­çš„â€œæ­é…ç‚¹â€ä¸Šæµ‹é‡çš„è¾¹ç•Œæ®‹å·®ã€‚



ä»–ä»¬åœ¨æ–‡ç« ä¸­åˆ—å‡ºäº†æ•°ä¸ªç»å…¸pdeç®—ä¾‹æ¥éªŒè¯PINNçš„æ­£ç¡®æ€§ã€‚åœ¨ä»–ä»¬çš„githubä¸»é¡µä¸Šæä¾›äº†æºç ï¼Œä½¿ç”¨äº†Tensorflow1.xçš„æ¡†æ¶ã€‚

æˆªæ­¢åˆ°2022-06-29ï¼ŒTensorFlowæ›´æ–°åˆ°2.9ï¼ŒTensorFlow1.xä»£ç é£æ ¼è·ŸTensorFlow2.xç›¸å·®ç”šè¿œï¼Œè€Œä¸”Raissiç­‰äººæä¾›çš„ä»£ç ä¹Ÿæœ‰è®¸å¤šå€¼å¾—ä¼˜åŒ–çš„åœ°æ–¹ã€‚

æˆ‘åœ¨githubä¸Šæ‰¾åˆ°ä¸€ä¸ªäº†PINNåœ¨TensorFlow2.xä¸Šçš„ä»£ç æ¡†æ¶ [omniscientoctopus/Physics-Informed-Neural-Networks: Investigating PINNs (github.com)](https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks),



### æœ¬æ–‡ç ”ç©¶ä»€ä¹ˆï¼Ÿ

åˆ©ç”¨æš‘å‡ç ”ç©¶ä¸€ä¸‹å¦‚ä¸‹é—®é¢˜ï¼š 

- PINNæ¨¡å‹åœ¨è€¦åˆé—®é¢˜ä¸Šçš„åº”ç”¨ï¼ˆè¿™æ–¹é¢çš„ç ”ç©¶ä¼¼ä¹è¿˜ä¸å¤šï¼‰ï¼Œ
- PINNæ­£è´Ÿé—®é¢˜æ±‚è§£ï¼ˆçœ‹äº†ä¸€äº›æ–‡ç« è¯´PINNçš„ä¼˜åŠ¿å…¶å®åœ¨äº **é«˜ç»´é—®é¢˜ & åé—®é¢˜æ±‚è§£**
- ä»£ç å®ç°è‹¥å¹²ç®—ä¾‹ï¼ˆä¹Ÿè®¸å¯èƒ½maybe Only oneğŸ˜ï¼‰ï¼Œæœ€å¥½èƒ½æŠŠæ¨¡å‹åº”ç”¨åˆ°3ç»´ã€‚



### PINNæ¨¡å‹æ­å»º

> å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨ï¼Œå…ˆæŒæ¡TensorFlowï¼Œå¹¶ä½¿ç”¨TensorFlowè‡ªå®šä¹‰PINNæ¨¡å‹æ˜¯å¿…è¦çš„ã€‚

å…ˆè®¾æƒ³ä¸€ä¸ªä»£ç æ¡†æ¶ã€‚

PINNçš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æœå…ˆä¸è€ƒè™‘losså‡½æ•°ï¼Œé‚£ä¹ˆPINNå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å…¨è¿æ¥åºåˆ—æ¨¡å‹ï¼Œå¯ä»¥ç”¨ **tf.keras.Sequential()**æ„å»ºã€‚

å‡è®¾Layer = [inputs,n1,n2,...nk,n_outputs]ï¼Œä»å·¦è‡³å³ä»£è¡¨æ¯å±‚ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚

```python
def createModel(layer):
    if len(layer) < 2:
        print("å±‚æ•°å°äº2ï¼, æ— æ³•æ„å»ºç¥ç»ç½‘ç»œ")
        return 
    model = keras.Sequential(name = "PINN")
    model.add(keras.Input(shape=(layer[0],)))
    for i in range(1,len(layer)-1):
        model.add(layers.Dense(layer[i], activation="relu", name="layer{}".format(i)))
    model.add(layers.Dense(layer[-1], name="outputs"))
    
    #  model.compile( loss= , metrics = [], optimizer= )
    #	...
    #
    #
    
    return model
```



#### <font color='purple'>åç»­è¦ä¸º model æ·»åŠ  lossï¼ˆPINNä¸»è¦éƒ¨åˆ†ï¼‰ã€metricï¼ˆå¯ä»¥ä¸è¦ï¼‰ã€optimizerï¼ˆä¼˜åŒ–å™¨ï¼Œå¿…è¦ï¼‰</font>

> ä»¥ä¸Šå†…å®¹äº 2022 -  06 - 29  markdownã€‚

---

## è®°å½•æ—¶é—´ï¼š2022-06-30

### <font color='blue'>1.PINNæ¨¡å‹æ­å»ºï¼ˆç»­ï¼‰</font>

TensorFlowä¸ºæˆ‘ä»¬æä¾›äº†å¤šç§å¤šæ ·çš„é«˜é˜¶APIå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿæ­å»ºæ¨¡å‹ã€‚**ä½†æ˜¯ï¼Œå¿«é€Ÿæ–¹ä¾¿çš„ä»£ä»·å°±æ˜¯çµæ´»æ€§é™ä½ã€‚**ç‰¹åˆ«æ˜¯PINNè·Ÿä¸€èˆ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ­¥éª¤ä¸åŒï¼Œä¸»è¦æ¥æºäºloss functionéœ€è¦å¯¹é¢„æµ‹å€¼è¿›è¡Œæ±‚å¯¼è¿ç®—ã€‚

ä»Šå¤©çš„ä»£ç ç¼–å†™é­é‡äº†ä¸€äº›å›°éš¾ï¼Œæˆ‘æ˜¨å¤©çš„æ„æ€å®é™…ä¸Šä¸å¯è¡Œï¼š

1. ä½¿ç”¨ **tf.keras.Sequential()**æˆ–è€…**tf.keras.Model()å‡½æ•°å¼API** æ­å»ºæ¨¡å‹ Modelã€‚(è¿™ä¸€æ­¥æ²¡é—®é¢˜ï¼‰

2. ç¼–å†™è‡ªå®šä¹‰losså‡½æ•°ï¼Œç„¶åå°† è‡ªå®šä¹‰çš„loss & ä¼˜åŒ–å™¨ & metrics ä¼ å…¥è‡ªå¸¦çš„æ–¹æ³• **Model.compile()**

3. è°ƒç”¨è‡ªå¸¦çš„æ–¹æ³• **Model.fit()** ï¼Œä¼ å…¥è®­ç»ƒæ•°æ®è®­ç»ƒå³å¯ã€‚



æ­¥éª¤1 å’Œ ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚è€ŒPINNæ¨¡å‹ç»“æ„ç®€å•ï¼Œä½¿ç”¨æ­¥éª¤1æ­å»ºæ¨¡å‹å®Œå…¨æ²¡æœ‰é—®é¢˜ã€‚

é—®é¢˜å‡ºåœ¨ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰ã€‚

è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å…¶ä¸­ä¸¤ä¸ªå…³é”®çš„æ–¹æ³• **Model.compile() & Model.fit()** æ–¹æ³•ç©¶ç«Ÿåšäº†ä»€ä¹ˆï¼Ÿ

å¦‚æœæ˜ç™½äº†Model.fit()çš„å·¥ä½œæ­¥éª¤ï¼Œå°±ä¼šçŸ¥é“è¿™ç§è‡ªå¸¦çš„è®­ç»ƒæ–¹å¼å±€é™æ€§ï¼Œä»¥åŠä¸ºä»€ä¹ˆåœ¨è®­ç»ƒPINNæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ã€‚



**Model.fit()å¤§è‡´å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š**

```python
def fit(self,X_train,Y_train,epoches,batch_size,**kwargs):
    #
    #		ä¸»è¦çš„è®­ç»ƒéƒ¨åˆ†ä»£ç 
    #
	for epoch in epoches: ## ä½¿ç”¨ X_train,Y_train è¿›è¡Œ epoch æ¬¡è®­ç»ƒ
        for x_batch,y_batch in dataset(X_train,Y_train,batch_size): ## dataset(X_train,Y_train,batch_size) æ•°æ®åˆ†ç»„
            train_step((x_batch,y_batch))  ## train_step(self,data) æ˜¯ ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥é‡è½½ï¼Œä»è€Œæ”¹å˜fit()

def train_step(self,data):
    
    # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.
    x, y = data

    with tf.GradientTape(persistent=True) as tape:
        y_pred = self(x, training=True)  # Forward pass
        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
        #è¿™é‡Œç”¨åˆ°çš„ self.compiled_loss() å°±æ˜¯ Model.compile(loss) ä¼ å…¥çš„loss
        #å³ Model.compile() ä½œç”¨æ˜¯è®© self.compiled_loss = loss
                       
	# Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)

    del tape

    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    
    #æŒ‡æ ‡è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    #....
```



å¯ä»¥çœ‹åˆ° **Model.fit()**åªæ˜¯ä¸ºæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªä¸€èˆ¬æ™®é€šçš„è®­ç»ƒæ­¥éª¤å‡½æ•°ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šã€‚

**Model.fit()** ä¼šåœ¨ train_step() ä¸­è°ƒç”¨ Model.compile() ä¼ å…¥çš„lossã€ä¼˜åŒ–å™¨ã€æŒ‡æ ‡ã€‚

```python
loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
```

ä¸Šè¿°ä»£ç ä¸­compiled_loss()é™å®šäº†å‚æ•° y , y_predã€‚yæ˜¯æ ‡ç­¾å€¼ï¼Œy_predæ˜¯é€šè¿‡æ¨¡å‹é¢„æµ‹çš„å€¼ã€‚



è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®˜æ–¹æ–‡æ¡£ã€ç½‘ä¸Šçš„å¸–å­è¯´ï¼š

**å¦‚æœæƒ³è‡ªå®šä¹‰lossï¼Œé‚£ä¹ˆä½ è‡ªå®šä¹‰çš„losså‡½æ•°ä¸€å®šè¦æœ‰ä¸¤ä¸ªè¾“å…¥å‚æ•°ï¼ˆyï¼Œy_pred)ã€‚**

**å› ä¸ºåœ¨ fit() â€”â€”ã€‹train_step()ä¸­ï¼Œè°ƒç”¨äº†self.compiled_loss()ï¼Œå¹¶è§„å®šäº†å®ƒçš„å‚å…¥å‚æ•°ä¸ºï¼ˆy,y_pred)**ã€‚



å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡è½½ train_step() ä½¿è‡ªå®šä¹‰losså‡½æ•°è‡ªç”±åº¦æ›´é«˜ï¼Œå¹¶ä»èƒ½ä½¿ç”¨fit()ã€‚

ç”šè‡³é‡è½½fit()ï¼Œä½†æ—¢ç„¶éƒ½é‡è½½fit()äº†ï¼Œå®ƒç­‰ä»·äºä»0ç¼–å†™è®­ç»ƒè¿‡ç¨‹ã€‚â€”â€”ä¸è¿‡ï¼Œè¿™æ­£æ˜¯PINNè®­ç»ƒéœ€è¦çš„ã€‚

**å› ä¸ºæˆ‘ä»¬æœ‰ä¸åŒæ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆå¸¦æ ‡ç­¾å’Œä¸å¸¦æ ‡ç­¾ï¼‰ã€å¤æ‚çš„losså‡½æ•°ï¼ˆä¸ä»…ä»…éœ€è¦yï¼Œy_predï¼Œè¿˜éœ€è¦y_predå¯¹xï¼Œtçš„å¯¼æ•°ï¼‰ï¼Œæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»ç„¶ä½¿ç”¨fitï¼ˆï¼‰ä»£ç æ¡†æ¶ï¼Œéœ€è¦å¤§åˆ€é˜”æ–§åœ°ä¿®æ”¹ã€‚è¿˜ä¸å¦‚è‡ªå·±å†™è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å»ä½¿ç”¨æ‰€è°“çš„é«˜é˜¶API~~**



### <font color='blue'>2.å­ç±»åŒ–Sequential() / Model() , å®šä¹‰ MyPinnï¼Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹</font>

ä¹‹å‰è®¨è®ºè¿‡**æ­¥éª¤1ï¼šä½¿ç”¨Sequentialæ­å»ºæ¨¡å‹** æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œæˆ‘ä»¬éœ€è¦åšçš„ä»…ä»…æ˜¯é‡æ–°å®šä¹‰**è®­ç»ƒè¿‡ç¨‹**ã€‚

é‚£ä¹ˆï¼Œå­ç±»åŒ–**class MyPinn(tf.keras.Sequential())**ï¼ŒMyPinnä¿ç•™Sequential()å¥½ç”¨çš„åŠŸèƒ½ï¼Œå†é™„åŠ ä¸Šæˆ‘ä»¬è‡ªå®šä¹‰çš„**è®­ç»ƒè¿‡ç¨‹**

```python
class MyPinn(keras.Sequential): ## ä»¥Burgers_Equationä¸ºä¾‹
    def __init__(self,name = None):
        
        super(MyPinn, self).__init__(name=name)
        self.nu = tf.constant(0.01/np.pi)
    
    @tf.function
    def test_gradient(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape() as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)
        u_x = tape.gradient(u,x)
        tf.print(u_x)
    
    @tf.function
    def loss_U(self,X_u_train,u_train):
        u_pred = self(X_u_train)
        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))
        return loss_u
    
    
    @tf.function
    def loss_PDE(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape(persistent=True) as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)  
            u_x = tape.gradient(u,x)         
            
        u_t = tape.gradient(u, t)     
        u_xx = tape.gradient(u_x, x)
        
        del tape
        
        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx

        loss_f = tf.reduce_mean(tf.square(f))

        return loss_f
    
    
    def loss_Total(self,X_u_train,u_train,X_f_train):
        loss_u = self.loss_U(X_u_train,u_train)
        loss_f = self.loss_PDE(X_f_train)
        
        loss_total = loss_u + loss_f
        
        return loss_total
    
    @tf.function
    def train_step(self,X_u_train,u_train,X_f_train):
        with tf.GradientTape(persistent=True) as tape:
            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)
                   
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss_total, trainable_vars)
        
        del tape
        
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        return loss_total
    
    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):
        for epoch in tf.range(1,epochs+1):
            loss_total = self.train_step(X_u_train,u_train,X_f_train)
            if epoch % 10 == 0:                
                print(
                    "Training loss (for per 10 epoches) at epoch %d: %.4f"
                    % (epoch, float(loss_total))
                )
```



<font color='purple'> **åœ¨å½“å‰æ–‡ä»¶å¤¹ myPINN.py è¿›è¡Œäº†train_modelæµ‹è¯•ï¼ŒæˆåŠŸè¿è¡Œ**</font>

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 6-30 markdown
