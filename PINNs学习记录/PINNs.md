è€¦åˆPINNã€æ­£åé—®é¢˜ã€3Dé—®é¢˜çš„å­¦ä¹ ç ”ç©¶



# 06-29 

### ä»€ä¹ˆæ˜¯PINNï¼Ÿ

Raissiç­‰äººåœ¨2018å¹´ 

 [Physics-informed neural networks: A deep learning 
framework for solving forward and inverse problems involving 
nonlinear partial differential equations](https://github.com/maziarraissi/PINNs)

ä¸­æå‡ºäº†PINNï¼Œé€šè¿‡åœ¨æŸè€—å‡½æ•°ä¸­ç»“åˆç‰©ç†åœºï¼ˆå³åå¾®åˆ†æ–¹ç¨‹ï¼‰å’Œè¾¹ç•Œæ¡ä»¶æ¥è§£å†³åå¾®åˆ†æ–¹ç¨‹ã€‚æŸå¤±æ˜¯åå¾®åˆ†æ–¹ç¨‹çš„å‡æ–¹è¯¯å·®å’Œåœ¨åˆ†å¸ƒåœ¨åŸŸä¸­çš„â€œæ­é…ç‚¹â€ä¸Šæµ‹é‡çš„è¾¹ç•Œæ®‹å·®ã€‚



ä»–ä»¬åœ¨æ–‡ç« ä¸­åˆ—å‡ºäº†æ•°ä¸ªç»å…¸pdeç®—ä¾‹æ¥éªŒè¯PINNçš„æ­£ç¡®æ€§ã€‚åœ¨ä»–ä»¬çš„githubä¸»é¡µä¸Šæä¾›äº†æºç ï¼Œä½¿ç”¨äº†Tensorflow1.xçš„æ¡†æ¶ã€‚

æˆªæ­¢åˆ°2022-06-29ï¼ŒTensorFlowæ›´æ–°åˆ°2.9ï¼ŒTensorFlow1.xä»£ç é£æ ¼è·ŸTensorFlow2.xç›¸å·®ç”šè¿œï¼Œè€Œä¸”Raissiç­‰äººæä¾›çš„ä»£ç ä¹Ÿæœ‰è®¸å¤šå€¼å¾—ä¼˜åŒ–çš„åœ°æ–¹ã€‚

æˆ‘åœ¨githubä¸Šæ‰¾åˆ°ä¸€ä¸ªäº†PINNåœ¨TensorFlow2.xä¸Šçš„ä»£ç æ¡†æ¶ [omniscientoctopus/Physics-Informed-Neural-Networks: Investigating PINNs (github.com)](https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks),



### æœ¬æ–‡ç ”ç©¶ä»€ä¹ˆï¼Ÿ

åˆ©ç”¨æš‘å‡ç ”ç©¶ä¸€ä¸‹å¦‚ä¸‹é—®é¢˜ï¼š 

- PINNæ¨¡å‹åœ¨è€¦åˆé—®é¢˜ä¸Šçš„åº”ç”¨ï¼ˆè¿™æ–¹é¢çš„ç ”ç©¶ä¼¼ä¹è¿˜ä¸å¤šï¼‰ï¼Œ
- PINNæ­£è´Ÿé—®é¢˜æ±‚è§£ï¼ˆçœ‹äº†ä¸€äº›æ–‡ç« è¯´PINNçš„ä¼˜åŠ¿å…¶å®åœ¨äº **é«˜ç»´é—®é¢˜ & åé—®é¢˜æ±‚è§£**
- ä»£ç å®ç°è‹¥å¹²ç®—ä¾‹ï¼ˆä¹Ÿè®¸å¯èƒ½maybe Only oneğŸ˜ï¼‰ï¼Œæœ€å¥½èƒ½æŠŠæ¨¡å‹åº”ç”¨åˆ°3ç»´ã€‚



### PINNæ¨¡å‹æ­å»º

> å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨ï¼Œå…ˆæŒæ¡TensorFlowï¼Œå¹¶ä½¿ç”¨TensorFlowè‡ªå®šä¹‰PINNæ¨¡å‹æ˜¯å¿…è¦çš„ã€‚

å…ˆè®¾æƒ³ä¸€ä¸ªä»£ç æ¡†æ¶ã€‚

PINNçš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æœå…ˆä¸è€ƒè™‘losså‡½æ•°ï¼Œé‚£ä¹ˆPINNå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å…¨è¿æ¥åºåˆ—æ¨¡å‹ï¼Œå¯ä»¥ç”¨ **tf.keras.Sequential()**æ„å»ºã€‚

å‡è®¾Layer = [inputs,n1,n2,...nk,n_outputs]ï¼Œä»å·¦è‡³å³ä»£è¡¨æ¯å±‚ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚

```python
def createModel(layer):
    if len(layer) < 2:
        print("å±‚æ•°å°äº2ï¼, æ— æ³•æ„å»ºç¥ç»ç½‘ç»œ")
        return 
    model = keras.Sequential(name = "PINN")
    model.add(keras.Input(shape=(layer[0],)))
    for i in range(1,len(layer)-1):
        model.add(layers.Dense(layer[i], activation="relu", name="layer{}".format(i)))
    model.add(layers.Dense(layer[-1], name="outputs"))
    
    #  model.compile( loss= , metrics = [], optimizer= )
    #	...
    #
    #
    
    return model
```



#### <font color='purple'>åç»­è¦ä¸º model æ·»åŠ  lossï¼ˆPINNä¸»è¦éƒ¨åˆ†ï¼‰ã€metricï¼ˆå¯ä»¥ä¸è¦ï¼‰ã€optimizerï¼ˆä¼˜åŒ–å™¨ï¼Œå¿…è¦ï¼‰</font>

> ä»¥ä¸Šå†…å®¹äº 2022 -  06 - 29  markdownã€‚

---

# 06-30

### <font color='blue'>1.PINNæ¨¡å‹æ­å»ºï¼ˆç»­ï¼‰</font>

TensorFlowä¸ºæˆ‘ä»¬æä¾›äº†å¤šç§å¤šæ ·çš„é«˜é˜¶APIå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿæ­å»ºæ¨¡å‹ã€‚**ä½†æ˜¯ï¼Œå¿«é€Ÿæ–¹ä¾¿çš„ä»£ä»·å°±æ˜¯çµæ´»æ€§é™ä½ã€‚**ç‰¹åˆ«æ˜¯PINNè·Ÿä¸€èˆ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ­¥éª¤ä¸åŒï¼Œä¸»è¦æ¥æºäºloss functionéœ€è¦å¯¹é¢„æµ‹å€¼è¿›è¡Œæ±‚å¯¼è¿ç®—ã€‚

ä»Šå¤©çš„ä»£ç ç¼–å†™é­é‡äº†ä¸€äº›å›°éš¾ï¼Œæˆ‘æ˜¨å¤©çš„æ„æ€å®é™…ä¸Šä¸å¯è¡Œï¼š

1. ä½¿ç”¨ **tf.keras.Sequential()**æˆ–è€…**tf.keras.Model()å‡½æ•°å¼API** æ­å»ºæ¨¡å‹ Modelã€‚(è¿™ä¸€æ­¥æ²¡é—®é¢˜ï¼‰

2. ç¼–å†™è‡ªå®šä¹‰losså‡½æ•°ï¼Œç„¶åå°† è‡ªå®šä¹‰çš„loss & ä¼˜åŒ–å™¨ & metrics ä¼ å…¥è‡ªå¸¦çš„æ–¹æ³• **Model.compile()**

3. è°ƒç”¨è‡ªå¸¦çš„æ–¹æ³• **Model.fit()** ï¼Œä¼ å…¥è®­ç»ƒæ•°æ®è®­ç»ƒå³å¯ã€‚



æ­¥éª¤1 å’Œ ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚è€ŒPINNæ¨¡å‹ç»“æ„ç®€å•ï¼Œä½¿ç”¨æ­¥éª¤1æ­å»ºæ¨¡å‹å®Œå…¨æ²¡æœ‰é—®é¢˜ã€‚

é—®é¢˜å‡ºåœ¨ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰ã€‚

è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å…¶ä¸­ä¸¤ä¸ªå…³é”®çš„æ–¹æ³• **Model.compile() & Model.fit()** æ–¹æ³•ç©¶ç«Ÿåšäº†ä»€ä¹ˆï¼Ÿ

å¦‚æœæ˜ç™½äº†Model.fit()çš„å·¥ä½œæ­¥éª¤ï¼Œå°±ä¼šçŸ¥é“è¿™ç§è‡ªå¸¦çš„è®­ç»ƒæ–¹å¼å±€é™æ€§ï¼Œä»¥åŠä¸ºä»€ä¹ˆåœ¨è®­ç»ƒPINNæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ã€‚



**Model.fit()å¤§è‡´å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š**

```python
def fit(self,X_train,Y_train,epoches,batch_size,**kwargs):
    #
    #		ä¸»è¦çš„è®­ç»ƒéƒ¨åˆ†ä»£ç 
    #
    for epoch in epoches: ## ä½¿ç”¨ X_train,Y_train è¿›è¡Œ epoch æ¬¡è®­ç»ƒ
        for x_batch,y_batch in dataset(X_train,Y_train,batch_size): ## dataset(X_train,Y_train,batch_size) æ•°æ®åˆ†ç»„
            train_step((x_batch,y_batch))  ## train_step(self,data) æ˜¯ ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥é‡è½½ï¼Œä»è€Œæ”¹å˜fit()

def train_step(self,data):
    
    # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.
    x, y = data

    with tf.GradientTape(persistent=True) as tape:
        y_pred = self(x, training=True)  # Forward pass
        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
        #è¿™é‡Œç”¨åˆ°çš„ self.compiled_loss() å°±æ˜¯ Model.compile(loss) ä¼ å…¥çš„loss
        #å³ Model.compile() ä½œç”¨æ˜¯è®© self.compiled_loss = loss
                       
	# Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)

    del tape

    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    
    #æŒ‡æ ‡è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    #....
```



å¯ä»¥çœ‹åˆ° **Model.fit()**åªæ˜¯ä¸ºæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªä¸€èˆ¬æ™®é€šçš„è®­ç»ƒæ­¥éª¤å‡½æ•°ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šã€‚

**Model.fit()** ä¼šåœ¨ train_step() ä¸­è°ƒç”¨ Model.compile() ä¼ å…¥çš„lossã€ä¼˜åŒ–å™¨ã€æŒ‡æ ‡ã€‚

```python
loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
```

ä¸Šè¿°ä»£ç ä¸­compiled_loss()é™å®šäº†å‚æ•° y , y_predã€‚yæ˜¯æ ‡ç­¾å€¼ï¼Œy_predæ˜¯é€šè¿‡æ¨¡å‹é¢„æµ‹çš„å€¼ã€‚





è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®˜æ–¹æ–‡æ¡£ã€ç½‘ä¸Šçš„å¸–å­è¯´ï¼š

**å¦‚æœæƒ³è‡ªå®šä¹‰lossï¼Œé‚£ä¹ˆä½ è‡ªå®šä¹‰çš„losså‡½æ•°ä¸€å®šè¦æœ‰ä¸¤ä¸ªè¾“å…¥å‚æ•°ï¼ˆyï¼Œy_pred)ã€‚**

**å› ä¸ºåœ¨ fit() â€”â€”ã€‹train_step()ä¸­ï¼Œè°ƒç”¨äº†self.compiled_loss()ï¼Œå¹¶è§„å®šäº†å®ƒçš„å‚å…¥å‚æ•°ä¸ºï¼ˆy,y_pred)**ã€‚





å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡è½½ train_step() ä½¿è‡ªå®šä¹‰losså‡½æ•°è‡ªç”±åº¦æ›´é«˜ï¼Œå¹¶ä»èƒ½ä½¿ç”¨fit()ã€‚

ç”šè‡³é‡è½½fit()ï¼Œä½†æ—¢ç„¶éƒ½é‡è½½fit()äº†ï¼Œå®ƒç­‰ä»·äºä»0ç¼–å†™è®­ç»ƒè¿‡ç¨‹ã€‚â€”â€”ä¸è¿‡ï¼Œè¿™æ­£æ˜¯PINNè®­ç»ƒéœ€è¦çš„ã€‚

**å› ä¸ºæˆ‘ä»¬æœ‰ä¸åŒæ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆå¸¦æ ‡ç­¾å’Œä¸å¸¦æ ‡ç­¾ï¼‰ã€å¤æ‚çš„losså‡½æ•°ï¼ˆä¸ä»…ä»…éœ€è¦yï¼Œy_predï¼Œè¿˜éœ€è¦y_predå¯¹xï¼Œtçš„å¯¼æ•°ï¼‰ï¼Œæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»ç„¶ä½¿ç”¨fitï¼ˆï¼‰ä»£ç æ¡†æ¶ï¼Œéœ€è¦å¤§åˆ€é˜”æ–§åœ°ä¿®æ”¹ã€‚è¿˜ä¸å¦‚è‡ªå·±å†™è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å»ä½¿ç”¨æ‰€è°“çš„é«˜é˜¶API~~**



### <font color='blue'>2.å­ç±»åŒ–Sequential() / Model() , å®šä¹‰ MyPinnï¼Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹</font>

ä¹‹å‰è®¨è®ºè¿‡**æ­¥éª¤1ï¼šä½¿ç”¨Sequentialæ­å»ºæ¨¡å‹** æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œæˆ‘ä»¬éœ€è¦åšçš„ä»…ä»…æ˜¯é‡æ–°å®šä¹‰**è®­ç»ƒè¿‡ç¨‹**ã€‚

é‚£ä¹ˆï¼Œå­ç±»åŒ–**class MyPinn(tf.keras.Sequential())**ï¼ŒMyPinnä¿ç•™Sequential()å¥½ç”¨çš„åŠŸèƒ½ï¼Œå†é™„åŠ ä¸Šæˆ‘ä»¬è‡ªå®šä¹‰çš„**è®­ç»ƒè¿‡ç¨‹**

```python
class MyPinn(keras.Sequential): ## ä»¥Burgers_Equationä¸ºä¾‹
    def __init__(self,name = None):
        
        super(MyPinn, self).__init__(name=name)
        self.nu = tf.constant(0.01/np.pi)
    
    @tf.function
    def test_gradient(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape() as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)
        u_x = tape.gradient(u,x)
        tf.print(u_x)
    
    @tf.function
    def loss_U(self,X_u_train,u_train):
        u_pred = self(X_u_train)
        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))
        return loss_u
    
    
    @tf.function
    def loss_PDE(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape(persistent=True) as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)  
            u_x = tape.gradient(u,x)         
            
        u_t = tape.gradient(u, t)     
        u_xx = tape.gradient(u_x, x)
        
        del tape
        
        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx

        loss_f = tf.reduce_mean(tf.square(f))

        return loss_f
    
    
    def loss_Total(self,X_u_train,u_train,X_f_train):
        loss_u = self.loss_U(X_u_train,u_train)
        loss_f = self.loss_PDE(X_f_train)
        
        loss_total = loss_u + loss_f
        
        return loss_total
    
    @tf.function
    def train_step(self,X_u_train,u_train,X_f_train):
        with tf.GradientTape(persistent=True) as tape:
            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)
                   
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss_total, trainable_vars)
        
        del tape
        
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        return loss_total
    
    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):
        for epoch in tf.range(1,epochs+1):
            loss_total = self.train_step(X_u_train,u_train,X_f_train)
            if epoch % 10 == 0:                
                print(
                    "Training loss (for per 10 epoches) at epoch %d: %.4f"
                    % (epoch, float(loss_total))
                )
```



<font color='purple'> **åœ¨å½“å‰æ–‡ä»¶å¤¹ myPINN.py è¿›è¡Œäº†train_modelæµ‹è¯•ï¼ŒæˆåŠŸè¿è¡Œ**</font>

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 6-30 markdown

---

# 07-01

### <font color='blue'>ä¼˜åŒ–å™¨</font>

tf.keras.optimizersä¸ºæˆ‘ä»¬æä¾›äº†è®¸å¤šç°æˆçš„ä¼˜åŒ–å™¨ï¼Œæ¯”å¦‚SGDï¼ˆæœ€é€Ÿä¸‹é™ï¼‰ã€Adamã€RMSpropç­‰ç­‰ã€‚



å‡è®¾ï¼Œç°æœ‰æ¨¡å‹å¯¹è±¡ MyPinnã€‚

å¯ä»¥é€šè¿‡ tf.keras.optimizers.Optimizer() åˆ›å»ºä¼˜åŒ–å™¨å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œ MyPinn.optimizer = tf.keras.optimizers.SGD()

tf.keras.optimizers.Optimizer()ä¸»è¦æä¾›äº†ä¸¤ç§Methodsï¼Œä¸ºæˆ‘ä»¬çš„å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚

1. **apply_gradients(**
       **grads_and_vars, name=None, experimental_aggregate_gradients=True**
   **)**

â€‹	ä¹‹å‰å®šä¹‰çš„MyPinn.train_step()ä¸­å°±ä½¿ç”¨äº†è¿™ç§Methodã€‚

â€‹	æˆ‘ä»¬å…ˆè®¡ç®—å‡ºgradsï¼Œå†ä½¿ç”¨apply_gradient()ï¼Œè¿›è¡Œå‚æ•°ä¼˜åŒ–ã€‚

2. **minimize(**
       **loss, var_list, grad_loss=None, name=None, tape=None**
   **)**

   minimize()æ–¹æ³•å…ˆä½¿ç”¨tf.GradientTape()è®¡ç®—å‡ºlossï¼Œå†è°ƒç”¨apply_gradients()ã€‚ç›¸å½“äºæŠŠcompute gradientså’Œapply gradients å°è£…åœ¨ä¸€èµ·ã€‚

å¯ä»¥å‘ç°ï¼Œapply_gradients()å°±æ˜¯minimize()ä¸­çš„ç¬¬äºŒæ­¥ã€‚

**ä¸ºäº†ç²¾å‡†åœ°æ§åˆ¶ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ ä¸Šä¸€äº›åˆ«çš„æ“ä½œï¼Œæˆ‘ä»¬ä½¿ç”¨ ç¬¬1ç§æ–¹æ³• å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚**



### <font color='blue'>  Adam & L-BFGS </font>

Adamä¼˜åŒ–å™¨åœ¨deep neural networkä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¹‹å‰ä¹Ÿè¯´è¿‡ï¼Œtf.keras.optimizersé‡Œå†…ç½®äº†Adamä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨å°±å¥½ã€‚

åœ¨PINNåŸä½œè€…çš„ä»£ç ä¸­(tensorflow1.x)ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸¤ç§ä¼˜åŒ–å™¨ ï¼š Adam & L-BFGSã€‚

åœ¨ training model è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬å…ˆä½¿ç”¨ Adam è¿›è¡Œä¼˜åŒ–ï¼Œåä½¿ç”¨ L- BFGS è¿›è¡Œä¼˜åŒ–ã€‚



L-BFGS æ˜¯ ç§©2çš„æ‹Ÿç‰›é¡¿æ–¹æ³•(è¿™å­¦æœŸ â€œæœ€ä¼˜åŒ–æ–¹æ³•â€ è¯¾ä¸Šåˆšå¥½å­¦è¿‡)ï¼Œå®ƒæ˜¯åŸºæœ¬ç‰›é¡¿æ–¹æ³•çš„ä¸€ç§å˜å½¢ã€‚ç‰›é¡¿æ–¹æ³•åœ¨æå€¼ç‚¹é™„è¿‘æ—¶ï¼Œæ”¶æ•›é€Ÿåº¦å¿«ï¼Œè€Œæ‹Ÿç‰›é¡¿æ–¹æ³•åœ¨ä¿æŒè¿™ä¸ªä¼˜ç§€æ€§è´¨çš„åŸºç¡€ä¸Šï¼Œæ”¹è¿›äº†ç‰›é¡¿æ–¹æ³•è¿‡ç¨‹çš„ä¸€äº›ç¼ºç‚¹ï¼Œæ¯”å¦‚è®¡ç®—äºŒé˜¶å¯¼ã€çŸ©é˜µæ±‚é€†å’ŒGä¸æ­£å®šç­‰é—®é¢˜ã€‚





ç„¶è€Œåœ¨TensorFlow1.xä¸­ å¹¶æ²¡æœ‰å†…ç½®çš„ L-BFGSï¼Œä½œè€…å®é™…æ˜¯ä½¿ç”¨tensoflow1.x æä¾›çš„ä¸€ä¸ªæ¥å£ï¼Œä½¿ç”¨ Scipy åº“ä¸­çš„ L-BFGSã€‚

Scipyä¸­è°ƒç”¨L-BFGSçš„æ ¼å¼æ˜¯ï¼š

```python
scipy.optimize.minimize(fun, 
				x0, args=(),
                method='L-BFGS-B', 
                jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)
```

 å…¶ä¸­funæ˜¯ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œè¿”å›ç›®æ ‡å‡½æ•°å€¼ã€‚

> ```
> fun(x, *args) -> float
> ```

where `x` **is a 1-D array with shape (n,)** and `args` is a tuple of the fixed parameters needed to completely specify the function.

> å½“ jac = True æ—¶ï¼Œ fun()  retrun fval , gradients



è¿™æ—¶ï¼Œå†çœ‹ä¸€ä¸‹ä½œè€…è°ƒç”¨L-BFGSçš„ä»£ç ï¼Œå°±çŸ¥é“æ˜¯ä»€ä¹ˆæ„æ€äº†ã€‚ã€‚

```python
self.optimizer =tf.contrib.opt.ScipyOptimizerInterface (self.loss, 
                                                        method = 'L-BFGS-B', 
                                                        options = {'maxiter': 3000,
                                                                   'maxfun': 3000,
                                                                   'maxcor': 50,
                                                                   'maxls': 50,
                                                                   'ftol' : 1.0 * np.finfo(float).eps})
```

è¿™æ—¶å¦‚æœæˆ‘ä»¬å†™ self.optimizer.minimize() å®é™…ä¸Šå°±ä¼šè°ƒç”¨ scipy.optimize.minimize( args ) ï¼Œargs=ä¸Šè¿°ä»£ç ä¸­ä¼ å…¥çš„å‚æ•°ï¼Œself.loss ç›¸å½“äº funã€‚



é—æ†¾çš„æ˜¯ï¼Œåœ¨TensorFlow2.xä¸­ï¼Œè¯¥æ¥å£å·²ç»åˆ é™¤ã€‚

å½“ç„¶æˆ‘ä»¬ä»èƒ½æƒ³åŠæ³•ä½¿ç”¨Scipyä¸­çš„L-BFGSï¼Œ

æ— éå°±æ˜¯æŒ‰ç…§scipy.optimize.miminze()çš„è°ƒç”¨æ ¼å¼ï¼Œåœ¨MyPinnå†…éƒ¨å®šä¹‰ä¸€ä¸ªloss_fun(x) ï¼Œx is 1-D array with shape = (n,)ï¼Œ

ä½œä¸ºscipy.optimize.miminze(fun,...)ä¸­çš„funï¼Œä½†è¿™æ„å‘³ç€éœ€è¦æŠŠMyPinnçš„weightså’Œbias "æ‰å¹³åŒ–" æ”¾åœ¨ä¸€ä¸ª1ç»´æ•°ç»„ä¸­ï¼Œåœ¨ä¼˜åŒ–å®Œæ¯•åï¼Œè¿˜è¦æŠŠç»“æœå†å˜æˆåŸæ¥çš„å½¢çŠ¶ï¼Œæ”¾å›MyPinné‡Œã€‚



åˆä½†æ˜¯ï¼Œè™½ç„¶æ¥å£æ²¡äº†ï¼Œä½†TensorFlow2.0ä¸­ tfp åº“ä¸­æœ‰å®ç° L-BFGS ç®—æ³•ã€‚ğŸ˜



**ä¸‹é¢é“¾æ¥ä¸­ï¼Œé«˜èµå›ç­”è®¨è®ºäº†åœ¨TensorFlow2.xä¸­ä½¿ç”¨ Scipyçš„L-BFGS å’Œ è‡ªå¸¦çš„L-BFGS è®¡ç®—å·®åˆ«ã€‚**

[python - Use Scipy Optimizer with Tensorflow 2.0 for Neural Network training - Stack Overflow](https://stackoverflow.com/questions/59029854/use-scipy-optimizer-with-tensorflow-2-0-for-neural-network-training)

<img src="./Data/L-BFGS in scipy and tfp.png" />



**å¯ä»¥å‘ç°ä½¿ç”¨TensorFlow2.0 tfpä¸­çš„L-BFGSè®¡ç®—é€Ÿåº¦æ›´å¿«**

**ä¸è¿‡tfpä¸­çš„L-BFGSè®¡ç®—ç»“æœç•¥é€ŠäºScipyä¸­çš„L-BFGSï¼Œå¯èƒ½æ˜¯TensorFlowé»˜è®¤float32ï¼Œè€ŒScipyæ˜¯float64ï¼Œä»¥åŠScipyä¸­L-BFGSç®—æ³•çš„å®ç°æ¯”tfpçš„æ›´å¥½ã€‚**



[Optimize TensorFlow & Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

æ³¨æ„å¦‚æœæƒ³ä½¿ç”¨tfpçš„L-BFGSä¹Ÿæ˜¯è¦æ±‚è¾“å…¥å˜é‡æ˜¯1-Dçš„ã€‚è€Œæˆ‘ä»¬çš„PiNNæ¨¡å‹ä¸­çš„weightså’Œbiaséƒ½æ˜¯ä»¥å¤šç»´çš„å½¢å¼ä¿å­˜ï¼Œæ‰€ä»¥è¦å…ˆå°†å®ƒä»¬è¿›è¡Œâ€œæ‰å¹³åŒ–â€ï¼Œå†ä¼ å…¥L-BFGSå‡½æ•°ä¸­ã€‚

ä¸Šé¢çš„é“¾æ¥è®¨è®ºäº†å¦‚ä½•å®šä¹‰å°†modelä¸­çš„å˜é‡â€œæ‰å¹³åŒ–â€ï¼Œå˜å›æ¥ ä»¥åŠ å¦‚ä½•å®šä¹‰â€œfunction_factory"è¿”å›ä¸€ä¸ªLBFGSéœ€è¦çš„functionã€‚



æˆ‘ä»¬çœ‹ä¸‹tfpä¸­L-BFGSçš„è°ƒç”¨æ ¼å¼ï¼š

```python
tfp.optimizer.lbfgs_minimize(
    value_and_gradients_function,
    initial_position,
    num_correction_pairs=10,
    tolerance=1e-08,
    x_tolerance=0,
    f_relative_tolerance=0,
    initial_inverse_hessian_estimate=None,
    max_iterations=50,
    parallel_iterations=1,
    stopping_condition=None,
    name=None
)
# value_and_gradients_function æ˜¯ä¸€ä¸ªå‡½æ•°, 
# Input: paramters with shape = 1-D ; Output: loss and gradients with paramters, gradients are also 1-D.

# initial_position: initial paramters 
```

<font color='purple' >**å¯¹MyPinnæ¨¡å‹(keras.Sequentialæ¨¡å‹)ä½¿ç”¨tfp L-BFGS()è¿›è¡Œå‚æ•°ä¼˜åŒ–æµç¨‹å¦‚ä¸‹ï¼š** </font>

1. æå–MyPinnä¸­çš„weights å’Œ bias (å³éœ€è¦ä¼˜åŒ–çš„parameters)ï¼Œæ­¤æ—¶å®ƒä»¬å°±æ˜¯initial_position(æœªæ‰å¹³åŒ–)ã€‚

2. åˆ›å»ºä¸¤ä¸ªåˆ—è¡¨ idx=[],part=[]

3. æŠŠMyPinnæ¯å±‚å‚æ•°çš„shapeç­‰è‹¥å¹²ä¿¡æ¯ï¼Œç”¨å¾ªç¯appendåˆ°idxå’Œpartã€‚

   idxå¸®åŠ©æˆ‘ä»¬è°ƒç”¨ tf.dynamic_stitch()å°†weightså’Œbias"æ‰å¹³åŒ–"æˆparams_1dã€‚

   partå¸®åŠ©æˆ‘ä»¬params_1då˜å›weightså’Œbias,å¹¶æ›´æ–°MyPinnä¸­çš„å‚æ•°ã€‚

4. å®šä¹‰ä¸€ä¸ªfuncå‡½æ•°,func(params_1d)

â€‹		Input:  params_1d

â€‹		Output: loss , gradients

â€‹		Insideï¼šå…ˆæŠŠ params_1d è½¬å˜å› MyPinn ä¸­ weights,bias çš„shapeï¼Œå¹¶æ›´æ–°å®ƒä»¬ã€‚

â€‹					  ä½¿ç”¨MyPinnä¸­å·²å®šä¹‰çš„loss_Total()æ–¹æ³•è®¡ç®—loss å’Œ gradientsã€‚

â€‹				      æ³¨æ„ï¼šéœ€è¦å°† gradients ä¹Ÿæ‰å¹³åŒ– å†returnã€‚( gradients.shape = [weights,bias].shape,æ•…ä¹Ÿå¯ä»¥ç”¨idxæ‰å¹³åŒ– )

5.  å°†ç¬¬ä¸€æ­¥æå–å‡ºæ¥çš„weights å’Œ bias æ‰å¹³åŒ–å¤„ç†ï¼Œä½œä¸ºinitial_position
6. è°ƒç”¨tfp.optimizer.lbfgs_minimize(func,initial_position)å³å¯ï¼

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-1 markdown

---



# 07-04

â€‹	ä»Šå¤©ä¸»è¦æ·»åŠ äº† Data Preparation å’Œ Plot çš„ä»£ç ã€‚å¹¶ä¸”ï¼ŒæŒ‰æ—¥æœŸå‘½åï¼Œå°†ä»£ç åˆ†å¼€åœ¨ä¸åŒçš„NoteBookã€‚

â€‹    ä½¿ç”¨ä¸‹é¢é“¾æ¥ä¸­çš„è®­ç»ƒæ•°æ®ã€‚

â€‹	[Optimize TensorFlow & Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

â€‹	ä¸è¿‡ï¼Œæˆ‘åœ¨ä½¿ç”¨MyPinnè®­ç»ƒBurgers Equationï¼Œè®­ç»ƒç»“æœä¸å¤ªç†æƒ³ã€‚ä¸€å¼€å§‹ï¼Œæˆ‘æ€€ç–‘æ˜¯ï¼Œfloat32æ ¼å¼ **And** tfpä¸­lbfgsä¸Scipyä¸­lbfgsçš„å·®åˆ«ã€‚debugäº†å¾ˆä¹…ï¼Œå‘ç°ä¸æ˜¯è¿™äº›åŸå› ã€‚

â€‹	å› ä¸ºä¸Šé¢çš„é“¾æ¥ä¸­ï¼Œä½œè€…ä¹Ÿæœ‰ç”¨tfpä¸­çš„lbfgsè®­ç»ƒæ¨¡å‹ï¼Œæˆ‘è¿è¡Œäº†ä¸€éï¼Œä»ç„¶å¯ä»¥è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœï¼Œçœ‹æ¥è¿˜éœ€è¦debug(æ¼ã€‚



â€‹	é¢˜å¤–è¯ï¼Œä½¿ç”¨Google colabå¯ä»¥ç™½å«–ç®—åŠ›ï¼Œå°†.ipynbæ–‡ä»¶ä¸Šä¼ ï¼Œå¯ä»¥åœ¨äº‘ç«¯è®¡ç®—ï¼Œè¿˜å…è´¹ï¼Œè€Œä¸”æˆ‘çš„ç”µè„‘å†…å­˜æœ‰æ—¶å€™ä¸å¤ªå¤Ÿç”¨ï¼Œæ‰€ä»¥colabå°±å¾ˆniceã€‚



> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-4 markdown

---



# 07-05 Debug

ä»Šå¤©èŠ±äº†ä¸å°‘æ—¶é—´ï¼Œæ€»ç®—è®©æˆ‘å‘ç°äº†è¿™ä¸ªæ‰€è°“çš„â€œbugâ€ã€‚**â€œæ‰¾bugæ—¶é—´ï¼Œæ¯”å†™ä»£ç æ—¶é—´è¦é•¿â€è¿™æ¬¡çœŸçš„å°è¯äº†è¿™å¥è¯å§ã€‚**

è¿‡ç¨‹ä¸­ï¼Œæˆ‘ç”šè‡³ä¸€åº¦æƒ³è¦æ”¾å¼ƒè‡ªå·±çš„MyPinnæ¨¡å‹ï¼Œä½¿ç”¨åˆ«äººçš„æ„å»ºæ¨¡å‹ä»£ç ï¼ˆæ¯”å¦‚ä¸Šé¢çš„é“¾æ¥ï¼‰ ã€‚

**åœ¨è§£é‡Šbugä¹‹å‰**ï¼Œæˆ‘éœ€è¦è¯´æ˜ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆæˆ‘è‡ªå·±è¦ç”¨æ„å»ºä¸€å¥—TensorFlow2.xçš„Pinn classï¼Œè€Œä¸æ˜¯ç”¨åˆ«äººçš„ã€‚

[Optimize TensorFlow & Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

**ä¸ºæ–¹ä¾¿è¯´æ˜ï¼Œæˆ‘å°†ä¸Šè¿°é“¾æ¥ä¸­çš„pinnä»£ç ç§°ä¸º code**ã€‚

1. TensorFlow2.xé»˜è®¤æ˜¯eageråŠ¨æ€å›¾æ¨¡å¼ï¼Œè¿™ç§æ¨¡å¼è®¡ç®—é€Ÿåº¦æ¯”è¾ƒæ…¢ï¼Œä½†æ–¹ä¾¿è°ƒè¯•ã€‚ä½¿ç”¨@tf.functionå‡½æ•°è£…é¥°å™¨ï¼Œèƒ½å°† eagerè½¬ä¸º**AutoGraphåŠ¨æ€å›¾æ¨¡å¼**è®¡ç®—ï¼Œæ•ˆç‡å ªæ¯”TensorFlow1.xçš„é™æ€å›¾ï¼ˆtf1.xåªæ”¯æŒé™æ€å›¾ï¼‰ï¼ŒåŒæ—¶ä¹Ÿæ–¹ä¾¿è°ƒè¯•ã€‚**ä½†æ˜¯å‘¢ï¼Œ@tf.function æœ‰ä¸€å®šçš„ç¼–ç¨‹è§„èŒƒã€‚**

   è€Œ **code**ï¼Œæ²¡æœ‰ä½¿ç”¨TensorFlow2.0çš„@tf.functionåŠŸèƒ½ã€‚

   è¿™æ˜¯åŸå› ä¸€ã€‚

2. **é«˜é˜¶API**ã€‚tf.keraså·²ç»ä¸ºæˆ‘ä»¬æä¾›äº†å„ç§å±‚ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰ ä»¥åŠ **2ç§ä¸»è¦æ„å»ºæ¨¡å‹çš„æ–¹å¼**ï¼šSequential()ï¼Œå‡½æ•°å¼APIã€‚çµæ´»ä½¿ç”¨å‡½æ•°å¼APIï¼Œç†è®ºä¸Šå¯ä»¥æ„å»ºä»»ä½•æ¨¡å‹ï¼Œä¸å•å•æ˜¯ç¥ç»ç½‘ç»œã€‚è€ŒPinnä½¿ç”¨Sequential()å³å¯ã€‚æ¯”å¦‚ï¼Œæˆ‘çš„ä»£ç å°±æ˜¯è®©MyPinnç»§æ‰¿keras.Sequential

   è€Œåœ¨**code**ä¸­ï¼ŒPinnç»§æ‰¿çš„æ˜¯keras.Moduleï¼ŒModuleå®é™…ä¸Šæ˜¯Sequentialç­‰é«˜é˜¶APIçš„åŸºç±»ï¼Œå®ƒæä¾›çš„åŠŸèƒ½æ˜¯è®°å½•åœ¨ç±»ä¸­å‡ºç°çš„Variablesï¼Œè€Œæ²¡æœ‰Sequentialä¸­æ›´å¤šå¥½ç”¨æ–¹ä¾¿çš„åŠŸèƒ½ï¼Œæ¯”å¦‚ç›´æ¥ä½¿ç”¨addæ–¹æ³•ï¼Œæ­é…keras.layers.Denseï¼Œä¸ºMyPinnæ·»åŠ å±‚(æœ¬è´¨ä¸Šä¹Ÿæ˜¯æ·»åŠ Variables)ã€‚

   è‹¥ä½¿ç”¨**code**ä¸­çš„æ–¹æ³•ï¼Œè®©Pinnå•å•ç»§æ‰¿Moduleï¼Œåˆ™éœ€è¦è‡ªå·±åšå˜é‡åˆå§‹åŒ–ï¼Œå¹¶ä¸”æ²¡æœ‰åˆ«çš„å¥½ç”¨çš„åŠŸèƒ½ã€‚å®ƒåªèƒ½è®°å½•classä¸­å‡ºç°çš„å˜é‡è€Œå·²ã€‚

   è¿™æ˜¯åŸå› äºŒã€‚

3. **ä¼˜åŒ–å™¨**ã€‚åœ¨codeä¸­ä¸ºäº†ä½¿ç”¨LBFGSï¼Œåœ¨ç±»ä¸­å®šä¹‰äº† â€œå˜é‡ä¸€ç»´åŒ–ã€ä¸€ç»´å˜é‡è½¬ä¸ºåŸæ¥å½¢çŠ¶â€ç­‰ ä¸“é—¨ä¸ºä½¿ç”¨LBFGSçš„å‡½æ•°ã€‚æˆ‘ä¸å–œæ¬¢è¿™æ ·å­ã€‚

   **ä¸ºäº†æ¸…æ™°æ€§ï¼Œæˆ‘æ›´å€¾å‘äºå°†æ¨¡å‹å®šä¹‰å’Œä¼˜åŒ–æ–¹æ³•åˆ†ç¦»å¼€ï¼Œè€Œä¸æ˜¯å…¨åœ¨å†™åœ¨classä¸­ã€‚**

    â€œå˜é‡ä¸€ç»´åŒ–ã€ä¸€ç»´å˜é‡è½¬ä¸ºåŸæ¥å½¢çŠ¶â€è¿™æ ·çš„å‡½æ•°ï¼Œåªæ˜¯ä¸ºä½¿ç”¨LBFGSæœåŠ¡çš„ï¼Œä»–åº”è¯¥å®šä¹‰åœ¨å¤–é¢ã€‚å¦‚æœä½¿ç”¨å†…ç½®çš„Adamã€SDGç­‰ä¼˜åŒ–å™¨ï¼Œå®Œå…¨ä¸éœ€è¦å®šä¹‰è¿™äº›å‡½æ•°ï¼Œç›´æ¥åœ¨train_step()ä¸­ï¼Œoptimizer.apply_gradientï¼ˆï¼‰å³å¯ã€‚

   è¿™æ˜¯åŸå› ä¸‰ã€‚



æ‰€ä»¥bugå‡ºç°åœ¨å“ªå‘¢ï¼Ÿ@tf.function ç¼–ç¨‹ä¸è§„èŒƒï¼Ÿ å˜é‡åå†™é”™ï¼Ÿ

**åœ¨loss_PDEçš„å®šä¹‰ä¸­ã€‚**



ä¸‹é¢æˆ‘åˆ†åˆ«ç»™å‡ºæˆ‘è‡ªå·±çš„å®šä¹‰ä»¥åŠcodeçš„å®šä¹‰

```python
def loss_PDE(self,X_f_train): #æˆ‘çš„å®šä¹‰
    
      x = X_f_train[:,0]  # x.shape =(nums,)
      t = X_f_train[:,1]  # t.shape =(nums,)
      with tf.GradientTape(persistent=True) as tape:
          tape.watch([x,t])
          X = tf.stack([x,t],axis=-1) # X.shape = (nums,2)
          u = self(X)  # predict u
          u_x = tape.gradient(u,x)
```

```python
def loss_PDE(self,X_f_train): #codeçš„å®šä¹‰
      
      x = X_f_train[:,0:1] # x.shape = (nums,2)
      t = X_f_train[:,1:2] # t.shape = (nums,2)
      with tf.GradientTape(persistent=True) as tape:
          tape.watch([x,t])
          X = tf.stack([x[:,0],t[:,0]],axis=1) #X.shape = (nums,2)
          u = self(X)  # predict u
          u_x = tape.gradient(u,x)
```

ä¸¤ç§ä»£ç çš„ç¬¬3è¡Œå’Œç¬¬4è¡Œï¼Œå¹²çš„éƒ½æ˜¯åŒä¸€ä»¶äº‹ï¼ŒæŠŠX_f_trainçš„ x å’Œ t åˆ†ç¦»å‡ºæ¥ï¼ˆåé¢è¦å¯¹xï¼Œtæ±‚å¯¼ï¼‰ã€‚

```python
with tf.GradientTape(persistent=True) as tape:
	....
```

ä¸Šè¿°å°†è®¡ç®—è¿‡ç¨‹è®°å½•åœ¨â€œtapeâ€ä¸­ï¼Œåé¢å¯ä»¥æ±‚tapeä¸­å‡ºç°å˜é‡ï¼ˆåŒ…æ‹¬ä¸­é—´å˜é‡ï¼‰çš„å¯¼æ•°ã€‚

tape.watch([x,t])å°†xï¼Œtä¹Ÿè®°å½•åˆ°tapeä¸­ï¼Œå› ä¸ºé»˜è®¤åªè®°å½•variablesï¼Œè€Œæ­¤å¤„xå’Œfæ˜¯constantã€‚

æˆ‘ä»¬å°†x,té€šè¿‡tf.stack()æ‹¼æ¥åˆ°ä¸€èµ·ç»„æˆæ¨¡å‹çš„inputï¼šXï¼Œè°ƒç”¨self(X) predict uã€‚tf.stackä¹Ÿä¼šè¢«çœ‹åšä¸€ç§ç®—å­ï¼Œè¢«è®°å½•åˆ°tapeä¸­ã€‚

ä½ æˆ–è®¸å‘ç°Xå’Œloss_PDEå‡½æ•°ä¼ å…¥çš„X_f_train**æ•°å€¼ä¸Šä¸€æ ·**ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥self(X_f_trian)å‘¢ï¼Ÿ**å› ä¸ºå®ƒä»…ä»…æ˜¯æ•°å€¼ä¸Šä¸€æ ·ã€‚ã€‚ã€‚**

å¯ä»¥è¯•ä¸€ä¸‹æ”¹ç”¨u=self(X_f_train)ï¼Œé‚£ä¹ˆu_xè¿”å›çš„Noneã€‚



ä»¤äººæˆ‘æ„Ÿåˆ°å¥‡æ€ªçš„æ˜¯ï¼Œä¸¤ç§ä»£ç çš„ç¬¬7æ­¥ï¼Œæœ€åè¿”å›çš„Xæ˜¯ä¸€æ ·çš„ï¼Œä¸ºä»€ä¹ˆåªæœ‰ç¬¬äºŒç§ï¼ˆcodeçš„å®ç°ï¼‰OKï¼Ÿè€Œæˆ‘çš„å°±ä¸è¡Œã€‚

å…¶å®æˆ‘åœ¨å†™ä»£ç çš„æ—¶å€™ï¼Œå‚è€ƒäº†codeçš„loss_PDEï¼Œè§‰å¾—ä»–çš„æ“ä½œæœ‰ç‚¹**å¤šä½™**ï¼Œå°±ç”¨äº†ä¸€ç§æ›´æ¸…æ¥šçš„å†™æ³•ï¼Œæ­£å¦‚ç¬¬ä¸€ç§å†™æ³•é‚£æ ·ï¼Œç„¶è€Œè®­ç»ƒçš„ç»“æœå°±æ˜¯ï¼Œç¬¬ä¸€ç§ä¸è¡Œï¼ï¼Ÿ



**ä¸‹é¢æˆ‘å†éªŒè¯ä¸€ä¸‹ä¸¤ç§å†™æ³•çš„(ç¬¬7è¡Œ)Xæ˜¯å¦ä¸€è‡´?** 

<img src="./Data/bug.png" style="zoom:75%;" />

ç»“æœæ˜¾ç¤º g å’Œ gg ï¼Œç¡®å®æ˜¯ä¸€æ ·çš„ï¼ˆå…¨æ˜¯Trueï¼Œæ²¡æœ‰æˆªå®Œå…¨)ï¼Œä»¤äººç–‘æƒ‘ã€‚

**ä¸ç®¡äº†ã€‚åé¢æˆ‘ä½¿ç”¨äº†codeçš„å†™æ³•.æ¨¡å‹çš„losså‡½æ•°èƒ½ä¸æ–­ä¸‹é™äº†ï¼Œæ•ˆæœè§7_4-7_5MyPINN_Burgers.ipynb**

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-5 markdown

---

# 07-06

ä»Šå¤©é˜…è¯»å­¦ä¹ äº†ä¸€äº›NNåœ¨PDEæ±‚è§£é—®é¢˜ä¸­çš„å¤šç§æ–¹æ³•ã€‚

### ç¥ç»ç½‘ç»œå¸¸è§ç»“æ„

<img src='./Data/NN_structures.png' style="zoom:50%;"  >



**ç›®å‰PINNå¤§å¤šä¸ºå…¨è¿æ¥å‰å‘ç½‘ç»œ(tensorflowä¸­çš„Denseå±‚)ï¼Œå°è¯•ä½¿ç”¨å¦‚å·ç§¯å±‚ã€å¾ªç¯ç½‘ç»œã€åé¦ˆç½‘ç»œï¼Ÿ**



### ç¥ç»ç½‘ç»œå¯¹è¾¹ç•Œæ¡ä»¶çš„å¤„ç†æ–¹å¼

ANN: artificial neural network

1. çº¯ç½‘ç»œå‹

   1. 1  Loss = $\lambda_1 * MSE_{pde} + \lambda_2 * MSE_{BC}$  ï¼Œ åŸå§‹çš„PINNæ¨¡å‹å°±ä½¿ç”¨çš„æ˜¯è¿™ç§æ–¹å¼ï¼Œåˆ©ç”¨è¾¹ç•Œæ¡ä»¶å’Œé¢„æµ‹å€¼è®¡ç®—$MSE_{BC}$,çº³å…¥lossä¸­ã€‚

   1. 2  å°†è¾¹ç•Œæ¡ä»¶å¸¦å…¥ç¥ç»ç½‘ç»œè¡¨è¾¾å¼â€”â€”å–ä»£éƒ¨åˆ†weightsï¼Œé€šè¿‡å¾®åˆ†æ–¹ç¨‹æ®‹å·®$MSE_{pde}$ä¼˜åŒ–å‰©ä½™æƒå€¼ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src='./Data/BC_solution1.png' style="zoom:70%;" >

â€‹			**ä½†æˆ‘è®¤ä¸ºï¼Œå¯¹äºæŸä¸€ä¸ªå¤æ‚pdeï¼Œä»£ç å®ç°è¿™ç§æ€æƒ³å¹¶ä¸å®¹æ˜“ã€‚**

2. è¾¹ç•Œå¸æ”¶å‹

â€‹	è¾¹ç•Œå¸æ”¶å‹çš„æ€æƒ³æ˜¯ï¼šæŠŠç¥ç»ç½‘ç»œçœ‹åšå‡½æ•° ANN(X)ï¼›æ„é€ è¾¹ç•Œå‡½æ•°BC(X)ï¼šå½“Xâˆˆè¾¹ç•Œæ—¶ï¼ŒBCä¸ºè¾¹ç•Œå€¼ï¼Œå¦åˆ™ä¸º0ï¼›æ„é€  L(X)ï¼Œå½“Xâˆˆè¾¹ç•Œæ—¶ï¼ŒL(X)=0.

ä»¤è¯•è§£  $y_t = BC(X) + L(X) * ANN(X) $, æ­¤å‡½æ•°ä¸¥æ ¼æ»¡è¶³è¾¹ç•Œæ¡ä»¶ã€‚å†é€šè¿‡åŸŸå†…ç‚¹è®¡ç®—$MSE_{pde}$ï¼Œæ›´æ–°ANNã€‚

â€‹	**BC(X),L(X)çš„æ„é€ æ–¹æ³•ï¼š**

â€‹	<img src='./Data/BC_ODE.png'>



<img src='./Data/BC_PDE.png'>

**åœ¨PDEä¸­å¦‚ä½•è§£é‡ŠANN(X)çš„ä½œç”¨ï¼Ÿ** é€šç”¨BC(X),L(X)çš„æ„é€ æ–¹æ³•ï¼Ÿ



è¿›ä¸€æ­¥ï¼Œä½¿ç”¨ç‹¬ç«‹ç½‘ç»œ$ANN_{BC}$ä»£æ›¿BC(X)ã€‚

<img src='./Data/Duo_NN.png' style="zoom:50%;" >

æœ‰äº†è¿™ç§ç‹¬ç«‹ç½‘ç»œçš„æ€æƒ³ï¼Œä¹Ÿæœ‰äººæå‡ºå¤šç½‘ç»œçš„æ¨¡å‹ã€‚å³ç”¨ç‹¬ç«‹çš„ç½‘ç»œåˆ†åˆ«é¢„æµ‹PDEä¸­çš„å„ç§åå¯¼ï¼Œæœ€åæŠŠlossåŠ åœ¨ä¸€èµ·ã€‚
