# è€¦åˆPINNã€æ­£åé—®é¢˜ã€3Dé—®é¢˜çš„å­¦ä¹ ç ”ç©¶

# 06-29

### ä»€ä¹ˆæ˜¯PINNï¼Ÿ

Raissiç­‰äººåœ¨2018å¹´

 [Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving
nonlinear partial differential equations](https://github.com/maziarraissi/PINNs)

ä¸­æå‡ºäº†PINNï¼Œé€šè¿‡åœ¨æŸè€—å‡½æ•°ä¸­ç»“åˆç‰©ç†åœºï¼ˆå³åå¾®åˆ†æ–¹ç¨‹ï¼‰å’Œè¾¹ç•Œæ¡ä»¶æ¥è§£å†³åå¾®åˆ†æ–¹ç¨‹ã€‚æŸå¤±å‡½æ•°æ˜¯åå¾®åˆ†æ–¹ç¨‹åˆ†å¸ƒåœ¨åŸŸä¸­çš„â€œæ­é…ç‚¹â€çš„å‡æ–¹è¯¯å·®å’Œè¾¹ç•Œæ®‹å·®ä¹‹å’Œã€‚

ä»–ä»¬åœ¨æ–‡ç« ä¸­åˆ—å‡ºäº†æ•°ä¸ªç»å…¸pdeç®—ä¾‹æ¥éªŒè¯PINNçš„æ­£ç¡®æ€§ã€‚åœ¨ä»–ä»¬çš„githubä¸»é¡µä¸Šæä¾›äº†æºç ï¼Œä½¿ç”¨äº†Tensorflow1.xçš„æ¡†æ¶ã€‚

æˆªæ­¢åˆ°2022-06-29ï¼ŒTensorFlowæ›´æ–°åˆ°2.9ï¼ŒTensorFlow1.xä»£ç é£æ ¼è·ŸTensorFlow2.xç›¸å·®ç”šè¿œï¼Œè€Œä¸”Raissiç­‰äººæä¾›çš„ä»£ç ä¹Ÿæœ‰è®¸å¤šå€¼å¾—ä¼˜åŒ–çš„åœ°æ–¹ã€‚

æˆ‘åœ¨githubä¸Šæ‰¾åˆ°ä¸€ä¸ªäº†PINNåœ¨TensorFlow2.xä¸Šçš„ä»£ç æ¡†æ¶ [omniscientoctopus/Physics-Informed-Neural-Networks: Investigating PINNs (github.com)](https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks),

### æœ¬æ–‡ç ”ç©¶ä»€ä¹ˆï¼Ÿ

åˆ©ç”¨æš‘å‡ç ”ç©¶ä¸€ä¸‹å¦‚ä¸‹é—®é¢˜ï¼š

- PINNæ¨¡å‹åœ¨è€¦åˆé—®é¢˜ä¸Šçš„åº”ç”¨ï¼ˆè¿™æ–¹é¢çš„ç ”ç©¶ä¼¼ä¹è¿˜ä¸å¤šï¼‰ï¼Œ
- PINNæ­£è´Ÿé—®é¢˜æ±‚è§£ï¼ˆçœ‹äº†ä¸€äº›æ–‡ç« è¯´PINNçš„ä¼˜åŠ¿å…¶å®åœ¨äº **é«˜ç»´é—®é¢˜ & åé—®é¢˜æ±‚è§£**
- ä½¿ç”¨tensorflow2.0ï¼Œä»£ç å®ç°è‹¥å¹²ç®—ä¾‹ï¼ˆä¹Ÿè®¸å¯èƒ½maybe Only oneğŸ˜ï¼‰ï¼Œæœ€å¥½èƒ½æŠŠæ¨¡å‹åº”ç”¨åˆ°3ç»´ã€‚

### PINNæ¨¡å‹æ­å»º

> å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨ï¼Œå…ˆæŒæ¡TensorFlowï¼Œå¹¶ä½¿ç”¨TensorFlowè‡ªå®šä¹‰PINNæ¨¡å‹æ˜¯å¿…è¦çš„ã€‚

å…ˆè®¾æƒ³ä¸€ä¸ªä»£ç æ¡†æ¶ã€‚

PINNçš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æœå…ˆä¸è€ƒè™‘losså‡½æ•°ï¼Œé‚£ä¹ˆPINNå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å…¨è¿æ¥åºåˆ—æ¨¡å‹ï¼Œå¯ä»¥ç”¨ **tf.keras.Sequential()**æ„å»ºã€‚

å‡è®¾Layer = [inputs,n1,n2,...nk,n_outputs]ï¼Œä»å·¦è‡³å³ä»£è¡¨æ¯å±‚ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚

```python
def createModel(layer):
    if len(layer) < 2:
        print("å±‚æ•°å°äº2ï¼, æ— æ³•æ„å»ºç¥ç»ç½‘ç»œ")
        return 
    model = keras.Sequential(name = "PINN")
    model.add(keras.Input(shape=(layer[0],)))
    for i in range(1,len(layer)-1):
        model.add(layers.Dense(layer[i], activation="relu", name="layer{}".format(i)))
    model.add(layers.Dense(layer[-1], name="outputs"))
  
    #  model.compile( loss= , metrics = [], optimizer= )
    #	...
    #
    #
  
    return model
```

**`<font color='purple'>`åç»­è¦ä¸º model æ·»åŠ  lossï¼ˆPINNä¸»è¦éƒ¨åˆ†ï¼‰ã€metricï¼ˆå¯ä»¥ä¸è¦ï¼‰ã€optimizerï¼ˆä¼˜åŒ–å™¨ï¼Œå¿…è¦ï¼‰`</font>`**

> ä»¥ä¸Šå†…å®¹äº 2022 -  06 - 29  markdownã€‚

---

# 06-30

### `<font color='blue'>`1.PINNæ¨¡å‹æ­å»ºï¼ˆç»­ï¼‰`</font>`

TensorFlowä¸ºæˆ‘ä»¬æä¾›äº†å¤šç§å¤šæ ·çš„é«˜é˜¶APIå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿæ­å»ºæ¨¡å‹ã€‚**ä½†æ˜¯ï¼Œå¿«é€Ÿæ–¹ä¾¿çš„ä»£ä»·å°±æ˜¯çµæ´»æ€§é™ä½ã€‚**ç‰¹åˆ«æ˜¯PINNè·Ÿä¸€èˆ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ­¥éª¤ä¸åŒï¼Œä¸»è¦æ¥æºäºloss functionéœ€è¦å¯¹é¢„æµ‹å€¼è¿›è¡Œæ±‚å¯¼è¿ç®—ã€‚

ä»Šå¤©çš„ä»£ç ç¼–å†™é­é‡äº†ä¸€äº›å›°éš¾ï¼Œæˆ‘æ˜¨å¤©çš„æ„æ€å®é™…ä¸Šä¸å¯è¡Œï¼š

1. ä½¿ç”¨ **tf.keras.Sequential()**æˆ–è€…**tf.keras.Model()å‡½æ•°å¼API** æ­å»ºæ¨¡å‹ Modelã€‚(è¿™ä¸€æ­¥æ²¡é—®é¢˜ï¼‰
2. ç¼–å†™è‡ªå®šä¹‰losså‡½æ•°ï¼Œç„¶åå°† è‡ªå®šä¹‰çš„loss & ä¼˜åŒ–å™¨ & metrics ä¼ å…¥è‡ªå¸¦çš„æ–¹æ³• **Model.compile()**
3. è°ƒç”¨è‡ªå¸¦çš„æ–¹æ³• **Model.fit()** ï¼Œä¼ å…¥è®­ç»ƒæ•°æ®è®­ç»ƒå³å¯ã€‚

æ­¥éª¤1 å’Œ ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚è€ŒPINNæ¨¡å‹ç»“æ„ç®€å•ï¼Œä½¿ç”¨æ­¥éª¤1æ­å»ºæ¨¡å‹å®Œå…¨æ²¡æœ‰é—®é¢˜ã€‚

é—®é¢˜å‡ºåœ¨ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰ã€‚

è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å…¶ä¸­ä¸¤ä¸ªå…³é”®çš„æ–¹æ³• **Model.compile() & Model.fit()** æ–¹æ³•ç©¶ç«Ÿåšäº†ä»€ä¹ˆï¼Ÿ

å¦‚æœæ˜ç™½äº†Model.fit()çš„å·¥ä½œæ­¥éª¤ï¼Œå°±ä¼šçŸ¥é“è¿™ç§è‡ªå¸¦çš„è®­ç»ƒæ–¹å¼å±€é™æ€§ï¼Œä»¥åŠä¸ºä»€ä¹ˆåœ¨è®­ç»ƒPINNæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ã€‚

**Model.fit()å¤§è‡´å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š**

```python
def fit(self,X_train,Y_train,epoches,batch_size,**kwargs):
    #
    #		ä¸»è¦çš„è®­ç»ƒéƒ¨åˆ†ä»£ç 
    #
    for epoch in epoches: ## ä½¿ç”¨ X_train,Y_train è¿›è¡Œ epoch æ¬¡è®­ç»ƒ
        for x_batch,y_batch in dataset(X_train,Y_train,batch_size): ## dataset(X_train,Y_train,batch_size) æ•°æ®åˆ†ç»„
            train_step((x_batch,y_batch))  ## train_step(self,data) æ˜¯ ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥é‡è½½ï¼Œä»è€Œæ”¹å˜fit()

def train_step(self,data):
  
    # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.
    x, y = data

    with tf.GradientTape(persistent=True) as tape:
        y_pred = self(x, training=True)  # Forward pass
        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
        #è¿™é‡Œç”¨åˆ°çš„ self.compiled_loss() å°±æ˜¯ Model.compile(loss) ä¼ å…¥çš„loss
        #å³ Model.compile() ä½œç”¨æ˜¯è®© self.compiled_loss = loss
                   
	# Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)

    del tape

    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
  
    #æŒ‡æ ‡è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    #....
```

å¯ä»¥çœ‹åˆ° **Model.fit()**åªæ˜¯ä¸ºæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªä¸€èˆ¬æ™®é€šçš„è®­ç»ƒæ­¥éª¤å‡½æ•°ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šã€‚

**Model.fit()** ä¼šåœ¨ train_step() ä¸­è°ƒç”¨ Model.compile() ä¼ å…¥çš„lossã€ä¼˜åŒ–å™¨ã€æŒ‡æ ‡ã€‚

```python
loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
```

ä¸Šè¿°ä»£ç ä¸­compiled_loss()é™å®šäº†å‚æ•° y , y_predã€‚yæ˜¯æ ‡ç­¾å€¼ï¼Œy_predæ˜¯é€šè¿‡æ¨¡å‹é¢„æµ‹çš„å€¼ã€‚

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®˜æ–¹æ–‡æ¡£ã€ç½‘ä¸Šçš„å¸–å­è¯´ï¼š

**å¦‚æœæƒ³è‡ªå®šä¹‰lossï¼Œé‚£ä¹ˆä½ è‡ªå®šä¹‰çš„losså‡½æ•°ä¸€å®šè¦æœ‰ä¸¤ä¸ªè¾“å…¥å‚æ•°ï¼ˆyï¼Œy_pred)ã€‚**

**å› ä¸ºåœ¨ fit() â€”â€”ã€‹train_step()ä¸­ï¼Œè°ƒç”¨äº†self.compiled_loss()ï¼Œå¹¶è§„å®šäº†å®ƒçš„å‚å…¥å‚æ•°ä¸ºï¼ˆy,y_pred)**ã€‚

å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡è½½ train_step() ä½¿è‡ªå®šä¹‰losså‡½æ•°è‡ªç”±åº¦æ›´é«˜ï¼Œå¹¶ä»èƒ½ä½¿ç”¨fit()ã€‚

ç”šè‡³é‡è½½fit()ï¼Œä½†æ—¢ç„¶éƒ½é‡è½½fit()äº†ï¼Œå®ƒç­‰ä»·äºä»0ç¼–å†™è®­ç»ƒè¿‡ç¨‹ã€‚â€”â€”ä¸è¿‡ï¼Œè¿™æ­£æ˜¯PINNè®­ç»ƒéœ€è¦çš„ã€‚

**å› ä¸ºæˆ‘ä»¬æœ‰ä¸åŒæ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆå¸¦æ ‡ç­¾å’Œä¸å¸¦æ ‡ç­¾ï¼‰ã€å¤æ‚çš„losså‡½æ•°ï¼ˆä¸ä»…ä»…éœ€è¦yï¼Œy_predï¼Œè¿˜éœ€è¦y_predå¯¹xï¼Œtçš„å¯¼æ•°ï¼‰ï¼Œæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»ç„¶ä½¿ç”¨fitï¼ˆï¼‰ä»£ç æ¡†æ¶ï¼Œéœ€è¦å¤§åˆ€é˜”æ–§åœ°ä¿®æ”¹ã€‚è¿˜ä¸å¦‚è‡ªå·±å†™è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å»ä½¿ç”¨æ‰€è°“çš„é«˜é˜¶API~~**

### `<font color='blue'>`2.å­ç±»åŒ–Sequential() / Model() , å®šä¹‰ MyPinnï¼Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ `</font>`

ä¹‹å‰è®¨è®ºè¿‡**æ­¥éª¤1ï¼šä½¿ç”¨Sequentialæ­å»ºæ¨¡å‹** æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œæˆ‘ä»¬éœ€è¦åšçš„ä»…ä»…æ˜¯é‡æ–°å®šä¹‰**è®­ç»ƒè¿‡ç¨‹**ã€‚

é‚£ä¹ˆï¼Œå­ç±»åŒ–**class MyPinn(tf.keras.Sequential())**ï¼ŒMyPinnä¿ç•™Sequential()å¥½ç”¨çš„åŠŸèƒ½ï¼Œå†é™„åŠ ä¸Šæˆ‘ä»¬è‡ªå®šä¹‰çš„**è®­ç»ƒè¿‡ç¨‹**

```python
class MyPinn(keras.Sequential): ## ä»¥Burgers_Equationä¸ºä¾‹
    def __init__(self,name = None):
    
        super(MyPinn, self).__init__(name=name)
        self.nu = tf.constant(0.01/np.pi)
  
    @tf.function
    def test_gradient(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape() as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)
        u_x = tape.gradient(u,x)
        tf.print(u_x)
  
    @tf.function
    def loss_U(self,X_u_train,u_train):
        u_pred = self(X_u_train)
        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))
        return loss_u
  
  
    @tf.function
    def loss_PDE(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape(persistent=True) as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)  
            u_x = tape.gradient(u,x)     
        
        u_t = tape.gradient(u, t)   
        u_xx = tape.gradient(u_x, x)
    
        del tape
    
        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx

        loss_f = tf.reduce_mean(tf.square(f))

        return loss_f
  
  
    def loss_Total(self,X_u_train,u_train,X_f_train):
        loss_u = self.loss_U(X_u_train,u_train)
        loss_f = self.loss_PDE(X_f_train)
    
        loss_total = loss_u + loss_f
    
        return loss_total
  
    @tf.function
    def train_step(self,X_u_train,u_train,X_f_train):
        with tf.GradientTape(persistent=True) as tape:
            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)
               
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss_total, trainable_vars)
    
        del tape
    
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        return loss_total
  
    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):
        for epoch in tf.range(1,epochs+1):
            loss_total = self.train_step(X_u_train,u_train,X_f_train)
            if epoch % 10 == 0:            
                print(
                    "Training loss (for per 10 epoches) at epoch %d: %.4f"
                    % (epoch, float(loss_total))
                )
```

`<font color='purple'>` **åœ¨å½“å‰æ–‡ä»¶å¤¹ myPINN.py è¿›è¡Œäº†train_modelæµ‹è¯•ï¼ŒæˆåŠŸè¿è¡Œ** `</font>`

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 6-30 markdown

---

# 07-01

### `<font color='blue'>`ä¼˜åŒ–å™¨ `</font>`

tf.keras.optimizersä¸ºæˆ‘ä»¬æä¾›äº†è®¸å¤šç°æˆçš„ä¼˜åŒ–å™¨ï¼Œæ¯”å¦‚SGDï¼ˆæœ€é€Ÿä¸‹é™ï¼‰ã€Adamã€RMSpropç­‰ç­‰ã€‚

å‡è®¾ï¼Œç°æœ‰æ¨¡å‹å¯¹è±¡ MyPinnã€‚

å¯ä»¥é€šè¿‡ tf.keras.optimizers.Optimizer() åˆ›å»ºä¼˜åŒ–å™¨å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œ MyPinn.optimizer = tf.keras.optimizers.SGD()

tf.keras.optimizers.Optimizer()ä¸»è¦æä¾›äº†ä¸¤ç§Methodsï¼Œä¸ºæˆ‘ä»¬çš„å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚

1. **apply_gradients(**
   **grads_and_vars, name=None, experimental_aggregate_gradients=True**
   **)**

   ä¹‹å‰å®šä¹‰çš„MyPinn.train_step()ä¸­å°±ä½¿ç”¨äº†è¿™ç§Methodã€‚

   æˆ‘ä»¬å…ˆè®¡ç®—å‡ºgradsï¼Œå†ä½¿ç”¨apply_gradient()ï¼Œè¿›è¡Œå‚æ•°ä¼˜åŒ–ã€‚
2. **minimize(**
   **loss, var_list, grad_loss=None, name=None, tape=None**
   **)**

   minimize()æ–¹æ³•å…ˆä½¿ç”¨tf.GradientTape()è®¡ç®—å‡ºlossï¼Œå†è°ƒç”¨apply_gradients()ã€‚ç›¸å½“äºæŠŠcompute gradientså’Œapply gradients å°è£…åœ¨ä¸€èµ·ã€‚

å¯ä»¥å‘ç°ï¼Œapply_gradients()å°±æ˜¯minimize()ä¸­çš„ç¬¬äºŒæ­¥ã€‚

**ä¸ºäº†ç²¾å‡†åœ°æ§åˆ¶ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ ä¸Šä¸€äº›åˆ«çš„æ“ä½œï¼Œæˆ‘ä»¬ä½¿ç”¨ ç¬¬1ç§æ–¹æ³• å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚**

### `<font color='blue'>`  Adam & L-BFGS `</font>`

Adamä¼˜åŒ–å™¨åœ¨deep neural networkä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¹‹å‰ä¹Ÿè¯´è¿‡ï¼Œtf.keras.optimizersé‡Œå†…ç½®äº†Adamä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨å°±å¥½ã€‚

åœ¨PINNåŸä½œè€…çš„ä»£ç ä¸­(tensorflow1.x)ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸¤ç§ä¼˜åŒ–å™¨ ï¼š Adam & L-BFGSã€‚

åœ¨ training model è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬å…ˆä½¿ç”¨ Adam è¿›è¡Œä¼˜åŒ–ï¼Œåä½¿ç”¨ L- BFGS è¿›è¡Œä¼˜åŒ–ã€‚

L-BFGS æ˜¯ ç§©2çš„æ‹Ÿç‰›é¡¿æ–¹æ³•(è¿™å­¦æœŸ â€œæœ€ä¼˜åŒ–æ–¹æ³•â€ è¯¾ä¸Šåˆšå¥½å­¦è¿‡)ï¼Œå®ƒæ˜¯åŸºæœ¬ç‰›é¡¿æ–¹æ³•çš„ä¸€ç§å˜å½¢ã€‚ç‰›é¡¿æ–¹æ³•åœ¨æå€¼ç‚¹é™„è¿‘æ—¶ï¼Œæ”¶æ•›é€Ÿåº¦å¿«ï¼Œè€Œæ‹Ÿç‰›é¡¿æ–¹æ³•åœ¨ä¿æŒè¿™ä¸ªä¼˜ç§€æ€§è´¨çš„åŸºç¡€ä¸Šï¼Œæ”¹è¿›äº†ç‰›é¡¿æ–¹æ³•è¿‡ç¨‹çš„ä¸€äº›ç¼ºç‚¹ï¼Œæ¯”å¦‚è®¡ç®—äºŒé˜¶å¯¼ã€çŸ©é˜µæ±‚é€†å’ŒGä¸æ­£å®šç­‰é—®é¢˜ã€‚

ç„¶è€Œåœ¨TensorFlow1.xä¸­ å¹¶æ²¡æœ‰å†…ç½®çš„ L-BFGSï¼Œä½œè€…å®é™…æ˜¯ä½¿ç”¨tensoflow1.x æä¾›çš„ä¸€ä¸ªæ¥å£ï¼Œä½¿ç”¨ Scipy åº“ä¸­çš„ L-BFGSã€‚

Scipyä¸­è°ƒç”¨L-BFGSçš„æ ¼å¼æ˜¯ï¼š

```python
scipy.optimize.minimize(fun, 
				x0, args=(),
                method='L-BFGS-B', 
                jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)
```

 å…¶ä¸­funæ˜¯ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œè¿”å›ç›®æ ‡å‡½æ•°å€¼ã€‚

> ```
> fun(x, *args) -> float
> ```

where `x` **is a 1-D array with shape (n,)** and `args` is a tuple of the fixed parameters needed to completely specify the function.

> å½“ jac = True æ—¶ï¼Œ fun()  retrun fval , gradients

è¿™æ—¶ï¼Œå†çœ‹ä¸€ä¸‹ä½œè€…è°ƒç”¨L-BFGSçš„ä»£ç ï¼Œå°±çŸ¥é“æ˜¯ä»€ä¹ˆæ„æ€äº†ã€‚ã€‚

```python
self.optimizer =tf.contrib.opt.ScipyOptimizerInterface (self.loss, 
                                                        method = 'L-BFGS-B', 
                                                        options = {'maxiter': 3000,
                                                                   'maxfun': 3000,
                                                                   'maxcor': 50,
                                                                   'maxls': 50,
                                                                   'ftol' : 1.0 * np.finfo(float).eps})
```

è¿™æ—¶å¦‚æœæˆ‘ä»¬å†™ self.optimizer.minimize() å®é™…ä¸Šå°±ä¼šè°ƒç”¨ scipy.optimize.minimize( args ) ï¼Œargs=ä¸Šè¿°ä»£ç ä¸­ä¼ å…¥çš„å‚æ•°ï¼Œself.loss ç›¸å½“äº funã€‚

é—æ†¾çš„æ˜¯ï¼Œåœ¨TensorFlow2.xä¸­ï¼Œè¯¥æ¥å£å·²ç»åˆ é™¤ã€‚

å½“ç„¶æˆ‘ä»¬ä»èƒ½æƒ³åŠæ³•ä½¿ç”¨Scipyä¸­çš„L-BFGSï¼Œ

æ— éå°±æ˜¯æŒ‰ç…§scipy.optimize.miminze()çš„è°ƒç”¨æ ¼å¼ï¼Œåœ¨MyPinnå†…éƒ¨å®šä¹‰ä¸€ä¸ªloss_fun(x) ï¼Œx is 1-D array with shape = (n,)ï¼Œ

ä½œä¸ºscipy.optimize.miminze(fun,...)ä¸­çš„funï¼Œä½†è¿™æ„å‘³ç€éœ€è¦æŠŠMyPinnçš„weightså’Œbias "æ‰å¹³åŒ–" æ”¾åœ¨ä¸€ä¸ª1ç»´æ•°ç»„ä¸­ï¼Œåœ¨ä¼˜åŒ–å®Œæ¯•åï¼Œè¿˜è¦æŠŠç»“æœå†å˜æˆåŸæ¥çš„å½¢çŠ¶ï¼Œæ”¾å›MyPinné‡Œã€‚

åˆä½†æ˜¯ï¼Œè™½ç„¶æ¥å£æ²¡äº†ï¼Œä½†TensorFlow2.0ä¸­ tfp åº“ä¸­æœ‰å®ç° L-BFGS ç®—æ³•ã€‚ğŸ˜

**ä¸‹é¢é“¾æ¥ä¸­ï¼Œé«˜èµå›ç­”è®¨è®ºäº†åœ¨TensorFlow2.xä¸­ä½¿ç”¨ Scipyçš„L-BFGS å’Œ è‡ªå¸¦çš„L-BFGS è®¡ç®—å·®åˆ«ã€‚**

[python - Use Scipy Optimizer with Tensorflow 2.0 for Neural Network training - Stack Overflow](https://stackoverflow.com/questions/59029854/use-scipy-optimizer-with-tensorflow-2-0-for-neural-network-training)

`<img src="./Data/L-BFGS in scipy and tfp.png" />`

**å¯ä»¥å‘ç°ä½¿ç”¨TensorFlow2.0 tfpä¸­çš„L-BFGSè®¡ç®—é€Ÿåº¦æ›´å¿«**

**ä¸è¿‡tfpä¸­çš„L-BFGSè®¡ç®—ç»“æœç•¥é€ŠäºScipyä¸­çš„L-BFGSï¼Œå¯èƒ½æ˜¯TensorFlowé»˜è®¤float32ï¼Œè€ŒScipyæ˜¯float64ï¼Œä»¥åŠScipyä¸­L-BFGSç®—æ³•çš„å®ç°æ¯”tfpçš„æ›´å¥½ã€‚**

[Optimize TensorFlow &amp; Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

æ³¨æ„å¦‚æœæƒ³ä½¿ç”¨tfpçš„L-BFGSä¹Ÿæ˜¯è¦æ±‚è¾“å…¥å˜é‡æ˜¯1-Dçš„ã€‚è€Œæˆ‘ä»¬çš„PiNNæ¨¡å‹ä¸­çš„weightså’Œbiaséƒ½æ˜¯ä»¥å¤šç»´çš„å½¢å¼ä¿å­˜ï¼Œæ‰€ä»¥è¦å…ˆå°†å®ƒä»¬è¿›è¡Œâ€œæ‰å¹³åŒ–â€ï¼Œå†ä¼ å…¥L-BFGSå‡½æ•°ä¸­ã€‚

ä¸Šé¢çš„é“¾æ¥è®¨è®ºäº†å¦‚ä½•å®šä¹‰å°†modelä¸­çš„å˜é‡â€œæ‰å¹³åŒ–â€ï¼Œå˜å›æ¥ ä»¥åŠ å¦‚ä½•å®šä¹‰â€œfunction_factory"è¿”å›ä¸€ä¸ªLBFGSéœ€è¦çš„functionã€‚

æˆ‘ä»¬çœ‹ä¸‹tfpä¸­L-BFGSçš„è°ƒç”¨æ ¼å¼ï¼š

```python
tfp.optimizer.lbfgs_minimize(
    value_and_gradients_function,
    initial_position,
    num_correction_pairs=10,
    tolerance=1e-08,
    x_tolerance=0,
    f_relative_tolerance=0,
    initial_inverse_hessian_estimate=None,
    max_iterations=50,
    parallel_iterations=1,
    stopping_condition=None,
    name=None
)
# value_and_gradients_function æ˜¯ä¸€ä¸ªå‡½æ•°, 
# Input: paramters with shape = 1-D ; Output: loss and gradients with paramters, gradients are also 1-D.

# initial_position: initial paramters 
```

<font color='purple' >**å¯¹MyPinnæ¨¡å‹(keras.Sequentialæ¨¡å‹)ä½¿ç”¨tfp L-BFGS()è¿›è¡Œå‚æ•°ä¼˜åŒ–æµç¨‹å¦‚ä¸‹ï¼š** </font>

1. æå–MyPinnä¸­çš„weights å’Œ bias (å³éœ€è¦ä¼˜åŒ–çš„parameters)ï¼Œæ­¤æ—¶å®ƒä»¬å°±æ˜¯initial_position(æœªæ‰å¹³åŒ–)ã€‚
2. åˆ›å»ºä¸¤ä¸ªåˆ—è¡¨ idx=[],part=[]
3. æŠŠMyPinnæ¯å±‚å‚æ•°çš„shapeç­‰è‹¥å¹²ä¿¡æ¯ï¼Œç”¨å¾ªç¯appendåˆ°idxå’Œpartã€‚

   idxå¸®åŠ©æˆ‘ä»¬è°ƒç”¨ tf.dynamic_stitch()å°†weightså’Œbias"æ‰å¹³åŒ–"æˆparams_1dã€‚

   partå¸®åŠ©æˆ‘ä»¬params_1då˜å›weightså’Œbias,å¹¶æ›´æ–°MyPinnä¸­çš„å‚æ•°ã€‚
4. å®šä¹‰ä¸€ä¸ªfuncå‡½æ•°,func(params_1d)

   Input:  params_1d

   Output: loss , gradients

   Insideï¼šå…ˆæŠŠ params_1d è½¬å˜å› MyPinn ä¸­ weights,bias çš„shapeï¼Œå¹¶æ›´æ–°å®ƒä»¬ã€‚

   ä½¿ç”¨MyPinnä¸­å·²å®šä¹‰çš„loss_Total()æ–¹æ³•è®¡ç®—loss å’Œ gradientsã€‚

   æ³¨æ„ï¼šéœ€è¦å°† gradients ä¹Ÿæ‰å¹³åŒ– å†returnã€‚( gradients.shape = [weights,bias].shape,æ•…ä¹Ÿå¯ä»¥ç”¨idxæ‰å¹³åŒ– )
5. å°†ç¬¬ä¸€æ­¥æå–å‡ºæ¥çš„weights å’Œ bias æ‰å¹³åŒ–å¤„ç†ï¼Œä½œä¸ºinitial_position
6. è°ƒç”¨tfp.optimizer.lbfgs_minimize(func,initial_position)å³å¯ï¼

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-1 markdown

---

# 07-04

    ä»Šå¤©ä¸»è¦æ·»åŠ äº† Data Preparation å’Œ Plot çš„ä»£ç ã€‚å¹¶ä¸”ï¼ŒæŒ‰æ—¥æœŸå‘½åï¼Œå°†ä»£ç åˆ†å¼€åœ¨ä¸åŒçš„NoteBookã€‚

    ä½¿ç”¨ä¸‹é¢é“¾æ¥ä¸­çš„è®­ç»ƒæ•°æ®ã€‚

    [Optimize TensorFlow &amp; Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

    ä¸è¿‡ï¼Œæˆ‘åœ¨ä½¿ç”¨MyPinnè®­ç»ƒBurgers Equationï¼Œè®­ç»ƒç»“æœä¸å¤ªç†æƒ³ã€‚ä¸€å¼€å§‹ï¼Œæˆ‘æ€€ç–‘æ˜¯ï¼Œfloat32æ ¼å¼**And** tfpä¸­lbfgsä¸Scipyä¸­lbfgsçš„å·®åˆ«ã€‚debugäº†å¾ˆä¹…ï¼Œå‘ç°ä¸æ˜¯è¿™äº›åŸå› ã€‚

    å› ä¸ºä¸Šé¢çš„é“¾æ¥ä¸­ï¼Œä½œè€…ä¹Ÿæœ‰ç”¨tfpä¸­çš„lbfgsè®­ç»ƒæ¨¡å‹ï¼Œæˆ‘è¿è¡Œäº†ä¸€éï¼Œä»ç„¶å¯ä»¥è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœï¼Œçœ‹æ¥è¿˜éœ€è¦debug(æ¼ã€‚

    é¢˜å¤–è¯ï¼Œä½¿ç”¨Google colabå¯ä»¥ç™½å«–ç®—åŠ›ï¼Œå°†.ipynbæ–‡ä»¶ä¸Šä¼ ï¼Œå¯ä»¥åœ¨äº‘ç«¯è®¡ç®—ï¼Œè¿˜å…è´¹ï¼Œè€Œä¸”æˆ‘çš„ç”µè„‘å†…å­˜æœ‰æ—¶å€™ä¸å¤ªå¤Ÿç”¨ï¼Œæ‰€ä»¥colabå°±å¾ˆniceã€‚

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-4 markdown

---

# 07-05 Debug

ä»Šå¤©èŠ±äº†ä¸å°‘æ—¶é—´ï¼Œæ€»ç®—è®©æˆ‘å‘ç°äº†è¿™ä¸ªæ‰€è°“çš„â€œbugâ€ã€‚**â€œæ‰¾bugæ—¶é—´ï¼Œæ¯”å†™ä»£ç æ—¶é—´è¦é•¿â€è¿™æ¬¡çœŸçš„å°è¯äº†è¿™å¥è¯å§ã€‚**

è¿‡ç¨‹ä¸­ï¼Œæˆ‘ç”šè‡³ä¸€åº¦æƒ³è¦æ”¾å¼ƒè‡ªå·±çš„MyPinnæ¨¡å‹ï¼Œä½¿ç”¨åˆ«äººçš„æ„å»ºæ¨¡å‹ä»£ç ï¼ˆæ¯”å¦‚ä¸Šé¢çš„é“¾æ¥ï¼‰ ã€‚

**åœ¨è§£é‡Šbugä¹‹å‰**ï¼Œæˆ‘éœ€è¦è¯´æ˜ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆæˆ‘è‡ªå·±è¦ç”¨æ„å»ºä¸€å¥—TensorFlow2.xçš„Pinn classï¼Œè€Œä¸æ˜¯ç”¨åˆ«äººçš„ã€‚

[Optimize TensorFlow &amp; Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

**ä¸ºæ–¹ä¾¿è¯´æ˜ï¼Œæˆ‘å°†ä¸Šè¿°é“¾æ¥ä¸­çš„pinnä»£ç ç§°ä¸º code**ã€‚

1. TensorFlow2.xé»˜è®¤æ˜¯eageråŠ¨æ€å›¾æ¨¡å¼ï¼Œè¿™ç§æ¨¡å¼è®¡ç®—é€Ÿåº¦æ¯”è¾ƒæ…¢ï¼Œä½†æ–¹ä¾¿è°ƒè¯•ã€‚ä½¿ç”¨@tf.functionå‡½æ•°è£…é¥°å™¨ï¼Œèƒ½å°† eagerè½¬ä¸º**AutoGraphåŠ¨æ€å›¾æ¨¡å¼**è®¡ç®—ï¼Œæ•ˆç‡å ªæ¯”TensorFlow1.xçš„é™æ€å›¾ï¼ˆtf1.xåªæ”¯æŒé™æ€å›¾ï¼‰ï¼ŒåŒæ—¶ä¹Ÿæ–¹ä¾¿è°ƒè¯•ã€‚**ä½†æ˜¯å‘¢ï¼Œ@tf.function æœ‰ä¸€å®šçš„ç¼–ç¨‹è§„èŒƒã€‚**

   è€Œ **code**ï¼Œæ²¡æœ‰ä½¿ç”¨TensorFlow2.0çš„@tf.functionåŠŸèƒ½ã€‚

   è¿™æ˜¯åŸå› ä¸€ã€‚
2. **é«˜é˜¶API**ã€‚tf.keraså·²ç»ä¸ºæˆ‘ä»¬æä¾›äº†å„ç§å±‚ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰ ä»¥åŠ **2ç§ä¸»è¦æ„å»ºæ¨¡å‹çš„æ–¹å¼**ï¼šSequential()ï¼Œå‡½æ•°å¼APIã€‚çµæ´»ä½¿ç”¨å‡½æ•°å¼APIï¼Œç†è®ºä¸Šå¯ä»¥æ„å»ºä»»ä½•æ¨¡å‹ï¼Œä¸å•å•æ˜¯ç¥ç»ç½‘ç»œã€‚è€ŒPinnä½¿ç”¨Sequential()å³å¯ã€‚æ¯”å¦‚ï¼Œæˆ‘çš„ä»£ç å°±æ˜¯è®©MyPinnç»§æ‰¿keras.Sequential

   è€Œåœ¨**code**ä¸­ï¼ŒPinnç»§æ‰¿çš„æ˜¯keras.Moduleï¼ŒModuleå®é™…ä¸Šæ˜¯Sequentialç­‰é«˜é˜¶APIçš„åŸºç±»ï¼Œå®ƒæä¾›çš„åŠŸèƒ½æ˜¯è®°å½•åœ¨ç±»ä¸­å‡ºç°çš„Variablesï¼Œè€Œæ²¡æœ‰Sequentialä¸­æ›´å¤šå¥½ç”¨æ–¹ä¾¿çš„åŠŸèƒ½ï¼Œæ¯”å¦‚ç›´æ¥ä½¿ç”¨addæ–¹æ³•ï¼Œæ­é…keras.layers.Denseï¼Œä¸ºMyPinnæ·»åŠ å±‚(æœ¬è´¨ä¸Šä¹Ÿæ˜¯æ·»åŠ Variables)ã€‚

   è‹¥ä½¿ç”¨**code**ä¸­çš„æ–¹æ³•ï¼Œè®©Pinnå•å•ç»§æ‰¿Moduleï¼Œåˆ™éœ€è¦è‡ªå·±åšå˜é‡åˆå§‹åŒ–ï¼Œå¹¶ä¸”æ²¡æœ‰åˆ«çš„å¥½ç”¨çš„åŠŸèƒ½ã€‚å®ƒåªèƒ½è®°å½•classä¸­å‡ºç°çš„å˜é‡è€Œå·²ã€‚

   è¿™æ˜¯åŸå› äºŒã€‚
3. **ä¼˜åŒ–å™¨**ã€‚åœ¨codeä¸­ä¸ºäº†ä½¿ç”¨LBFGSï¼Œåœ¨ç±»ä¸­å®šä¹‰äº† â€œå˜é‡ä¸€ç»´åŒ–ã€ä¸€ç»´å˜é‡è½¬ä¸ºåŸæ¥å½¢çŠ¶â€ç­‰ ä¸“é—¨ä¸ºä½¿ç”¨LBFGSçš„å‡½æ•°ã€‚æˆ‘ä¸å–œæ¬¢è¿™æ ·å­ã€‚

   **ä¸ºäº†æ¸…æ™°æ€§ï¼Œæˆ‘æ›´å€¾å‘äºå°†æ¨¡å‹å®šä¹‰å’Œä¼˜åŒ–æ–¹æ³•åˆ†ç¦»å¼€ï¼Œè€Œä¸æ˜¯å…¨åœ¨å†™åœ¨classä¸­ã€‚**

   â€œå˜é‡ä¸€ç»´åŒ–ã€ä¸€ç»´å˜é‡è½¬ä¸ºåŸæ¥å½¢çŠ¶â€è¿™æ ·çš„å‡½æ•°ï¼Œåªæ˜¯ä¸ºä½¿ç”¨LBFGSæœåŠ¡çš„ï¼Œä»–åº”è¯¥å®šä¹‰åœ¨å¤–é¢ã€‚å¦‚æœä½¿ç”¨å†…ç½®çš„Adamã€SDGç­‰ä¼˜åŒ–å™¨ï¼Œå®Œå…¨ä¸éœ€è¦å®šä¹‰è¿™äº›å‡½æ•°ï¼Œç›´æ¥åœ¨train_step()ä¸­ï¼Œoptimizer.apply_gradientï¼ˆï¼‰å³å¯ã€‚

   è¿™æ˜¯åŸå› ä¸‰ã€‚

æ‰€ä»¥bugå‡ºç°åœ¨å“ªå‘¢ï¼Ÿ@tf.function ç¼–ç¨‹ä¸è§„èŒƒï¼Ÿ å˜é‡åå†™é”™ï¼Ÿ

**åœ¨loss_PDEçš„å®šä¹‰ä¸­ã€‚**

ä¸‹é¢æˆ‘åˆ†åˆ«ç»™å‡ºæˆ‘è‡ªå·±çš„å®šä¹‰ä»¥åŠcodeçš„å®šä¹‰

```python
def loss_PDE(self,X_f_train): #æˆ‘çš„å®šä¹‰
  
      x = X_f_train[:,0]  # x.shape =(nums,)
      t = X_f_train[:,1]  # t.shape =(nums,)
      with tf.GradientTape(persistent=True) as tape:
          tape.watch([x,t])
          X = tf.stack([x,t],axis=-1) # X.shape = (nums,2)
          u = self(X)  # predict u
          u_x = tape.gradient(u,x)
```

```python
def loss_PDE(self,X_f_train): #codeçš„å®šä¹‰
  
      x = X_f_train[:,0:1] # x.shape = (nums,2)
      t = X_f_train[:,1:2] # t.shape = (nums,2)
      with tf.GradientTape(persistent=True) as tape:
          tape.watch([x,t])
          X = tf.stack([x[:,0],t[:,0]],axis=1) #X.shape = (nums,2)
          u = self(X)  # predict u
          u_x = tape.gradient(u,x)
```

ä¸¤ç§ä»£ç çš„ç¬¬3è¡Œå’Œç¬¬4è¡Œï¼Œå¹²çš„éƒ½æ˜¯åŒä¸€ä»¶äº‹ï¼ŒæŠŠX_f_trainçš„ x å’Œ t åˆ†ç¦»å‡ºæ¥ï¼ˆåé¢è¦å¯¹xï¼Œtæ±‚å¯¼ï¼‰ã€‚

```python
with tf.GradientTape(persistent=True) as tape:
	....
```

ä¸Šè¿°å°†è®¡ç®—è¿‡ç¨‹è®°å½•åœ¨â€œtapeâ€ä¸­ï¼Œåé¢å¯ä»¥æ±‚tapeä¸­å‡ºç°å˜é‡ï¼ˆåŒ…æ‹¬ä¸­é—´å˜é‡ï¼‰çš„å¯¼æ•°ã€‚

tape.watch([x,t])å°†xï¼Œtä¹Ÿè®°å½•åˆ°tapeä¸­ï¼Œå› ä¸ºé»˜è®¤åªè®°å½•variablesï¼Œè€Œæ­¤å¤„xå’Œfæ˜¯constantã€‚

æˆ‘ä»¬å°†x,té€šè¿‡tf.stack()æ‹¼æ¥åˆ°ä¸€èµ·ç»„æˆæ¨¡å‹çš„inputï¼šXï¼Œè°ƒç”¨self(X) predict uã€‚tf.stackä¹Ÿä¼šè¢«çœ‹åšä¸€ç§ç®—å­ï¼Œè¢«è®°å½•åˆ°tapeä¸­ã€‚

ä½ æˆ–è®¸å‘ç°Xå’Œloss_PDEå‡½æ•°ä¼ å…¥çš„X_f_train**æ•°å€¼ä¸Šä¸€æ ·**ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥self(X_f_trian)å‘¢ï¼Ÿ**å› ä¸ºå®ƒä»…ä»…æ˜¯æ•°å€¼ä¸Šä¸€æ ·ã€‚ã€‚ã€‚**

å¯ä»¥è¯•ä¸€ä¸‹æ”¹ç”¨u=self(X_f_train)ï¼Œé‚£ä¹ˆu_xè¿”å›çš„Noneã€‚

ä»¤äººæˆ‘æ„Ÿåˆ°å¥‡æ€ªçš„æ˜¯ï¼Œä¸¤ç§ä»£ç çš„ç¬¬7æ­¥ï¼Œæœ€åè¿”å›çš„Xæ˜¯ä¸€æ ·çš„ï¼Œä¸ºä»€ä¹ˆåªæœ‰ç¬¬äºŒç§ï¼ˆcodeçš„å®ç°ï¼‰OKï¼Ÿè€Œæˆ‘çš„å°±ä¸è¡Œã€‚

å…¶å®æˆ‘åœ¨å†™ä»£ç çš„æ—¶å€™ï¼Œå‚è€ƒäº†codeçš„loss_PDEï¼Œè§‰å¾—ä»–çš„æ“ä½œæœ‰ç‚¹**å¤šä½™**ï¼Œå°±ç”¨äº†ä¸€ç§æ›´æ¸…æ¥šçš„å†™æ³•ï¼Œæ­£å¦‚ç¬¬ä¸€ç§å†™æ³•é‚£æ ·ï¼Œç„¶è€Œè®­ç»ƒçš„ç»“æœå°±æ˜¯ï¼Œç¬¬ä¸€ç§ä¸è¡Œï¼ï¼Ÿ

**ä¸‹é¢æˆ‘å†éªŒè¯ä¸€ä¸‹ä¸¤ç§å†™æ³•çš„(ç¬¬7è¡Œ)Xæ˜¯å¦ä¸€è‡´?**

<img src="./Data/bug.png" style="zoom:75%;" />

ç»“æœæ˜¾ç¤º g å’Œ gg ï¼Œç¡®å®æ˜¯ä¸€æ ·çš„ï¼ˆå…¨æ˜¯Trueï¼Œæ²¡æœ‰æˆªå®Œå…¨)ï¼Œä»¤äººç–‘æƒ‘ã€‚

**ä¸ç®¡äº†ã€‚åé¢æˆ‘ä½¿ç”¨äº†codeçš„å†™æ³•.æ¨¡å‹çš„losså‡½æ•°èƒ½ä¸æ–­ä¸‹é™äº†ï¼Œæ•ˆæœè§7_4-7_5MyPINN_Burgers.ipynb**

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-5 markdown

---

# 07-06

ä»Šå¤©é˜…è¯»å­¦ä¹ äº†ä¸€äº›NNåœ¨PDEæ±‚è§£é—®é¢˜ä¸­çš„å¤šç§æ–¹æ³•ã€‚

### ç¥ç»ç½‘ç»œå¸¸è§ç»“æ„

<img src='./Data/NN_structures.png' style="zoom:50%;"  >

**ç›®å‰PINNå¤§å¤šä¸ºå…¨è¿æ¥å‰å‘ç½‘ç»œ(tensorflowä¸­çš„Denseå±‚)ï¼Œå°è¯•ä½¿ç”¨å¦‚å·ç§¯å±‚ã€å¾ªç¯ç½‘ç»œã€åé¦ˆç½‘ç»œï¼Ÿ**

### ç¥ç»ç½‘ç»œå¯¹è¾¹ç•Œæ¡ä»¶çš„å¤„ç†æ–¹å¼

ANN: artificial neural network

1. **çº¯ç½‘ç»œå‹**
2. 1  Loss = $\lambda_1 * MSE_{pde} + \lambda_2 * MSE_{BC}$  ï¼Œ åŸå§‹çš„PINNæ¨¡å‹å°±ä½¿ç”¨çš„æ˜¯è¿™ç§æ–¹å¼ï¼Œåˆ©ç”¨è¾¹ç•Œæ¡ä»¶å’Œé¢„æµ‹å€¼è®¡ç®—$MSE_{BC}$,çº³å…¥lossä¸­ã€‚
3. 2  å°†è¾¹ç•Œæ¡ä»¶å¸¦å…¥ç¥ç»ç½‘ç»œè¡¨è¾¾å¼â€”â€”å–ä»£éƒ¨åˆ†weightsï¼Œé€šè¿‡å¾®åˆ†æ–¹ç¨‹æ®‹å·®$MSE_{pde}$ä¼˜åŒ–å‰©ä½™æƒå€¼ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src='./Data/BC_solution1.png' style="zoom:70%;" >

    **ä½†æˆ‘è®¤ä¸ºï¼Œå¯¹äºæŸä¸€ä¸ªå¤æ‚pdeï¼Œä»£ç å®ç°è¿™ç§æ€æƒ³å¹¶ä¸å®¹æ˜“ã€‚**

2. **è¾¹ç•Œå¸æ”¶å‹**

   è¾¹ç•Œå¸æ”¶å‹çš„æ€æƒ³æ˜¯ï¼šæŠŠç¥ç»ç½‘ç»œçœ‹åšå‡½æ•° ANN(X)ï¼›æ„é€ è¾¹ç•Œå‡½æ•°BC(X)ï¼šå½“Xâˆˆè¾¹ç•Œæ—¶ï¼ŒBCä¸ºè¾¹ç•Œå€¼ï¼Œå¦åˆ™ä¸º0ï¼›æ„é€  L(X)ï¼Œå½“Xâˆˆè¾¹ç•Œæ—¶ï¼ŒL(X)=0.

ä»¤è¯•è§£  $y_t = BC(X) + L(X) * ANN(X) $, æ­¤å‡½æ•°ä¸¥æ ¼æ»¡è¶³è¾¹ç•Œæ¡ä»¶ã€‚å†é€šè¿‡åŸŸå†…ç‚¹è®¡ç®—$MSE_{pde}$ï¼Œæ›´æ–°ANNã€‚

    **BC(X),L(X)çš„æ„é€ æ–¹æ³•ï¼š**

    `<img src='./Data/BC_ODE.png'>`

<img src='./Data/BC_PDE.png'>

**åœ¨PDEä¸­å¦‚ä½•è§£é‡ŠANN(X)çš„ä½œç”¨ï¼Ÿ** é€šç”¨BC(X),L(X)çš„æ„é€ æ–¹æ³•ï¼Ÿ

è¿›ä¸€æ­¥ï¼Œä½¿ç”¨ç‹¬ç«‹ç½‘ç»œ$ANN_{BC}$ä»£æ›¿BC(X)ã€‚

<img src='./Data/Duo_NN.png' style="zoom:50%;" >

æœ‰äº†è¿™ç§ç‹¬ç«‹ç½‘ç»œçš„æ€æƒ³ï¼Œä¹Ÿæœ‰äººæå‡ºå¤šç½‘ç»œçš„æ¨¡å‹ã€‚å³ç”¨ç‹¬ç«‹çš„ç½‘ç»œåˆ†åˆ«é¢„æµ‹PDEä¸­çš„å„ç§åå¯¼ï¼Œæœ€åæŠŠlossåŠ åœ¨ä¸€èµ·ã€‚

---

# 07-07

ä»Šå¤©ä¸»è¦é˜…è¯»å­¦ä¹ äº†å¼ å­¦é•¿çš„è®ºæ–‡ï¼Œå…³äºä½¿ç”¨PINNæ±‚è§£Navier-Stokes/Darcyè€¦åˆæ¨¡å‹çš„æ­£åé—®é¢˜ã€‚

## æ­£é—®é¢˜ä¸åé—®é¢˜

æ­£é—®é¢˜ï¼š

å·²çŸ¥pdeæ–¹ç¨‹ç»„ï¼Œåˆè¾¹å€¼æ¡ä»¶ï¼Œæ±‚è§£æè§£æˆ–è€…æ•°å€¼è§£ã€‚

åé—®é¢˜ï¼š

å·²çŸ¥pdeæ–¹ç¨‹ç»„ï¼Œæˆ–ä¸çŸ¥é“åˆè¾¹å€¼æ¡ä»¶ï¼Œæˆ–ä¸çŸ¥é“æ¨¡å‹ä¸­æŸäº›å‚æ•°çš„å€¼ï¼Œå–è€Œä»£ä¹‹ï¼ŒçŸ¥é“åŒºåŸŸå†…éƒ¨çš„ä¸€äº›æ•°å€¼çœŸè§£ï¼Œåæ¨æ•´ä¸ªåŒºåŸŸçš„è§£æˆ–è€…æ¨¡å‹å‚æ•°ã€‚

---

## æ±‚è§£æ€è·¯

**å¯¹äºæ­£é—®é¢˜ï¼š**

æ„å»ºä¸¤ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œ$NN_1 å’Œ NN_2$ï¼Œä½†æ˜¯ä»–ä»¬çš„è¾“å…¥å±‚inputæ˜¯ç›¸åŒçš„ï¼Œ$NN_1$ç”¨äºæ±‚è§£Naiver-Stokesæ–¹ç¨‹$U_1$ï¼Œ$NN_2$ç”¨äºæ±‚è§£$U_2$,æ ¹æ®Navier-Stokes/Darcyè€¦åˆæ¨¡å‹ä¸­çš„pdeæ–¹ç¨‹å’Œåˆè¾¹å€¼æ¡ä»¶ï¼Œç”¨$U_1 å’Œ U_2$æ„é€ æ®‹å·®ï¼ŒåŠ åœ¨ä¸€èµ·ï¼Œç»„æˆlossã€‚è®­ç»ƒç¥ç»ç½‘ç»œã€‚

**å¯¹äºåé—®é¢˜ï¼š**

å‡è®¾ï¼Œåˆè¾¹å€¼æ¡ä»¶æœªçŸ¥ï¼Œä½†çŸ¥é“ä¸€äº›å†…éƒ¨åŒºåŸŸçš„æ•°å€¼è§£ã€‚åªéœ€è¦æŠŠlossä¸­çš„åˆè¾¹å€¼MSEå»æ‰ï¼Œæ”¹æˆå…³äºå†…éƒ¨åŒºåŸŸçš„æ•°å€¼å€¼çš„MSEã€‚å…¶å®åœ¨PINNä¸­ï¼Œåˆè¾¹å€¼å’Œå†…éƒ¨åŒºåŸŸçš„æ•°å€¼è§£ å¯ä»¥çœ‹åšæ˜¯åŒç­‰ä½ä½çš„ã€‚

å‡è®¾ï¼Œæ–¹ç¨‹ç»„ä¸­æŸäº›ç‰©ç†å‚æ•°å€¼æœªçŸ¥ã€‚åªéœ€è¦æŠŠå®ƒä»¬è®¾ç½®ä¸ºVariablesï¼Œçº³å…¥lossï¼Œä¸NNçš„å‚æ•°ä¸€èµ·è®­ç»ƒã€‚

ä¸å¾—ä¸è¯´ï¼ŒNative-stokes/Darcyè€¦åˆæ¨¡å‹çš„æ–¹ç¨‹ç»„è¿˜æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œå¼ å­¦é•¿çš„è®ºæ–‡ä¸­ï¼Œä½¿ç”¨2då…·æœ‰è§£æè§£çš„ç®—ä¾‹ï¼Œä½¿ç”¨æœ€åŸå§‹çš„PINNæ¨¡å‹ç»“æ„æ±‚è§£ï¼Œæ•ˆæœè¿˜ç®—ä¸é”™ã€‚

ä½†å½“é—®é¢˜å˜æˆ3dï¼Œlossæ›´åŠ å¤æ‚æ—¶ï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œç”±äºé«˜é˜¶å¯¼æ•°çš„å­˜åœ¨ï¼Œä½¿ç”¨è¿™ç§æœ€åŸå§‹çš„PINNæ¨¡å‹è§£Navier-Stokes/Darcyè€¦åˆæ¨¡å‹ï¼Œåº”è¯¥å¾ˆæ…¢ã€‚

ä»NNçš„æ¨¡å‹ç»“æ„å’Œè®­ç»ƒç­–ç•¥ä¸Šåšä¸€äº›è°ƒæ•´ï¼Œåº”è¯¥ç®—æ˜¯æ¯”è¾ƒè‡ªç„¶çš„ä¼˜åŒ–ç­–ç•¥ã€‚

---

# 07-08

## å°æ‰¹é‡è®­ç»ƒæ¨¡å¼å®éªŒ

    é˜…è¯»ä¸€ç¯‡è®ºæ–‡ï¼Œå…³äºpdeè€¦åˆæ¨¡å‹çš„æ•°å€¼æ±‚è§£æ–¹æ³•ã€‚

    PARTITIONED TIMESTEPPING FOR A PARABOLIC TWO DOMA.pdf

    è¯•äº†ä¸‹å°æ‰¹é‡è®­ç»ƒã€‚å°†å…¨éƒ¨è®­ç»ƒæ•°æ®**nç­‰åˆ†å**ï¼Œè¿›è¡Œå°æ‰¹é‡è®­ç»ƒã€‚åœ¨ç›¸åŒçš„epochsä¸‹ï¼Œå°æ‰¹é‡è®­ç»ƒæ•ˆæœæ¯”åŸæ¥å¥½ã€‚æ•ˆæœè§"7_8_myPINN_Burgers.ipynb"

---

# 07-11

## PINNæ±‚è§£parabolicè€¦åˆpdeæ¨¡å‹

    ç”¨PINNæ±‚è§£æœ€ç®€å•çš„parabolicè€¦åˆpdeæ¨¡å‹â€”â€”PARTITIONED TIMESTEPPING FOR A PARABOLIC TWO DOMA.pdf

<img src='./Data/å…¬å¼0.png'>

<img src='./Data/å…¬å¼1.png'>

æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒæ¨¡å‹çš„ä»£ç å†™å¥½äº†ï¼Œç”»å›¾çš„è¿˜æ²¡å†™ã€‚è®­ç»ƒæ•°æ®çš„ç”Ÿæˆä»£ç ï¼Œå†™çš„æœ‰äº›å†—é•¿ï¼Œåé¢ä¼šä¼˜åŒ–ä¸€ä¸‹ã€‚

ä»£ç è§ **7_11_Parabolicè€¦åˆpdeæ¨¡å‹.ipynb**

**ç½‘ç»œç»“æ„:**

ä¸¤ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œ$NN_1 , NN_2$ï¼Œåˆ†åˆ«ç”¨äºé¢„æµ‹$u_1,u_2$, æ„é€  $ loss = loss_{u1} + loss_{u2} + loss_{interface}$ï¼Œæ¯æ¬¡è®­ç»ƒåŒæ—¶è®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼Œè€¦åˆæ€§ä½“ç°åœ¨$loss_{interface}$.

<img src='./Data/coupleNN.png' style='zoom:50%'>

æ¯æ¬¡è®­ç»ƒæ¨¡å‹ï¼Œ$NN_1 å’Œ NN_2 $å„è‡ªä¼ å…¥ä¸€æ‰¹"ä¸åŒçš„" å†…éƒ¨ç‚¹è®­ç»ƒé›† å’Œ è¾¹ç•Œç‚¹è®­ç»ƒé›†(ä¸åŒ…æ‹¬interface)ã€‚ $NN_få’ŒNN_p$åœ¨interfaceå¤„çš„è®­ç»ƒé›†ã€‚

---

# 07-12

## è®­ç»ƒparabolicè€¦åˆpdeçš„PINNæ¨¡å‹

    u1æ‹Ÿåˆçš„æ¯”è¾ƒå¥½ã€‚u2æ‹Ÿåˆæ•ˆæœå¾ˆå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾¹ç•Œå¤„ã€‚

    æ­£åœ¨ç ”ç©¶ï¼Œä¸çŸ¥æ˜¯ä»£ç æœ‰é”™ï¼Œè¿˜æ˜¯è¯´å› ä¸ºu2è¡¨è¾¾å¼æ¯”è¾ƒå¤æ‚ï¼Œæœ‰yçš„äºŒæ¬¡é¡¹ã€‚

    è®­ç»ƒä»£ç è§**7_11_Parabolicè€¦åˆpdeæ¨¡å‹.ipynb**

---

# 07-13

## **ä¸2ä½å­¦é•¿ä¼šè®®äº¤æµï¼Œè®¨è®ºPINN**

è§£å†³äº†ä¸å°‘ç–‘é—®ï¼ŒPINNåœ¨è¾¹ç•Œå¤„çš„æ‹Ÿåˆæ•ˆæœç¡®å®ä¸€èˆ¬ã€‚

`<img src = './Data/ä¼šè®®.png'>`

---

# 07-15â€”07-17

## **æ”¹è¿›parabolicè€¦åˆpdeçš„ä»£ç ã€‚**

æ”¹è¿›æ–¹æ¡ˆå¦‚ä¸‹ï¼š

1. ä¸åŒå­¦è®¨è®ºå‘ç°ï¼Œä¹‹å‰çš„é‡‡æ ·ç‚¹ä¸ºç­‰åˆ†ï¼Œä¸å¤Ÿâ€œéšæœºâ€ï¼Œä½¿ç”¨normalæˆ–è€…æ‹‰ä¸é«˜æ¬¡æ–¹é‡‡æ ·æ•ˆæœæ›´å¥½
2. è®­ç»ƒæ¬¡æ•°ä¸è¶³ï¼Œå¢åŠ è®­ç»ƒæ¬¡æ•°
3. å°†è¾¹ç•Œç‚¹ä¹ŸåŒæ—¶çº³å…¥å†…éƒ¨ç‚¹è®­ç»ƒï¼Œå¯¼è‡´åœ¨è¾ƒå°‘è®­ç»ƒæ¬¡æ•°ä¸‹ï¼Œè¾¹ç•Œå¤„æ•ˆæœä¸å¥½
4. è€¦åˆé˜¶æ®µè®­ç»ƒç»“æŸåï¼Œç»§ç»­å¯¹ä¸¤ä¸ªåŒºåŸŸåˆ†å¸ƒè¿›è¡Œå•ç‹¬çš„PINNè®­ç»ƒï¼Œä½¿å¾—ç²¾å‡†åº¦è¿›ä¸€æ­¥æå‡

æ•ˆæœè§ **7_15_æ”¹è¿›ç‰ˆParabolicè€¦åˆpde.ipynb**

---

# 07-18

## ä»£ç  & è®ºæ–‡é˜…è¯»

    å­¦ä¹ TensorFlow2.0 Metricè¯„ä¼°å‡½æ•° ï¼Œä»£ç è§â€œ**tensorflowå­¦ä¹ è®°å½•/12_Metric.ipynb**â€

    é˜…è¯» [Deep Learning-An Introduction](../è®ºæ–‡èµ„æ–™/Deep Learning-An Introduction.pdf )ã€‚è¿™ç¯‡æ–‡ç« ä»æ•°å­¦è§’åº¦ï¼Œä»é›¶å¼€å§‹ä»‹ç»Deep Learningï¼Œæ˜¯ä¸€ç¯‡ä»‹ç»æ€§çš„æ–‡ç« ã€‚

---

# 07-20

## Self-Adaptive-Weight c-PINN

**ä»Šå¤©å¯¹parabolicè€¦åˆpdeçš„PINNæ¨¡å‹è¿›ä¸€æ­¥æ”¹è¿›**ã€‚ä¸»è¦æœ‰å¦‚ä¸‹ä¸¤ä¸ªæ–¹é¢ï¼š

1. é¢„è®­ç»ƒæ¨¡å¼
2. è‡ªé€‚åº”losså‡½æ•°å› å­

**é¢„è®­ç»ƒæ¨¡å¼**ï¼šåœ¨è®­ç»ƒè€¦åˆæ¨¡å‹å‰ï¼Œå…ˆå„ç§å•åŒºåŸŸè¿›è¡ŒPINNè®­ç»ƒã€‚æœ‰åˆ©äºè€¦åˆæ¨¡å‹è¾¹ç•Œè®­ç»ƒï¼Œä»¥åŠè®­ç»ƒæ•ˆç‡ã€‚

**è‡ªé€‚åº”losså‡½æ•°å› å­ï¼š**

$$
Loss = \alpha_1 * loss_{u1} + \alpha_2 * loss_{u2} \\
loss_{u1} := loss_{u1}^{bc} + loss_{u1}^{f} + loss_{u1}^{i} \\
loss_{u2} := loss_{u2}^{bc} + loss_{u2}^{f} + loss_{u2}^{i} \\
$$

    å…¶ä¸­$\alpha_i$å°±æ˜¯è‡ªé€‚åº”å› å­ã€‚

    è€ƒè™‘åˆ°å®é™…è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œu1å’Œu2çš„losså¤§å°ä¸ä¸€æ ·ï¼Œ**â€ä¼˜å…ˆâ€œ**è®­ç»ƒlossè¾ƒå¤§çš„ä¸€æ–¹ï¼Œå³åœ¨$loss_{ui}$å‰ä¹˜ä¸Šä¸€ä¸ªè¾ƒå¤§çš„å› å­ï¼Œä½¿å…¶åœ¨æ•´ä¸ª**$Loss$**ä¸­å æ¯”æ›´å¤§ï¼Œä»è€Œè¾¾åˆ°ä¼˜å…ˆè®­ç»ƒçš„ç›®æ ‡ã€‚

### **ä»€ä¹ˆæ˜¯è‡ªé€‚åº”æƒé‡ Self-Adaptive-Weightï¼Ÿ**

    æŠŠ$\alpha_1,\alpha_2$ä¹Ÿçœ‹åšå˜é‡ã€‚åœ¨è®­ç»ƒæ¨¡å‹å‚æ•°çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ˜¯åŸºäºâ€œè´Ÿæ¢¯åº¦â€ã€‚

    å¦‚æœä½¿ç”¨**â€œæ­£æ¢¯åº¦â€**å»æ”¹å˜$\alpha_1,\alpha_2$ï¼Œèƒ½å¤Ÿä½¿å¾—$loss_{ui}$å¯¹åº”çš„$\alpha_{i}$æ›´å¤§ã€‚

    å®é™…ä¸Šï¼Œä½¿ç”¨è¿™ç§ç­–ç•¥ï¼Œä¸æ–­åœ°è®­ç»ƒä¼šä½¿å¾—$\alpha$ä¸€ç›´å¢å¤§ï¼ŒåŒæ—¶ä¸ºäº†æ§åˆ¶$\alpha$çš„å€¼ï¼Œå¯ä»¥å¥—ä¸€å±‚sigmoidå‡½æ•°ï¼Œä½¿å¾—$\alpha$æ§åˆ¶åœ¨0-1ä¹‹é—´ã€‚$åˆå§‹åŒ–\alpha=0ï¼Œ\alpha=tf.math.sigmoid(\alpha)$,

### åŠ æƒç­–ç•¥

    å¯¹$loss_{u1} å’Œ loss_{u2}$ åŠ æƒçš„**ç›®çš„**ï¼šä½¿å¾—æŸå¤±è¾ƒå¤§çš„ä¸€æ–¹åœ¨æ•´ä¸ªlossä¸­çš„è´¡çŒ®æ›´å¤§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå€¾å‘äºè®­ç»ƒæŸå¤±æ›´å¤§çš„ä¸€æ–¹ã€‚

    å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœä¸¤ä¸ªç¥ç»ç½‘ç»œä¹‹é—´æ²¡æœ‰è”ç³»ï¼Œå³$loss_{u1}(x_1;\theta_1) , loss_{u2}(x_2;\theta_2) $çš„è‡ªå˜é‡$(x_1,\theta_1),(x_2,\theta_2)$ä¹‹é—´æ²¡æœ‰é‡åˆçš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆå¯¹$loss_{u1} å’Œ loss_{u2}$ åŠ æƒå®é™…ä¸Šæ˜¯æ²¡æœ‰"æ•ˆæœ"çš„ã€‚

    åŸå› æ˜¯ï¼Œå¦‚æœä¸¤ä¸ªç¥ç»ç½‘ç»œä¹‹é—´æ²¡æœ‰è”ç³»æ—¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯¹$Loss = \alpha_1 * loss_{u1} + \alpha_2 * loss_{u2}$æ±‚å…³äº$\theta_{1}$çš„å¯¼æ•°ï¼Œ$\frac{\partial loss}{\partial \theta_1} =\alpha_1 *  \frac{\partial loss_{u1}}{\partial \theta_1}$ï¼Œå¯ä»¥å‘ç°ä¸$\theta_2$æ— å…³ï¼Œå³è·Ÿç¬¬äºŒä¸ªç¥ç»ç½‘ç»œæ— å…³ï¼Œåªæ˜¯åœ¨è®­ç»ƒå•ä¸ªç¥ç»ç½‘ç»œè€Œå·²ï¼Œè€Œå¯¹å•ä¸ªç¥ç»ç½‘ç»œçš„lossä¹˜ä»¥ä¸€ä¸ªæ•°ï¼Œå®é™…æ˜¯æ²¡æœ‰ç”¨çš„ï¼Œç›¸å½“äºå¯¹ä¼˜åŒ–é—®é¢˜ä¸­ç›®æ ‡å‡½æ•°ä¹˜ä¸Šä¸€ä¸ªå¸¸æ•°ï¼Œæ˜¾ç„¶ä¸å½±å“æˆ‘ä»¬å¯»æ‰¾æœ€ä¼˜è§£ã€‚

    å› æ­¤ï¼Œæ­¤å¤„çš„$loss_{u1},loss_{u2}$å…·ä½“ä¸ºï¼š

$$
loss_{u1} := loss_{u1}^{bc} + loss_{u1}^{f} + loss_{u1}^{i} \\
loss_{u2} := loss_{u2}^{bc} + loss_{u2}^{f} + loss_{u2}^{i} \\
$$

å…¶ä¸­$loss_{u1}^{i}$æ˜¯u1åœ¨äº¤ç•Œå¤„Interface çš„æŸå¤±å‡½æ•°ï¼Œä¸u1,u2æœ‰å…³ï¼Œå³ä¸$\theta_{1},\theta_{2}$æœ‰å…³ï¼Œå®ƒä½¿å¾—ä¸¤ä¸ªç¥ç»ç½‘ç»œè”ç³»åœ¨ä¸€èµ·ã€‚å½“$loss_{u1}^{i}$æƒé‡æ›´å¤§æ—¶ï¼Œæ¨¡å‹é€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°ä¸¤ä¸ªç½‘ç»œçš„å‚æ•°$\theta_{1},\theta_{2}$,ä¼šæ›´å€¾å‘äºä½¿å¾—$loss_{u1}$æ›´å°ã€‚

åœ¨å®é™…æ“ä½œä¸­ï¼Œæ— è®ºæ˜¯å•åŒºåŸŸçš„PINNå’Œè€¦åˆçš„PINNï¼Œåœ¨è¾¹ç•Œå¤„çš„æ‹Ÿåˆæ•ˆæœç›¸è¾ƒäºå†…éƒ¨çš„æ‹Ÿåˆæ•ˆæœæ›´å·®ã€‚

å¾€å¾€åœ¨$loss_u^{bc}$å‰ä¹˜ä¸Šä¸€ä¸ªå¸¸æ•°kï¼Œæ¯”å¦‚k=10ï¼š

$$
Loss = \alpha_1 * loss_{u1} + \alpha_2 * loss_{u2} \\
loss_{u1} := 10* loss_{u1}^{bc} + loss_{u1}^{f} + loss_{u1}^{i} \\
loss_{u2} := 10* loss_{u2}^{bc} + loss_{u2}^{f} + loss_{u2}^{i} \\
$$

### å¤šç§è‡ªé€‚åº”åŠ æƒç­–ç•¥

é™¤äº†é‚£ç§è‡ªé€‚åº”æƒé‡ Self-Adaptive-Weightä¹‹å¤–ï¼Œæˆ‘è¿˜æ„é€ äº†ä¸€ç§æ–°çš„ã€‚

ä»¤:

$$
Loss^{(k)} = loss_{u1}^{(k)} + \alpha^{(k)} loss_{u2}^{(k)} ,\alpha^{(0)} = 1 \\

 \alpha^{(k+1)} = \frac{loss_{u2}^{(k)} }{loss_{u1}^{(k)} + eps}, å…¶ä¸­ epsæ˜¯ä¸€ä¸ªå¾ˆå°çš„æ­£æ•°ï¼Œé˜²æ­¢åˆ†æ¯ä¸º0\\
$$

æƒ³æ³•å°±æ˜¯ï¼Œå½“å‰æ­¥çš„æƒé‡$\alpha^{(k)}$æ˜¯æ ¹æ®ä¸Šä¸€æ­¥$loss_{u1}^{(k-1)},loss_{u2}^{(k-1)}$çš„æ¯”å€¼æ¥è°ƒæ•´,å½“$\alpha^{(k)}$>1æ—¶ï¼Œè¡¨ç¤ºä¸Šä¸€æ­¥loss_u2çš„å€¼æ›´å¤§ï¼Œæ•…åœ¨å½“å‰æ­¥ï¼Œè®©loss_u2ä¹˜ä¸Š$a^{(k)}>1$,ä½¿å¾—å½“å‰æ­¥loss_u2ä¸‹é™å¾—æ›´å¿«ã€‚

è¿™ç§â€è‡ªé€‚åº”â€œç­–ç•¥æœ‰å¾ˆå¤šï¼Œæƒ³æ€ä¹ˆæ„é€ å°±æ€ä¹ˆæ„é€ ï¼Œè¦æŠ“çš„ç‚¹å°±æ˜¯æ ¹æ®ä»¥å‰çš„lossï¼ŒåŠ¨æ€åœ°è°ƒæ•´æƒé‡ï¼Œä½¿å¾—å½“å‰æ­¥å€¾å‘è®­ç»ƒäºå‰Næ­¥ä¸­è¾ƒå¤§çš„ **å­loss** ã€‚

---

# 07-21

## ä¼˜åŒ–è®­ç»ƒæ­¥éª¤

1. é¢„è®­ç»ƒâ€”â€”å•åŒºåŸŸè®­ç»ƒ
2. è€¦åˆè®­ç»ƒâ€”â€”Adamç®—æ³•
3. è€¦åˆè®­ç»ƒâ€”â€”LBFGSç®—æ³•

**ä»£ç è§0721_è‡ªé€‚åº”&LBFGS_Parabolicè€¦åˆæ¨¡å‹.ipynb** `<br />`æ¯”è¾ƒAdamç®—æ³•å’ŒLBFGSç®—æ³•çš„è®­ç»ƒè¡¨ç°ã€‚`<br />`ï¼ˆæœ‰å¿…è¦æ·±å…¥äº†è§£Adamçš„æ€§è´¨ï¼Œåœ¨è®­ç»ƒåæœŸè¡¨ç°è¿œä¸å¦‚LBFGSï¼‰

# 07-22

## åŒºåŸŸåé—®é¢˜

    æµä½“åŠ›å­¦é¢†åŸŸè¿˜å­˜åœ¨å„ç§å„æ ·çš„åé—®é¢˜ï¼Œæ¯”å¦‚ç‰©ç†æ¨¡å‹çš„åˆè¾¹å€¼ æ¡ä»¶æ˜¯æœªçŸ¥çš„ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯å·²çŸ¥å†…éƒ¨éƒ¨åˆ†åŒºåŸŸæˆ–éƒ¨åˆ†ç‰©ç†é‡çš„æ•°å€¼çœŸè§£ï¼Œä»¥æ­¤ åæ¨æ•´ä¸ªåŒºåŸŸçš„æµä½“è¿åŠ¨æƒ…å†µï¼›æˆ–è€…ï¼Œç‰©ç†æ¨¡å‹çš„æ–¹ç¨‹æœ¬èº«å…·æœ‰ä¸€äº›æœªçŸ¥å‚æ•°ï¼Œ éœ€è¦é€šè¿‡çœŸå®çš„æ•°å€¼ç»“æœè¿›è¡Œåæ¨ã€‚è¿™ç±»é—®é¢˜åœ¨å·¥ç¨‹åº”ç”¨ä¸­å…·æœ‰å¾ˆå¤§æ„ä¹‰ï¼Œç„¶è€Œ å„ç§ä¼ ç»Ÿæ–¹æ³•å¯¹æ­¤ç±»é—®é¢˜çš„æ±‚è§£å…·æœ‰ä¸€å®šçš„éš¾åº¦ï¼Œåœ¨æœ¬æ–‡ç¥ç»ç½‘ç»œæ±‚è§£çš„æ¡†æ¶ ä¸‹ï¼Œå´å¾ˆå®¹æ˜“å¯¹è¯¥ç±»åé—®é¢˜å°è¯•è¿›è¡Œæ±‚è§£ã€‚

    ä½¿ç”¨ä¹‹å‰çš„Parabolic è€¦åˆPDEæ¨¡å‹è¿›è¡ŒåŒºåŸŸåé—®é¢˜çš„å®éªŒã€‚

å¯¹**åŒºåŸŸ1{(x,y)|0<=x<=1,0<=y<=1}**çš„åˆ’åˆ†ä¸ºï¼š

    regions_x = [ [0.10,0.30],[0.40,0.60],[0.70,0.90] ]

    regions_y = [ [0.10,0.30],[0.40,0.60],[0.70,0.90] ]

å¯¹**åŒºåŸŸ2{(x,y)|0<=x<=1,-1<=y<=0}**çš„åˆ’åˆ†ä¸ºï¼š

    regions_x = [ [0.10,0.30],[0.40,0.60],[0.70,0.90] ]

    regions_y = [ [-0.10,-0.30],[-0.40,-0.60],[-0.70,-0.90] ]

3 * 3 = 9ï¼Œæ¯ä¸ªåŒºåŸŸè¢«åˆ†ä¸º9ä¸ªå­åŒºåŸŸ

**å°†åŸæ¥è¾¹ç•Œå¤„çš„X_u_train,u_trainæ›¿æ¢ä¼šè¿™äº›å­åŒºåŸŸå†…çš„ç‚¹å’Œå¯¹åº”çš„è§£ã€‚**

> åŒºåŸŸçš„åˆ’åˆ†æœ‰è®²ç©¶ï¼Œè‹¥åŒºåŸŸè¿‡äºå°æˆ–è€…è¦†ç›–ç‡ä½ç­‰ï¼Œå¯èƒ½å¯¼è‡´æ•´ä¸ªåŒºåŸŸå†…æ‹Ÿåˆæ•ˆæœå’ŒçœŸè§£å·®è·å¤§ï¼Œè§£å¯èƒ½ä¸å”¯ä¸€ã€‚

ä»£ç è§ **0722_åŒºåŸŸåé—®é¢˜ Parabolicè€¦åˆæ¨¡å‹.ipynb**

---

# 07-23

## å‚æ•°åé—®é¢˜

    å‚æ•°åé—®é¢˜æ˜¯æŒ‡å·²çŸ¥éƒ¨åˆ†ã€ä¹ƒè‡³å…¨éƒ¨æ•°å€¼çœŸè§£ï¼Œåæ¨æ¨¡å‹çš„å‚æ•°ï¼Œä»¥ä¼ ç»Ÿæ–¹æ³•æ¥è¯´ï¼Œè¿™æ˜¯å¾ˆå›°éš¾çš„ï¼Œä½†åœ¨PINNæ¡†æ¶ä¸‹ï¼Œåªéœ€è¦å°†æ¨¡å‹å‚æ•°è®¾ä¸ºå˜é‡ï¼Œå¸¦å…¥çœŸè§£è®­ç»ƒæ¨¡å‹ï¼ˆåŒæ—¶è®­ç»ƒå‚æ•°ï¼‰ï¼Œå¯ä»¥åæ¨å‚æ•°ã€‚

ä»£ç è§ **0723_å‚æ•°åé—®é¢˜_Parabolicè€¦åˆæ¨¡å‹.ipynb**

---

# 07-26

## 3d ç®—ä¾‹& n-d ç®—ä¾‹

> *ä¹‹å‰æ‰€æœ‰çš„ä»£ç éƒ½æ˜¯åŸºäºå¦‚ä¸‹çš„æ¨¡å‹å’Œç®—ä¾‹*ï¼Œå¯ä»¥çœ‹åˆ°æ˜¯2dçš„ï¼Œå³u(x,y,t)ã€‚

### **parabolic è€¦åˆPDEæ¨¡å‹**

 In this work, a simplified model of diffusion through two adjacent materials which are coupled across their shared and rigid interface $I$ through a jump condition is considered. This problem captures some of the time-stepping difficulties of the ocean-atmosphere problem described in 1.2. The domain consists of two subdomains $\Omega_{1}$ and $\Omega_{2}$ coupled across an interface $I$ (example in Figure $1.1$ below). The problem is: given $\nu_{i}>0, f_{i}:[0, T] \rightarrow H^{1}\left(\Omega_{i}\right), u_{i}(0) \in$ $H^{1}\left(\Omega_{i}\right)$ and $\kappa \in \mathbb{R}$, find (for $\left.i=1,2\right) u_{i}: \bar{\Omega}_{i} \times[0, T] \rightarrow \mathbb{R}$ satisfying

$$
\begin{aligned}
u_{i, t}-\nu_{i} \Delta u_{i} &=f_{i}, \quad \text { in } \Omega_{i}, &(1.1)\\
-\nu_{i} \nabla u_{i} \cdot \hat{n}_{i} &=\kappa\left(u_{i}-u_{j}\right), \quad \text { on } I, \quad i, j=1,2, i \neq j, &(1.2)\\
u_{i}(x, 0) &=u_{i}^{0}(x), \quad \text { in } \Omega_{i}, &(1.3)\\
u_{i} &=g_{i}, \quad \text { on } \Gamma_{i}=\partial \Omega_{i} \backslash I . &(1.4)
\end{aligned}
$$

### **2dç®—ä¾‹**

Assume $\Omega_{1}=[0,1] \times[0,1]$ and $\Omega_{2}=[0,1] \times[-1,0]$, so $I$ is the portion of the $x$-axis from 0 to 1 . Then $\mathbf{n}_{1}=[0,-1]^{T}$ and $\mathbf{n}_{2}=[0,1]^{T}$. For $a, \nu_{1}, \nu_{2}$, and $\kappa$ all arbitrary positive constants, the right hand side function $\mathbf{f}$ is chosen to ensure that

$$
\begin{aligned}
&u_{1}(t, x, y)=a x(1-x)(1-y) e^{-t} \\
&u_{2}(t, x, y)=a x(1-x)\left(c_{1}+c_{2} y+c_{3} y^{2}\right) e^{-t} .
\end{aligned}
$$

The constants $c_{1}, c_{2}, c_{3}$ are determined from the interface conditions (1.2) and the boundary conditions for $u_{2}$. One may verify that with the following choices for $c_{1}, c_{2}, c_{3}, u_{1}$ and $u_{2}$ will satisfy (1.1)-(1.4) with $g_{1}=g_{2}=0$, i. e. when $x \in\{0,1\}$ or $y \in\{-1,1\}$ :

$$
c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1} .
$$

The numerical analysis performed in Section 4 indicates that by choosing $\kappa$ to be no larger than $\nu_{1}, \nu_{2}$ the IMEX scheme should perform as well as the implicit scheme. Computational results comparing the performance of the two methods are listed for two test problems:

- Test Problem 1: $a=\nu_{1}=\nu_{2}=\kappa=1$.

 å®é™…ä¸Š `u1` å’Œ `u2`çš„æ„é€ æ˜¯åŸºäºä»¤$u_{i} =g_{i} =0, \quad \text { on } \Gamma_{i}=\partial \Omega_{i} \backslash I$, ä½ ä¼šå‘ç°åœ¨è¾¹ç•Œä¸Šu1 = 0 ã€‚

å†æ ¹æ®(1.2)å’Œ(1.4)æ¨å‡ºu2ä¸­å‚æ•°éœ€è¦æ»¡è¶³çš„å…³ç³»å¼ã€‚æœ€ååœ¨æ ¹æ®(1.1)æ±‚å‡ºf1å’Œf2ã€‚ç¡®å®šå‚æ•°çš„å€¼ï¼Œæœ€ç»ˆæ„æˆä¸€ä¸ªå®Œæ•´çš„ç®—ä¾‹ã€‚

> ä¸‹é¢æˆ‘å°†ç®—ä¾‹æ‹“å±•åˆ°3dï¼Œå®é™…ä¸Šå¯ä»¥æ¨å¹¿åˆ° n-dã€‚

### 3dç®—ä¾‹

Assume $\Omega_{1}=[0,1] \times[0,1] \times[0,1]$ and $\Omega_{2}=[0,1] \times[0,1]\times[-1,0]$, so $I$ is the plain of z=0 , the $x$-axis from 0 to 1 ,the $y$-axis from 0 to 1.Namely.  $I$ = $[0,1] \times[0,1] \times\{0\}$ .

Then $\mathbf{n}_{1}=[0,0,-1]^{T}$ and $\mathbf{n}_{2}=[0,0,1]^{T}$.

For $a, \nu_{1}, \nu_{2}$, and $\kappa$ all arbitrary positive constants, the right hand side function $\mathbf{f}$ is chosen to ensure that

$$
\begin{aligned}
&u_{1}(t, x, y, z)=a xy(1-x)(1-y)(1-z) e^{-t} \\
&u_{2}(t, x, y, z)=a xy(1-x)(1-y)\left(c_{1}+c_{2} z+c_{3} z^{2}\right) e^{-t} .
\end{aligned}
$$

The constants $c_{1}, c_{2}, c_{3}$ are determined from the interface conditions (1.2) and the boundary conditions for $u_{2}$. One may verify that with the following choices for $c_{1}, c_{2}, c_{3}, u_{1}$ and $u_{2}$ will satisfy (1.1)-(1.4) with $g_{1}=g_{2}=0$, i. e. when $(x,y,z) \in\{x=1,0\leq y \leq 1,0\leq z \leq 1\}, u_1(x,y,z,t) = 0$ :

$$
c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1}.
$$

The numerical analysis performed in Section 4 indicates that by choosing $\kappa$ to be no larger than $\nu_{1}, \nu_{2}$ the IMEX scheme should perform as well as the implicit scheme. Computational results comparing the performance of the two methods are listed for two test problems:

- Test Problem 1: $a=\nu_{1}=\nu_{2}=\kappa=1$.

### n-d ç®—ä¾‹

$\Omega_{1}=\underbrace{[0,1] \times... \times[0,1] }_{n-1}\times[0,1]$,$\Omega_{2}=\underbrace{[0,1] \times... \times[0,1] }_{n-1}\times[-1,0]$,  $I$ = $\underbrace{[0,1] \times... \times[0,1] }_{n-1}\times\{0\}$ .

Then $\mathbf{n}_{1}=\underbrace{[0,...,0}_{n-1},-1]^{T}$ and $\mathbf{n}_{2}=\underbrace{[0,...,0}_{n-1},1]^{T}$.

For $a, \nu_{1}, \nu_{2}$, and $\kappa$ all arbitrary positive constants, the right hand side function $\mathbf{f}$ is chosen to ensure that

$$
\begin{aligned}&u_{1}(t, x_1, x_2, ...,x_n)=ae^{-t} \prod \limits_{i=1}^n
x_i(1-x_i)  \\
&u_{2}(t, x_1, x_2, ...,x_n)=ae^{-t} \left(c_{1}+c_{2} x_n+c_{3} x_n^{2}\right)\prod \limits_{i=1}^{n-1}
x_i(1-x_i) .
\end{aligned}
$$

The constants $c_{1}, c_{2}, c_{3}$ are determined from the interface conditions (1.2) and the boundary conditions for $u_{2}$. One may verify that with the following choices for $c_{1}, c_{2}, c_{3}, u_{1}$ and $u_{2}$ will satisfy (1.1)-(1.4) with $g_{1}=g_{2}=0$:

$$
c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1} .c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1} .
$$

---

## DeepXDEåº“&TensorDiffEqåº“

DeepXDE&TensorDiffEqæ˜¯ç°æœ‰çš„PINNæ±‚è§£å™¨ã€‚

- [DeepXDE(å®˜æ–¹æ–‡æ¡£)](https://deepxde.readthedocs.io/en/latest/)ï¼Œå¸ƒæœ—å¤§å­¦ Lu åšå£«å¼€å‘çš„ï¼Œå°±æ˜¯ DeepONet é‚£ä½ Lu åšå£«ã€‚ä»–ä»¬ç»„æ˜¯æœ¬æ¬¡ PINN æ½®æµçš„å…ˆé©±ï¼Œåº”è¯¥ç®—æ˜¯ç¬¬ä¸€æ¬¾ä¹Ÿæ˜¯â€œå®˜æ–¹â€çš„ PINN æ±‚è§£å™¨ã€‚é›†æˆäº†åŸºäºæ®‹å·®çš„è‡ªé€‚åº”ç»†åŒ–ï¼ˆRARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åœ¨è®­ç»ƒé˜¶æ®µä¼˜åŒ–æ®‹å·®ç‚¹åˆ†å¸ƒçš„ç­–ç•¥ï¼Œå³åœ¨åå¾®åˆ†æ–¹ç¨‹æ®‹å·®è¾ƒå¤§çš„ä½ç½®æ·»åŠ æ›´å¤šç‚¹ã€‚è¿˜æ”¯æŒåŸºäºæ„é€ å®ä½“å‡ ä½• ï¼ˆCSGï¼‰ æŠ€æœ¯çš„å¤æ‚å‡ ä½•åŒºåŸŸå®šä¹‰ã€‚

> [DeepXDEè®ºæ–‡](../è®ºæ–‡èµ„æ–™/DeepXDE- A Deep Learning Library for Solving Differential Equations.pdf)

- [TensorDiffEq(å®˜æ–¹æ–‡æ¡£)](https://docs.tensordiffeq.io/)çœ‹åå­—å°±çŸ¥é“æ˜¯åŸºäº Tensorflowï¼Œç‰¹ç‚¹æ˜¯åšåˆ†å¸ƒå¼è®¡ç®—ã€‚ä¸»æ—¨æ˜¯é€šè¿‡å¯ä¼¸ç¼©ï¼ˆscalableï¼‰è®¡ç®—æ¡†æ¶æ¥æ±‚è§£ PINNï¼Œæ˜æ˜¾æ˜¯ä¸ºå¤§è§„æ¨¡å·¥ä¸šåº”ç”¨åšé“ºå«ã€‚

---

# 07-27

## 3D-parabolicä»£ç ç¼–å†™

    åœ¨2dåŸºç¡€ä¸Šæ–°å¢ä¸€ä¸ªç»´åº¦å³å¯ï¼Œå³åœ¨ç¥ç»ç½‘ç»œçš„è¾“å…¥Inputå±‚æ–°å¢ä¸€ä¸ªç»´åº¦andå¢åŠ å¯¹zçš„åå¯¼ã€‚æ³¨æ„è®­ç»ƒæ•°æ®çš„ç”Ÿæˆä»¥åŠå›¾åƒç”Ÿæˆéœ€è¦ç•¥å¾®æ”¹åŠ¨ã€‚

    å®éªŒæ•ˆæœè§**0727_3D_Parabolicè€¦åˆæ¨¡å‹.ipynb**

> å› ä¸ºå¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œè®­ç»ƒçš„éš¾åº¦æœ‰æ‰€ä¸Šå‡ï¼Œ
>
> - å¢åŠ ä¸€äº› hidden layers ä»¥åŠ hidden sizeã€‚
> - å¢åŠ åŒºåŸŸå†…é‡‡æ ·ç‚¹N_fçš„æ•°é‡ï¼Œå’Œåˆè¾¹å€¼æ¡ä»¶çš„è®­ç»ƒç‚¹ã€‚

# 07-30

## 3D-parabolicä»£ç ä¼˜åŒ–

åœ¨åŸå…ˆä»£ç çš„åŸºç¡€ä¸Šï¼Œä¸ºCouplePinnçš„modelå¢åŠ äº†è‹¥å¹²MetricsæŒ‡æ ‡
ï¼šlossï¼Œloss_u1,loss_u2,loss_i,error_u1,error_u2ã€‚

åœ¨æ¯æ¬¡epochç»“æŸï¼Œéƒ½ä¼šæ‰“å°è¿™äº›æŒ‡æ ‡çš„å€¼ï¼Œlossç±»æŒ‡æ ‡ååº”äº†æŸå¤±å‡½æ•°çš„å˜åŒ–ï¼Œerrorç±»æŒ‡æ ‡ååº”äº†æµ‹è¯•é›†çš„é”™è¯¯ç‡ï¼Œå³çœŸå®è§£ä¸æ¨¡å‹å¾—å‡ºçš„è§£ä¹‹é—´çš„å·®è·ã€‚

ä»£ç è§ `0730_3D_Parabolicè€¦åˆæ¨¡å‹_ä¼˜åŒ–.ipynb`ï¼Œæ‹Ÿåˆæ•ˆæœè¿˜æ˜¯ä¸é”™çš„ã€‚

æ­¤å¤–ï¼Œä¸Šè¿°æåˆ°çš„æŒ‡æ ‡æœ‰åŠ©äºæˆ‘ä»¬åˆ†ææ¨¡å‹åœ¨è®­ç»ƒä¸­æƒ…å†µï¼Œå¸®åŠ©æˆ‘ä»¬è¯Šæ–­è®­ç»ƒç»“æœçš„å¥½åã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä»¥ä¸‹å‡†åˆ™ï¼š

è®¾ lossï¼šè®­ç»ƒé›†çš„æŸå¤±å€¼ï¼Œerrorï¼šæµ‹è¯•é›†çš„é”™è¯¯ç‡

- æƒ…å†µä¸€ï¼šlossä¸æ–­ä¸‹é™ï¼Œerrorä¸æ–­ä¸‹é™ï¼Œè¯´æ˜**æ¨¡å‹ä»åœ¨å­¦ä¹ ä¸­**
  - è§£å†³åŠæ³•ï¼šæ­¤æ—¶æ¨¡å‹æ˜¯æœ€å¥½çš„ï¼Œä¸éœ€è¦å…¶ä»–æªæ–½
- æƒ…å†µäºŒï¼šlossä¸æ–­ä¸‹é™ï¼Œerrorä¿æŒä¸æ–­ï¼Œè¯´æ˜**æ¨¡å‹å‡ºç°è¿‡æ‹Ÿåˆ**
  - è§£å†³åŠæ³•ï¼šé‡‡ç”¨æ•°æ®å¢å¼º(è®­ç»ƒé›†N_u,N_fæ•°é‡å¢å¤§)ï¼Œæ­£åˆ™åŒ–ç­‰
- æƒ…å†µä¸‰ï¼šlossè¶‹äºä¸å˜ï¼Œerrorä¸æ–­ä¸‹é™ï¼Œè¯´æ˜**è®­ç»ƒæ•°æ®é›†100%æœ‰é—®é¢˜**
  - è§£å†³åŠæ³•ï¼šæ£€æŸ¥dataset
- æƒ…å†µå››ï¼šlossè¶‹äºä¸å˜ï¼Œerrorè¶‹äºä¸å˜ï¼Œè¯´æ˜**å­¦ä¹ é‡åˆ°ç“¶é¢ˆæˆ–æ”¶æ•›(errorå·²ç»å¾ˆä½)**
  - è§£å†³åŠæ³•ï¼šå‡å°‘å­¦ä¹ ç‡orå¢å¤§batch_size(å³å‡å°‘æ‰¹é‡æ€»æ•°)oråœæ­¢è®­ç»ƒ

---

# 08-01

## [Effective Tensorflow2(ä¸Š)](https://www.tensorflow.org/guide/effective_tf2)

    å­¦ä¹ TensorFlowå®˜ç½‘çš„**Effective Tensorflow2** æ–‡æ¡£ï¼Œ
    é«˜æ•ˆåœ°ä½¿ç”¨TensorFlow2æ­å»ºä»¥åŠè®­ç»ƒæ¨¡å‹ã€‚

## Overview

This guide provides a list of best practices for writing code using TensorFlow 2 (TF2)

## å¦‚ä½•æ›´å¥½ä½¿ç”¨ TensorFlow2

### **1.é‡æ„ä»£ç :æ›´å¤šçš„å­æ¨¡å—**

A good practice is to refactor your code into smaller functions that are called as needed. For best performance, you should try to decorate the largest blocks of computation that you can in a tf.function (note that the nested python functions called by a tf.function do not require their own separate decorations, unless you want to use different jit_compile settings for the tf.function). Depending on your use case, this could be multiple training steps or even your whole training loop. For inference use cases, it might be a single model forward pass.

ä½¿ç”¨ `@tf.function`è£…é¥°å™¨ åŒ…è£…å‡½æ•°ï¼Œç‰¹åˆ«å¯¹äºå¤§å‹è®¡ç®—ç±»å‡½æ•°ï¼Œæ¯”å¦‚å‘å‰ä¼ æ’­ `Forward Pass`, å•æ­¥è®­ç»ƒ `training by step`ã€‚

åœ¨TensorFlow2ä¸­ï¼Œé»˜è®¤åŠ¨æ€å›¾Eageræ¨¡å¼ï¼Œä½¿ç”¨ `@tf.function`è£…é¥°å‡½æ•°åï¼Œèƒ½ä½¿å¾—è¯¥å‡½æ•°å†…çš„è®¡ç®—è½¬ä¸ºAutoGraphæ¨¡å¼ã€‚

ç®€å•çš„ç†è§£å°±æ˜¯ï¼Œå‡½æ•°è¢«@tf.functionåï¼Œæ¯æ¬¡è°ƒç”¨å®ƒï¼Œéƒ½æ˜¯ä»å†…å­˜ä¸­å–å‡ºè¯¥å‡½æ•°çš„è®¡ç®—å›¾ï¼Œè€Œä¸æ˜¯Eageræ¨¡å‹ä¸‹å†æ¬¡åŠ¨æ€æ„å»ºè®¡ç®—å›¾ã€‚@tf.functionèƒ½å¤Ÿæé«˜è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹è®¡ç®—ã€‚

---

### **2.è°ƒèŠ‚ä¼˜åŒ–å™¨Optimizerçš„é»˜è®¤å­¦ä¹ ç‡**

TensorFlowçš„å†…ç½®ä¼˜åŒ–å™¨é›†æˆåœ¨tf.keras.optimizersä¸­ï¼Œæ¯”å¦‚SDGã€Adamã€Nadamã€RMSpropç­‰ã€‚

åœ¨TF2ä¸­ï¼Œéƒ¨åˆ†Kerasçš„ä¼˜åŒ–å™¨æœ‰ä¸åŒçš„é»˜è®¤å­¦ä¹ ç‡ã€‚å¦‚æœå‘ç°æ¨¡å‹çš„æ”¶æ•›è¡¨ç°ä¸åŒï¼Œè®°å¾—checkä¼˜åŒ–å™¨çš„é»˜è®¤å­¦ä¹ ç‡ã€‚

SDGï¼ŒAdamï¼ŒRMSpropçš„é»˜è®¤å­¦ä¹ ç‡æ˜¯ä¸€è‡´çš„ã€‚

The following default learning rates have changed:

- optimizers.Adagrad from `0.01` to `0.001`
- optimizers.Adadelta from `1.0` to `0.001`
- optimizers.Adamax from `0.002` to `0.001`
- optimizers.Nadam from `0.002` to `0.001`

---

### **3.ç»§æ‰¿tf.Moduleä»¥åŠä½¿ç”¨Keras.layersç®¡ç†Variables**

`tf.Module`å’Œ `tf.keras.layers.Layer`å«æœ‰ä¸¤ç§å¥½ç”¨çš„å±æ€§ `variables` and `trainable_variables`,å®ƒä»¬é€’å½’åœ°æ”¶é›†äº†æ¨¡å‹ä¸­æ‰€æœ‰çš„å˜é‡ã€‚

This makes it easy to manage variables locally to where they are being used.

Keras layers/models ç»§æ‰¿ `tf.train.Checkpointable` å­ç±» å¹¶ä¸ `@tf.function`ç›¸èåˆ,è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿç›´æ¥ä¿å­˜checkpointæˆ–è€…ä»Keras Objectsä¸­å¯¼å‡ºæ¨¡å‹è®¡ç®—å›¾ã€‚

---

### **4. ç»“åˆtf.data.Dataset å’Œ @tf.function**

`tf.data.Dataset`ä¹Ÿå«æ•°æ®ç®¡é“ã€‚å½“è®­ç»ƒæ•°æ®é‡å¤ªå¤šæ—¶ï¼Œå†…å­˜å®¹é‡ä¸è¶³ä»¥æ”¯æŒå°†æ•°æ®å…¨éƒ¨æ”¾å…¥å†…å­˜ä¸­å¹¶å¼€å§‹è®­ç»ƒã€‚æ•°æ®ç®¡é“çš„ä½œç”¨å°±æ˜¯æ¯æ¬¡è®­ç»ƒéƒ½ä»å¤–å­˜ä¸­æ‹¿ä¸€éƒ¨åˆ†æ•°æ®è¿›æ¥è®­ç»ƒã€‚

Tensorflow Datesets(tfds) åŒ…ï¼Œæä¾›äº†è®¸å¤šutilitiesï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡tf.data.Datasetçš„Object åŠ è½½predefinedçš„æ•°æ®é›†ã€‚

ä¾‹å¦‚ï¼Œä½¿ç”¨tfdsåŠ è½½MNISTæ•°æ®é›†ï¼š

```python
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = datasets['train'], datasets['test']
```

æ¥ç€é¢„å¤„ç†æ•°æ®ï¼Œåšæˆè®­ç»ƒæ•°æ®ï¼š

```python
BUFFER_SIZE = 10 # Use a much larger value for real code
BATCH_SIZE = 64
NUM_EPOCHS = 5


def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255

  return image, label
```

```python
train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
test_data = mnist_test.map(scale).batch(BATCH_SIZE)

STEPS_PER_EPOCH = 5

train_data = train_data.take(STEPS_PER_EPOCH)
test_data = test_data.take(STEPS_PER_EPOCH)
```

```python
image_batch, label_batch = next(iter(train_data))
# è¿™é‡Œ next,iteréƒ½æ˜¯pythonå†…ç½®çš„è¿­ä»£å™¨æ–¹æ³•ï¼Œç”¨äºå–å‡ºä¸€ä¸ªå…ƒç´ 
```

Use regular Python iteration to iterate over training data that fits in memory. Otherwise, tf.data.Dataset is the best way to stream training data from disk. Datasets are iterables (not iterators), and work just like other Python iterables in eager execution. You can fully utilize dataset async prefetching/streaming features by wrapping your code in tf.function, which replaces Python iteration with the equivalent graph operations using AutoGraph.

```python
@tf.function
def train(model, dataset, optimizer):
  for x, y in dataset: #ä»datasetä¸­è§£åŒ…å‡º xï¼Œy
    with tf.GradientTape() as tape:
      # training=True is only needed if there are layers with different
      # behavior during training versus inference (e.g. Dropout).
      prediction = model(x, training=True)
      loss = loss_fn(prediction, y)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

å¦‚æœä½¿ç”¨Modelå†…ç½®çš„fitæ–¹æ³•ï¼Œæ›´åŠ æ–¹ä¾¿ï¼Œéƒ½ä¸ç”¨è€ƒè™‘datasetçš„è¿­ä»£ã€‚

> fitæ˜¯è®­ç»ƒæ¨¡å‹çš„å‡½æ•°ï¼Œé«˜é˜¶APIï¼Œç¨‹åºå‘˜å·²ç»å¸®æˆ‘ä»¬å†™å¥½äº†ä¸€ç§ä¸€èˆ¬åŒ–çš„è®­ç»ƒæ­¥éª¤ï¼Œåªéœ€è¦æŒ‡å®šè®­ç»ƒæ•°æ®ï¼Œä¼˜åŒ–å™¨ç­‰æ¨¡å‹é…ç½®å³å¯å¼€å§‹è®­ç»ƒï¼Œä¸ç”¨è‡ªå·±ç¼–å†™è®­ç»ƒä»£ç .

```python
model.compile(optimizer=optimizer, loss=loss_fn)
model.fit(dataset)
```

---

### **5.ä½¿ç”¨Keraså†…ç½®çš„è®­ç»ƒloop**

å½“æˆ‘ä»¬ä¸éœ€è¦å¯¹è®­ç»ƒè¿‡ç¨‹è¿›è¡Œåº•å±‚çš„æ§åˆ¶ï¼Œé‚£ä¹ˆå°±ä½¿ç”¨Kerasçš„ bulit-in æ–¹æ³•å§ï¼Œæ¯”å¦‚ fitï¼Œevaluateå’Œpredictã€‚

- fitï¼šè®­ç»ƒæ¨¡å‹
- evaluateï¼šè¯„ä¼°æ¨¡å‹
- predictï¼šé¢„æµ‹/æ¨ç†

ä½¿ç”¨è¿™äº›æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¦‚ä¸‹ä¼˜åŠ¿ï¼š

- ç®€å•ï¼Œå‚»ç“œå¼
- æ”¯æŒ Numpy arrays, Python generators å’Œ `tf.data.Datasets`.
- è‡ªåŠ¨ä½¿ç”¨ regularization å’Œ æ¿€æ´»å‡½æ•°
- æ”¯æŒä»»æ„çš„callablesä½œä¸º losses and metrics.
- æ”¯æŒ callbacks(å›è°ƒå‡½æ•°) like tf.keras.callbacks.TensorBoardä»¥åŠè‡ªå®šä¹‰å›è°ƒå‡½æ•°ã€‚
- é«˜æ€§èƒ½ï¼Œè‡ªåŠ¨ä½¿ç”¨ TensorFlow graphs.

å¯ä»¥å‘ç°ä¸Šè¿°æ–¹æ³•æ”¯æŒçš„åŠŸèƒ½éå¸¸å¤šï¼Œåªéœ€è¦ä¼ å…¥ç›¸åº”çš„å‚æ•°ï¼Œè¿™äº›åŠŸèƒ½çš„å…·ä½“å®ç°éƒ½ä¸éœ€è¦è‡ªå·±å†™ã€‚

å½“ç„¶ï¼Œå¦‚æœæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¹ƒè‡³æ¨æ–­è¿‡ç¨‹éƒ½æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦å¯¹è®­ç»ƒè¿‡ç¨‹è¿›è¡Œåº•å±‚çš„æ§åˆ¶ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰ä»¥åŠé‡å†™æ–¹æ³•ã€‚

fit()æ–¹æ³•æ˜¯ä¼šè°ƒç”¨train_step()æ–¹æ³•ï¼Œå³æ¯ä¸€æ­¥çš„è®­ç»ƒè¿‡ç¨‹ã€‚
fitå¤§è‡´ç»“æ„æ˜¯ï¼š

```python
def fit(ds,epochs,*args,**kwargs):
    ...
    for epoch in tf.range(epochs):
        ...
        for data in ds:
            self.train_step(data)
        ...
    ...
    '''
    æ¯æ¬¡epochå¼€å§‹/ç»“æŸï¼Œä¼šè®¡ç®—MetricæŒ‡æ ‡(å¦‚æœä¼ å…¥)ï¼Œ
    ä»¥åŠå›è°ƒå‡½æ•°(å¦‚æœä¼ å…¥)ç­‰ã€‚ä½†è®­ç»ƒçš„æ ¸å¿ƒæ˜¯train_step()å‡½æ•°ã€‚

    å¾€å¾€æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥åªé‡è½½train_step()è¾¾åˆ°åº•å±‚æ§åˆ¶çš„ç›®çš„ï¼Œ
    åŒæ—¶è¿˜èƒ½ç»§ç»­ä½¿ç”¨fit()ä¸­å¥½ç”¨çš„åŠŸèƒ½ã€‚åƒæ­ç§¯æœ¨ä¸€æ ·ã€‚
    '''
```

ä¸‹é¢å±•ç¤ºä¸€ä¸ªä½¿ç”¨ `Dataset`è®­ç»ƒæ¨¡å‹çš„ä¾‹å­ï¼š

```python
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.02),
                           input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10)
])

# Model is the full model w/o custom layers
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(train_data, epochs=NUM_EPOCHS)
loss, acc = model.evaluate(test_data)

print("Loss {}, Accuracy {}".format(loss, acc))
```

```Plain
Epoch 1/5
5/5 [==============================] - 9s 7ms/step - loss: 1.5762 - accuracy: 0.4938
Epoch 2/5
5/5 [==============================] - 0s 6ms/step - loss: 0.5087 - accuracy: 0.8969
Epoch 3/5
5/5 [==============================] - 2s 5ms/step - loss: 0.3348 - accuracy: 0.9469
Epoch 4/5
5/5 [==============================] - 0s 5ms/step - loss: 0.2445 - accuracy: 0.9688
Epoch 5/5
5/5 [==============================] - 0s 6ms/step - loss: 0.2006 - accuracy: 0.9719
5/5 [==============================] - 1s 4ms/step - loss: 1.4553 - accuracy: 0.5781
Loss 1.4552843570709229, Accuracy 0.578125
```

---

### **6.è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹**

æˆ‘åœ¨å†™PINNå’ŒCouple PINNçš„ä»£ç å°±æ˜¯è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ã€‚

If Keras models work for you, but you need more flexibility and control of the training step or the outer training loops, you can implement your own training steps or even entire training loops.

You can also implement many things as a `tf.keras.callbacks.Callback`.

This method has many of the advantages mentioned previously, but gives you control of the train step and even the outer loop.

**Train Loop æ ‡å‡†åŒ–ä¸‰æ­¥èµ°ï¼š**

1. é€šè¿‡Python Generator or tf.data.Dataset è¿­ä»£è·å¾—ä¸€ä¸ªbatch
2. ä½¿ç”¨tf.GradientTape()æ¢¯åº¦ç£å¸¦ è®°å½•è®¡ç®—æµï¼Œç”¨äºæ±‚æ¢¯åº¦ã€‚
3. ä½¿ç”¨tf.keras.optimizers å°†æ¢¯åº¦ apply to weights in model.

**éœ€è¦æ³¨æ„çš„æ˜¯ï¼š**

1. å½“ call æ¨¡å‹æˆ–è€…layersæ—¶ï¼Œæ€»æ˜¯ä¼ å…¥trainingå‚æ•°
   `model(inputs,training=True)`
2. ç¡®ä¿trainingè®¾ç½®æ­£ç¡®ï¼Œæ¯”å¦‚ä¸Šè¾¹training = True
3. æ ¹æ®ä½¿ç”¨æƒ…å†µè€Œå®šï¼ŒæŸäº›variableså¯èƒ½åªæœ‰ å½“æ¨¡å‹åœ¨a batch of dataä¸Šè¿è¡Œæ—¶æ‰å­˜åœ¨ï¼Œå‡ºå»ä¹‹åå°±æ²¡äº†ã€‚
4. éœ€è¦æ‰‹åŠ¨å¤„ç†ä¸€äº›æ“ä½œï¼Œæ¯”å¦‚ regularization losses for the model.

There is no need to run variable initializers or to add manual control dependencies. tf.function handles automatic control dependencies and variable initialization on creation for you.

ä¸€ä¸ªä¾‹å­ï¼š

```python
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.02),
                           input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10)
])

optimizer = tf.keras.optimizers.Adam(0.001)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

@tf.function
def train_step(inputs, labels):
  with tf.GradientTape() as tape:
    predictions = model(inputs, training=True)
    regularization_loss=tf.math.add_n(model.losses)
    pred_loss=loss_fn(labels, predictions)
    total_loss=pred_loss + regularization_loss

  gradients = tape.gradient(total_loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

for epoch in range(NUM_EPOCHS):
  for inputs, labels in train_data:
    train_step(inputs, labels)
  print("Finished epoch", epoch)
```

---

### **7.ä¸Pythonæ§åˆ¶æµæ­é…ä½¿ç”¨tf.function**

`tf.function` æä¾›äº†ä¸€ç§å°†data-dependentæ§åˆ¶æµ è½¬ä¸º è®¡ç®—å›¾æ¨¡å¼çš„ç­‰æ•ˆæ§åˆ¶æµï¼Œæ¯”å¦‚ ``tf.cond``,``tf.range``,``tf.while_loop`` .

One common place where data-dependent control flow appears is in sequence models. tf.keras.layers.RNN wraps an RNN cell, allowing you to either statically or dynamically unroll the recurrence. As an example, you could reimplement dynamic unroll as follows.

```python
class DynamicRNN(tf.keras.Model):

  def __init__(self, rnn_cell):
    super(DynamicRNN, self).__init__(self)
    self.cell = rnn_cell

  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.float32, shape=[None, None, 3])])
  def call(self, input_data):

    # [batch, time, features] -> [time, batch, features]
    input_data = tf.transpose(input_data, [1, 0, 2])
    timesteps =  tf.shape(input_data)[0]
    batch_size = tf.shape(input_data)[1]
    outputs = tf.TensorArray(tf.float32, timesteps)
    state = self.cell.get_initial_state(batch_size = batch_size, dtype=tf.float32)
    for i in tf.range(timesteps):  # è¿™é‡Œå°±ç”¨åˆ°äº†tfçš„æ§åˆ¶æµ
      output, state = self.cell(input_data[i], state)
      outputs = outputs.write(i, output)
    return tf.transpose(outputs.stack(), [1, 0, 2]), state
```

# 08-02

## [Effective Tensorflow2(ä¸‹)](https://www.tensorflow.org/guide/effective_tf2)

---

### **8.New-style metrics and losses**

æŒ‡æ ‡Metricså’ŒæŸå¤±Lossesåœ¨kerasä¸­éƒ½æ˜¯ç”±@tf.functionè£…é¥°çš„objectã€‚

ä¸€ä¸ªLosså¯¹è±¡æ˜¯callableçš„(ä»¿å‡½æ•°çš„æ¦‚å¿µ),(y_ture,y_pred)ä½œä¸ºè¾“å…¥å‚æ•°ã€‚ä¾‹å¦‚ï¼š

```python
cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
cce([[1, 0]], [[-1.0,3.0]]).numpy()

4.01815
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Metricsæ¥æ”¶é›†å’Œå±•ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ•°æ®ã€‚

ä½¿ç”¨ `tf.metrics`æ”¶é›†æ•°æ® and ä½¿ç”¨ `tf.summary` å°†æ•°æ®æ‰“å°å‡ºæ¥ã€‚ä½¿ç”¨ `ä¸Šä¸‹æ–‡ç®¡ç†å™¨`å¯å°†æ•°æ®é‡å®šå‘åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå³æŠŠæ—¥å¿—ä¿¡æ¯å†™å…¥æœ¬åœ°æ–‡ä»¶ä¸­ã€‚

```python
summary_writer = tf.summary.create_file_writer('/tmp/summaries')
with summary_writer.as_default():
  tf.summary.scalar('loss', 0.1, step=42)
```

åœ¨è¾“å‡ºsummarysä¹‹å‰ï¼Œé¦–å…ˆéœ€è¦ä½¿ç”¨tf.metricsæ”¶é›†æ•°æ®ã€‚Metricæ˜¯æœ‰çŠ¶æ€çš„ï¼Œå®ƒä»¬åœ¨ä¸€ä¸ªepoch(è®­ç»ƒå‘¨æœŸ)ä¸­å°†æ•°æ®æ”¶é›†ï¼Œè°ƒç”¨ `result`æ–¹æ³•å°†è¿”å›Metricsä¸­æ•°æ®çš„ç´¯è®¡å’Œçš„å¹³å‡å€¼,æ¯”å¦‚Mean.result()ã€‚

ä½¿ç”¨Metric.reset_states()å°†Metricä¸­çš„æ•°æ®æƒ…å†µï¼Œé€šå¸¸è¯¥å‡½æ•°çš„è°ƒç”¨å‘ç”Ÿåœ¨ä¸€ä¸ªepochç»“æŸã€‚

```python
def train(model, optimizer, dataset, log_freq=10):
  avg_loss = tf.keras.metrics.Mean(name='loss', dtype=tf.float32) # avg_lossæ˜¯ä¸€ä¸ª"Mean"æŒ‡æ ‡å™¨ï¼Œç”¨äºè®°å½•å¹³å‡çš„loss
  for images, labels in dataset:
    loss = train_step(model, optimizer, images, labels)
    avg_loss.update_state(loss) # æ¯train_stepä¸€æ¬¡ï¼Œå°±lossç´¯åŠ åˆ°avg_lossä¸­
    if tf.equal(optimizer.iterations % log_freq, 0):
      # train_step 10æ¬¡(=log_freq)åï¼Œå°†lossçš„å¹³å‡å€¼è¿”å›
      # tf.summaryå°†è¯¥lossï¼Œå†™å…¥æœ¬åœ°æ–‡ä»¶(æ–‡æœ¬æ–‡ä»¶ï¼Œæ—¥å¿—log)
      # å†™å®Œåï¼Œreset_states()é‡ç½®avg_lossæŒ‡æ ‡å™¨ 
      tf.summary.scalar('loss', avg_loss.result(), step=optimizer.iterations)
      avg_loss.reset_states()

def test(model, test_x, test_y, step_num):
  loss = loss_fn(model(test_x, training=False), test_y)
  tf.summary.scalar('loss', loss, step=step_num)

train_summary_writer = tf.summary.create_file_writer('/tmp/summaries/train')
test_summary_writer = tf.summary.create_file_writer('/tmp/summaries/test')

with train_summary_writer.as_default():
  train(model, optimizer, dataset)

with test_summary_writer.as_default():
  test(model, test_x, test_y, optimizer.iterations)
```

åˆ©ç”¨ `TensorBoard`å¯è§†åŒ–å·²ç”Ÿæˆçš„summariesã€‚

`tensorboard --logdir /tmp/summaries`,å…¶ä¸­ --logdir æ˜¯å®šä½ã€æŒ‡å‘æœ¬åœ°summariesæ–‡ä»¶çš„æ„æ€ã€‚

Use the `tf.summary` API to write summary data for visualization in `TensorBoard`. For more info, read the `tf.summary` [guide](https://www.tensorflow.org/tensorboard/migrate#in_tf_2x).

**keras metric names**
keraså†…ç½®Metrics éƒ½æœ‰ä¸€ä¸ªå›ºå®šçš„stringåç§°ï¼Œæ¯”å¦‚ 'acc' = keras.Metrics.ACCã€‚å½“æˆ‘ä»¬åœ¨è°ƒç”¨compileå‡½æ•°ï¼Œä¼ å…¥metricsæ—¶ï¼Œå¯ä»¥ä½¿ç”¨metrics = ['acc',...] æ›¿ä»£metris=[keras.Metrics.ACC()].

åœ¨è¾“å‡ºæ—¥å¿—æ—¶ï¼ŒMetricsä»¥ `{'name' ï¼šå€¼}`çš„å½¢å¼è¾“å‡ºã€‚

ä¾‹å¦‚ï¼š

```python
model.compile(
    optimizer = tf.keras.optimizers.Adam(0.001),
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics = ['acc', 'accuracy', tf.keras.metrics.SparseCategoricalAccuracy(name="my_accuracy")])
history = model.fit(train_data)
#log
5/5 [==============================] - 1s 5ms/step - loss: 0.0963 - acc: 0.9969 - accuracy: 0.9969 - my_accuracy: 0.9969

history.history.keys()
#log
dict_keys(['loss', 'acc', 'accuracy', 'my_accuracy'])
```

---

### **9.Debugging**

ä½¿ç”¨Eageræ¨¡å¼å¯ä¸€æ­¥ä¸€æ­¥è¿è¡Œä»£ç  inspect shapes, data types and valuesï¼Œå³è°ƒè¯•åŠŸèƒ½ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒæŸäº›APIï¼Œå¦‚ `tf.funcion`,`tf.keras`ç­‰ï¼Œä½¿ç”¨Graph execution(tf1.0ä¸­å«é™æ€å›¾)ï¼Œè¿™ç§æ¨¡å¼å¸¦æ¥æ›´å¥½çš„è¿ç®—è¡¨ç°å’Œå¯ç§»æ¤æ€§ã€‚

å¦‚æœæƒ³è°ƒè¯•@tf.functionè£…é¥°çš„å‡½æ•°ï¼Œéœ€è¦è®¾ç½® `tf.config.run_functions_eagerly(True)` ä½¿å¾—ä»£ç ä»¥Eageræ¨¡å¼è¿è¡Œã€‚

ä¾‹å¦‚ï¼š

```python
@tf.function
def f(x):
  if x > 0:
    import pdb
    pdb.set_trace()
    x = x + 1
  return x

tf.config.run_functions_eagerly(True)
f(tf.constant(1))
```

```python
>>> f()
-> x = x + 1
(Pdb) l
  6     @tf.function
  7     def f(x):
  8       if x > 0:
  9         import pdb
 10         pdb.set_trace()
 11  ->     x = x + 1
 12       return x
 13
 14     tf.config.run_functions_eagerly(True)
 15     f(tf.constant(1))
[EOF]
```

è¯¥ç”¨æ³•ï¼Œå¯¹äºKerasæ¨¡å‹ä»¥åŠå…¶ä»–çš„APIä¹Ÿæ˜¯é€‚ç”¨çš„(åªè¦å®ƒæ”¯æŒeager)ã€‚

ä¾‹å¦‚ï¼š

```python
class CustomModel(tf.keras.models.Model):

  @tf.function
  def call(self, input_data):
    if tf.reduce_mean(input_data) > 0:
      return input_data
    else:
      import pdb
      pdb.set_trace()
      return input_data // 2


tf.config.run_functions_eagerly(True)
model = CustomModel()
model(tf.constant([-2, -4]))

####
>>> call()
-> return input_data // 2
(Pdb) l
 10         if tf.reduce_mean(input_data) > 0:
 11           return input_data
 12         else:
 13           import pdb
 14           pdb.set_trace()
 15  ->       return input_data // 2
 16
 17
 18     tf.config.run_functions_eagerly(True)
 19     model = CustomModel()
 20     model(tf.constant([-2, -4]))
```

Notes:

- `tf.keras.Model` methods such as `fit`, `evaluate,` and `predict` execute as graphs with tf.function under the hood. åœ¨å†…å±‚å¾ªç¯ç”¨graphsè®¡ç®—æ¨¡å¼ï¼Œæ„æ€æ˜¯è¯´ `fit`æ²¡æœ‰è¢«@tf.functionï¼Œfitå‡½æ•°é‡Œé¢è°ƒç”¨äº† `tf.function`ï¼Œæ¯”å¦‚ `train_step()`.
- When using `tf.keras.Model.compile`, set `run_eagerly = True` to disable the Model logic from being wrapped in a tf.function.
- Use `tf.data.experimental.enable_debug_mode` to enable the debug mode for `tf.data`. Read the [API docs](https://www.tensorflow.org/api_docs/python/tf/data/experimental/enable_debug_mode) for more details.

---

### **10.Do not keep tf.Tensors in your objects**

å¦‚æœä¸€ä¸ªå‡½æ•°å†…éƒ¨å­˜åœ¨åˆ›å»ºtf.Variablesçš„è¡Œä¸ºï¼Œé‚£ä¹ˆä¸¤ç§æ‰§è¡Œæ¨¡å¼ `@tf.function`å’ŒEageræ¨¡å¼(not wrappd in a tf.function)ä¸‹ï¼Œè¯¥å‡½æ•°çš„æ•ˆæœæ˜¯ä¸ä¸€æ ·çš„ã€‚

å®é™…ä¸Šï¼Œå¦‚æœä¸€ä¸ªå‡½æ•°å†…éƒ¨æœ‰åˆ›å»ºtf.Variablesçš„è¡Œä¸ºï¼Œå¹¶ä¸”è¢«@tf.functionï¼Œé‚£ä¹ˆtensorflowä¼šæŠ¥é”™ï¼Œå¹¶æç¤ºä½ ä¸è¦è¿™æ ·åšã€‚

å½“ä½ æƒ³ç”¨tf.Variableæ¥è®°å½•tf.functionä¸­çš„ä¸€äº›ä¿¡æ¯ï¼ŒæŠŠè¯¥variableå®šä¹‰åœ¨å‡½æ•°å¤–éƒ¨ï¼Œæ¯”å¦‚åˆ©ç”¨classå°è£…è¯¥å˜é‡å’Œtf.functionï¼Œself.x = tf.Variable(...)ã€‚

å½“ç„¶ä¹Ÿå¯ä»¥ä¸ç”¨classï¼Œåªéœ€è¦åœ¨tf.functionä¸Šæ–‡å®šä¹‰å¥½tf.Variableï¼Œåœ¨å‡½æ•°å†…ç›´æ¥ä½¿ç”¨æˆ–å®šä¹‰å‚æ•°ä¼ å…¥ã€‚

Always use `tf.Tensors` only for intermediate values.

> End

---

## [DeepXDEè®ºæ–‡é˜…è¯»(1)]

[DeepXDE- A Deep Learnin Library for Solving Differential Equations](../è®ºæ–‡èµ„æ–™/DeepXDE-%20A%20Deep%20Learning%20Library%20for%20Solving%20Differential%20Equations.pdf)

### æ‘˜è¦

è¿‘å¹´æ¥Deep Learningåœ¨å¾ˆå¤šåº”ç”¨åœºæ™¯ä¸‹å–å¾—äº†éå‡¡çš„æˆåŠŸï¼Œç„¶è€Œï¼Œå®ƒåœ¨æ±‚è§£PDEsçš„åº”ç”¨åœ¨è¿‘å‡ å¹´æ‰å¼€å§‹æ—¶å‡ºç°ã€‚

```Plaintext
è¿‘å‡ å¹´æ¥æ‰å¼€å§‹å‡ºç°ç¥ç»ç½‘ç»œæ±‚è§£PDEsçš„ç ”ç©¶çƒ­ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯æœ‰ç‚¹å¥‡æ€ªçš„ã€‚Deep Learningé€šå¸¸æŒ‡å¤šå±‚çš„ç¥ç»ç½‘ç»œç»“æ„ã€‚

å› ä¸ºï¼Œä»æ•°å­¦çš„è§’åº¦çœ‹ï¼Œç¥ç»ç½‘ç»œå®é™…æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”±ç¥ç»å…ƒæ‰€ä»£è¡¨çš„æ¿€æ´»å‡½æ•°å¤åˆè€Œæˆã€‚æˆ‘ä»¬åˆ©ç”¨è¯¥å‡½æ•°æ„é€ ç›®æ ‡å‡½æ•°lossï¼Œè€Œæ‰€è°“è®­ç»ƒå°±æ˜¯æŒ‡`æœ€ä¼˜åŒ–(minimize) loss`ã€‚

å¯ä»¥æŠŠDeep Learningå°±ç†è§£ä¸ºå‡½æ•°é€¼è¿‘ï¼Œç¥ç»ç½‘ç»œå°±å¯ä»¥çœ‹åšæ˜¯ä¸€ç§å¼ºå¤§çš„å‡½æ•°é€¼è¿‘å™¨ã€‚è‡³å°‘ä»æ•°å­¦è§’åº¦çœ‹ï¼Œè¿™æ ·çš„è¯´æ³•æ˜¯å®Œå…¨æ²¡æœ‰é—®é¢˜çš„ã€‚

è¯´åˆ°è¿™é‡Œï¼Œå°±å¾ˆè‡ªç„¶æŠ›å‡ºç–‘é—®ï¼šä¸ºä»€ä¹ˆä¸ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘PDEsçš„è§£å‘¢ï¼ŸDeep Learningæ–¹æ³•æ±‚è§£PDEsåœ¨è¿‘å‡ å¹´æ‰å‡ºç°ï¼Ÿ
```

`å›åˆ°è®ºæ–‡ä¸­:`

æˆ‘ä»¬é¦–å…ˆç»™å‡ºPINNs(physics-informed neural networks)çš„ä¸€ä¸ªæ€»è§ˆã€‚PINNså°†PDE"åµŒå…¥"ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°lossä¸­ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚PINNsçš„ç®—æ³•æ¡†æ¶å¾ˆsimpleï¼Œå¹¶ä¸”å®ƒå¯ä»¥åº”ç”¨åˆ°ä¸åŒç±»å‹çš„PDEsä¸­ï¼Œä¾‹å¦‚ï¼šintegro-differential equations,fractional PDEs, and stochastic PDEsã€‚æ›´è¿›ä¸€æ­¥ï¼Œä»åº”ç”¨è§’åº¦çœ‹ï¼ŒPINNsæ±‚è§£åé—®é¢˜ä¸æ­£é—®é¢˜æ˜¯åŒæ ·ç®€å•çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ `åŸºäºæ®‹å·®çš„é€‚åº”æ”¹è¿›æ–¹æ³•`æ¥æå‡PINNsçš„è®­ç»ƒæ•ˆç‡ã€‚

æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªpythonåº“â€”â€”DeepXDEï¼Œå¯æœåŠ¡äºPINNsæ•™å­¦å’ŒPINNsçš„ç ”ç©¶ã€‚DeepXDEå¯è§£å†³æ­£é—®é¢˜(æœ‰åˆè¾¹å€¼æ¡ä»¶å’Œæ±‚è§£åŒºåŸŸ)ä»¥åŠåé—®é¢˜(æœ‰å…¶ä»–æ¡ä»¶)ã€‚DeepXDEæ”¯æŒå¤æ•°åŸŸPDEsã€‚æˆ‘ä»¬ç»™å‡º5ä¸ªä¸åŒçš„ç®—ä¾‹ï¼Œè¯æ˜PINNsçš„æ±‚è§£èƒ½åŠ›ä»¥åŠDeepXDEçš„ä½¿ç”¨æ–¹æ³•ã€‚

# 08-03

## [DeepXDEè®ºæ–‡é˜…è¯»(2)]

[DeepXDE- A Deep Learnin Library for Solving Differential Equations](../è®ºæ–‡èµ„æ–™/DeepXDE-%20A%20Deep%20Learning%20Library%20for%20Solving%20Differential%20Equations.pdf)

### Introduction

è¿‡å»15å¹´å†…ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œå½¢å¼çš„æ·±åº¦å­¦ä¹ å‘å±•å¾ˆå¿«ã€‚(æ·±åº¦å­¦ä¹ æ˜¯ä¸ªå¾ˆæ³›çš„æ¦‚å¿µï¼Œç‹­ä¹‰æŒ‡æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå°±æ˜¯æœ‰å¾ˆå¤šå±‚çš„ç¥ç»ç½‘ç»œã€‚)
åœ¨è¿‘å‡ å¹´æ¥ï¼Œç¥ç»ç½‘ç»œåº”ç”¨äºæ•°å€¼è®¡ç®—å¼€å§‹å‡ºç°ï¼Œä¾‹å¦‚PINNã€‚æˆ‘ä»¬å€ŸåŠ©ç¥ç»ç½‘ç»œæ›¿ä»£ä¼ ç»Ÿçš„ç¦»æ•£åŒ–æ•°å€¼æ–¹æ³•å»é€¼è¿‘PDEçš„è§£ã€‚

ä¸ºäº†é€šè¿‡Deep Learningå¾—åˆ°ä¸€ä¸ªPDEçš„è¿‘ä¼¼è§£ï¼Œå…³é”®æ˜¯éœ€è¦é™åˆ¶ç¥ç»ç½‘ç»œï¼Œæœ€å°åŒ–PDEæ®‹å·®ã€‚ç°å·²æœ‰æ•°ä¸ªæ–¹æ³•èƒ½å®ç°è¿™ä¸ªæ€æƒ³ã€‚
ä¸ä¼ ç»Ÿçš„åŸºäº"ç”»ç½‘æ ¼"çš„æ•°å€¼æ–¹æ³•ä¸åŒï¼Œå¦‚FEMã€FDMï¼ŒDeep Learningé€šè¿‡è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯ï¼Œä½¿å¾—æˆ‘ä»¬ä¸éœ€è¦ç”»ç½‘æ ¼å°±èƒ½å¾—åˆ°è¿‘ä¼¼è§£ï¼Œç”šè‡³è¿˜èƒ½å¤Ÿçªç ´ç»´æ•°çš„æ¡æ¢ã€‚

è®¸å¤šä¼ ç»Ÿæ•°å€¼æ–¹æ³•åªèƒ½é’ˆå¯¹ç‰¹å®šé—®é¢˜ï¼Œè€Œä¸”ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„PDEséƒ½èƒ½ä»å·²çŸ¥çš„æ¨¡å‹æ¨æ–­ã€‚è€Œå¯¹äºDeepLearningæ¥è¯´ï¼Œè‡ªåŠ¨å¾®åˆ†æŠ€æœ¯å¯ä»¥é¿å…æˆªæ–­è¯¯å·®ä»¥åŠå˜åˆ†æŠ€æœ¯çš„æ•°å€¼æ±‚ç§¯è¯¯å·®ã€‚PINNsè¿˜æœ‰ä¸€ä¸ªç‰¹æ€§ï¼Œæ±‚è§£åé—®é¢˜ä¸æ­£é—®é¢˜çš„ä»£ç å‡ ä¹æ˜¯ä¸€æ ·çš„ï¼Œåªéœ€è¦å¾ˆå°çš„æ”¹åŠ¨ã€‚

Section2ï¼š
 ç®€å•ä»‹ç»æ·±åº¦ç¥ç»ç½‘ç»œå’Œè‡ªåŠ¨å¾®åˆ†ã€‚ä»‹ç»PINNsçš„ç®—æ³•ã€é€¼è¿‘ç†è®ºå’Œè¯¯å·®åˆ†æã€‚PINNsä¸FEMçš„å¯¹æ¯”ã€‚PINNsæ±‚è§£IDEså’Œåé—®é¢˜ã€‚RARæ–¹æ³•æå‡PINNsçš„è®­ç»ƒæ•ˆç‡ã€‚

Section3ï¼š
ä»‹ç»å¦‚ä½•ä½¿ç”¨DeepXDEå’Œè‡ªå®šä¹‰æ–¹æ³•ã€‚

Section4ï¼š
ä»¥5ä¸ªä¸åŒçš„ç®—ä¾‹è¯æ˜PINNSçš„æ±‚è§£èƒ½åŠ›å’ŒDeepXDEçš„friendly useã€‚

Section5ï¼š
æ€»ç»“ã€‚

### ç®—æ³•&è®­ç»ƒç­–ç•¥&æ”¶æ•›æ€§

2.1 Deep Neural Networksï¼Œ2.2 AutomaticDifferentiationç•¥ã€‚

**2.3. Physics-Informed Neural Networks (PINNs) for Solving PDEs. **

We consider the following PDE parameterized by $\lambda$ for the solution $u(\mathrm{x})$ with $\mathrm{x}=\left(x_{1}, \ldots, x_{d}\right)$ defined on a domain $\Omega \subset \mathbb{R}^{d}$ :
(2.1) $f\left(\mathbf{x} ; \frac{\partial u}{\partial x_{1}}, \ldots, \frac{\partial u}{\partial x_{d}} ; \frac{\partial^{2} u}{\partial x_{1} \partial x_{1}}, \ldots, \frac{\partial^{2} u}{\partial x_{1} \partial x_{d}} ; \ldots ; \boldsymbol{\lambda}\right)=0, \quad \mathbf{x} \in \Omega$,
with suitable boundary conditions

$$
\mathcal{B}(u, \mathrm{x})=0 \quad \text { on } \quad \partial \Omega,
$$

where $\mathcal{B}(u, \mathbf{x})$ could be Dirichlet, Neumann, Robin, or periodic boundary conditions. For time-dependent problems, we consider time $t$ as a special component of $\mathbf{x}$, and $\Omega$ contains the temporal domain. The initial condition can be simply treated as a special type of Dirichlet boundary condition on the spatio-temporal domain.

PINNæ˜¯ç®—æ³•æµç¨‹ä»¥åŠç½‘ç»œç¤ºæ„å›¾å¦‚ä¸‹:

<img src='./Data/PINNs.png'>

<img src='./Data/PINNs1.png'>

<img src='./Data/PINNs2.png'>

å½“æˆ‘ä»¬è¾“å…¥ä¸€ä¸ªxï¼Œtåï¼Œå¾—åˆ°uï¼Œè€Œlosså‡½æ•°ä¸­çš„uå„ç§å¯¼æ•°ä¿¡æ¯ï¼Œå¯å€ŸåŠ©è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯(AD)æ±‚å¾—ã€‚

æœ€åï¼Œæˆ‘ä»¬ä¼šæœç´¢ä¸€ç»„å‚æ•°$\theta$æ¥æœ€å°åŒ–losså‡½æ•°ï¼Œè¿™ä¸ªè¿‡ç¨‹å°±æ˜¯æ‰€è°“çš„â€œtrainingâ€ã€‚è€ƒè™‘åˆ°è¿™ä¸ªlossæ˜¯å…³äº$\theta$é«˜åº¦éçº¿æ€§çš„å’Œéå‡¸çš„,æˆ‘ä»¬é€šå¸¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼ŒAdamå’ŒL-BFGSæ–¹æ³•è¿›è¡Œæœ€ä¼˜åŒ–è¿‡ç¨‹ã€‚

ä»¥ç»éªŒæ¥è¯´ï¼Œå¯¹äºå…‰æ»‘çš„PDEsä½¿ç”¨L-BFGSç›¸è¾ƒäºAdamèƒ½å¤Ÿåœ¨æ›´å°‘çš„è¿­ä»£æ¬¡æ•°ä¸­å–å¾—å¥½çš„è§£ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºL-BFGSæ˜¯ä¸€ç§æ‹Ÿç‰›é¡¿æ–¹æ³•ï¼Œåˆ©ç”¨åˆ°â€œäºŒé˜¶å¯¼ä¿¡æ¯â€ã€‚(è¿™ç‚¹åœ¨æˆ‘çš„parabolicè€¦åˆpdeä¸­ç¡®å®šæœ‰ä½“ç°ï¼Œæˆ‘çš„parabolicç®—ä¾‹åˆšå¥½æ˜¯è§£æ¯”è¾ƒå…‰æ»‘çš„)

è€Œå¯¹äºåˆšæ€§çš„PDEsï¼ŒL-BFGSå®¹æ˜“é™·å…¥ä¸€ä¸ªå±€éƒ¨bad minimumã€‚

PINNsçš„è¿­ä»£æ¬¡æ•°å¾ˆå¤§å–å†³äºPDEsçš„å¤æ‚æ€§ï¼ˆå…‰æ»‘çš„PDEsæ”¶æ•›é€Ÿåº¦å°±æ¯”è¾ƒå¿«ï¼‰ã€‚ä½¿ç”¨è‡ªé€‚åº”çš„æ¿€æ´»å‡½æ•°èƒ½å¤ŸåŠ é€Ÿè®­ç»ƒï¼Œå¹¶ä¸”å¯èƒ½remove bad local minimaã€‚

ä¸ä¼ ç»Ÿæ•°å€¼æ–¹æ³•ä¸åŒï¼ŒPINNså¯¹è§£çš„å”¯ä¸€æ€§æ²¡æœ‰ä¿è¯ï¼Œå› ä¸ºPINNsçš„è§£æ˜¯é€šè¿‡è§£å†³éå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œè¿™é€šå¸¸æ¥è¯´æ˜¯æ²¡æœ‰å”¯ä¸€è§£çš„ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬éœ€è¦è·³è½¬äººå·¥çš„å˜é‡ï¼Œe.g. ç½‘ç»œç»“æ„å’Œè§„æ¨¡å¤§å°ï¼Œå­¦ä¹ ç‡ï¼Œæ®‹å·®ç‚¹çš„æ•°é‡ç­‰å¾…ã€‚é€šå¸¸ï¼Œç½‘ç»œè§„æ¨¡å¤§å°å–å†³äºPDEsè§£çš„å…‰æ»‘æ€§ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå°å‹çš„ç½‘ç»œ(åªæœ‰å‡ ä¸ªå±‚ï¼Œæ¯å±‚ç¥ç»å…ƒæ•°é‡ä¹Ÿä¸å¤š)å°±è¶³ä»¥æ±‚è§£1-Dçš„Poissonæ–¹ç¨‹ã€‚è€Œå¯¹äº1Dçš„Burgersæ–¹ç¨‹ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´å¤šçš„å±‚(deeper)å’Œç¥ç»å…ƒä¸ªæ•°(wider)

å¯¹äºä¸åŒçš„åˆå§‹å€¼$\theta$,PINNså¯èƒ½æ”¶æ•›åˆ°ä¸åŒçš„è§£ï¼Œä¸€ç§ç­–ç•¥æ˜¯éšæœºåˆå§‹åŒ–å¤šæ¬¡è¿›è¡Œè®­ç»ƒï¼Œé€‰æ‹©lossæœ€å°çš„ç½‘ç»œä½œä¸ºè§£ã€‚

PINNsçš„åŸå§‹ç‰ˆæœ¬ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢è®¨è®ºçš„PINNsï¼Œå®é™…ä¸Šæˆ‘ä»¬æ˜¯å°†è¾¹ç•Œæ¡ä»¶ä½œä¸ºâ€œè½¯çº¦æŸ(åŠ å…¥loss)â€ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿåº”ç”¨äºä»»ä½•å½¢çŠ¶çš„è¾¹ç•Œæ¡ä»¶ä»¥åŠå¤æ•°åŸŸã€‚ä»å¦ä¸€ä¸ªæ–¹é¢æ¥è¯´ï¼Œå¯¹äºä¸€äº›ç®€å•çš„è¾¹ç•Œæ¡ä»¶ï¼Œå®é™…ä¸Šæˆ‘ä»¬å¯ä»¥ä»¤è¾¹ç•Œæ¡ä»¶å˜æˆâ€œå¼ºçº¦æŸâ€ã€‚ä¾‹å¦‚ï¼Œå½“è¾¹ç•Œæ¡ä»¶æ˜¯$u(0)=u(1)=0 \ with \Omega=[0,1]$,å¯ä»¥å°†è§£è®¾ç½®ä¸º $\hat{u(x)}=x(x-1)N(x)$ï¼Œè‡ªåŠ¨æ»¡è¶³è¾¹ç•Œæ¡ä»¶ï¼Œ$N(x)$æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚

å¯¹äºæ®‹å·®ç‚¹$\tau$çš„é€‰æ‹©å…·æœ‰çµæ´»å¤šæ ·æ€§ï¼Œä¸‹é¢æä¾›ä¸‰ç§ç­–ç•¥ï¼š

1. åœ¨è®­ç»ƒå¼€å§‹å‰å°±æŒ‡å®šæ®‹å·®ç‚¹ï¼Œå¯ä»¥æ˜¯ç½‘æ ¼ç‚¹æˆ–è€…éšæœºé€‰å–ã€‚è€Œä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å†æ”¹å˜å®ƒä»¬ã€‚
2. æ¯æ¬¡ä¼˜åŒ–è¿­ä»£ï¼Œéƒ½é‡æ–°éšæœºé€‰æ‹©ä¸€æ‰¹æ®‹å·®ç‚¹ã€‚
3. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æ®‹å·®ç‚¹çš„ä½ç½®,subsection2.8ã€‚

<img src="./Data/PINNs3.png">

**2.4 PINNsçš„é€¼è¿‘ç†è®ºä¸è¯¯å·®åˆ†æ**
ä¸‹é¢æ˜¯ä¸€äº›å¯¹PINNsç†è®ºæ€§åˆ†æã€‚

å¯¹äºPINNsï¼Œæˆ‘ä»¬å…³å¿ƒçš„æœ€æ ¹æœ¬çš„é—®é¢˜æ˜¯ï¼šæ˜¯å¦å­˜åœ¨ç¥ç»ç½‘ç»œèƒ½åŒæ—¶æ»¡è¶³è¾¹ç•Œæ¡ä»¶å’ŒPDEæ–¹ç¨‹ï¼Œi.e. æ˜¯å¦å­˜åœ¨ç¥ç»ç½‘ç»œèƒ½åŒæ—¶ä¸”ä¸€è‡´åœ°é€¼è¿‘ä¸€ä¸ªå‡½æ•°åŠå…¶åå¯¼æ•°ã€‚

<center>
    <img src="./Data/PINNs4.png" width="70%">,
    <img src="./Data/PINNs5.png" width="70%">
    <br/>
</center>

å®šç†2.1ç”±Pinkusæå‡ºï¼Œæè¿°äº†å•éšè—å±‚çš„ç¥ç»ç½‘ç»œçš„å‡½æ•°è¡¨ç¤ºèƒ½åŠ›ã€‚ä¸€è¨€ä»¥è”½ä¹‹ï¼Œè¶³å¤Ÿå¤šç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œï¼Œèƒ½å¤ŸåŒæ—¶ä¸”ä¸€è‡´åœ°é€¼è¿‘ä¸€ä¸ªå‡½æ•°åŠå…¶åå¯¼æ•°ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œç¥ç»ç½‘ç»œçš„å¤§å°æ€»æ˜¯æœ‰é™çš„ã€‚å‡è®¾$\mathcal{F}$
è¡¨ç¤ºæŸä¸ªç¥ç»ç½‘ç»œèƒ½è¡¨ç¤ºçš„å…¨ä½“å‡½æ•°ï¼Œé€šå¸¸æ¥è¯´uä¸å¤ªå¯èƒ½å±äº$\mathcal{F}$,
å®šä¹‰ $u_{ \mathcal{F}} = arg min_{f \in \mathcal{F}}||f-u||$,$u_{ \mathcal{F}}$æ˜¯æœ€æ¥è¿‘uçš„å‡½æ•°ï¼Œå°±åƒä¸‹å›¾å±•ç¤ºçš„é‚£æ ·ã€‚

è€Œæˆ‘ä»¬æ˜¯åœ¨è®­ç»ƒé›†ä¸Šä¼˜åŒ–ç¥ç»ç½‘ç»œï¼Œå®šä¹‰$u_{\tau} = arg min_{f \in \mathcal{F}} \mathcal{L}(f;\tau)$ä½œä¸ºç¥ç»ç½‘ç»œåœ¨è®­ç»ƒé›†ä¸Šçš„æœ€ä¼˜è§£ã€‚ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾$u,u_{\mathcal{F}},u_{\tau}$éƒ½å…·æœ‰å”¯ä¸€æ€§å’Œè‰¯å®šçš„ã€‚è€Œæˆ‘ä»¬è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—æ˜¯å¤æ‚çš„ï¼Œé€šå¸¸åªèƒ½è¿”å›ä¸€ä¸ª$u_{\tau}$çš„è¿‘ä¼¼è§£
$u_{\hat{\tau}}$ã€‚

å¦‚ä¸‹å›¾æ‰€æŒ‡ï¼Œæˆ‘ä»¬å¯ä»¥è§£æ„æ•´ä½“æ€»ä½“è¯¯å·®$\epsilon$ä¸ºä¸‰ä¸ªéƒ¨åˆ†ã€‚

<center>
    <img src="./Data/PINNs6.png" width="80%">,
    <img src="./Data/PINNs7.png" height="100%" width="100%">
    <br/>
</center>

è¿‘ä¼¼è¯¯å·®$\mathcal{E}_{app}$è¡¡é‡çš„æ˜¯$u_{\mathcal{F}}$è¿‘ä¼¼$u$çš„ç¨‹åº¦ã€‚æ³›åŒ–è¯¯å·®$\mathcal{E}_{gen}$ç”±è®­ç»ƒé›†ä¸­æ®‹å·®ç‚¹çš„æ•°é‡/ä½ç½®å’Œ$\mathcal{f}$æ—çš„å®¹é‡å†³å®šã€‚ç¥ç»ç½‘ç»œè§„æ¨¡è¶Šå¤§ï¼Œå…¶è¿‘ä¼¼è¯¯å·®è¶Šå°ï¼Œä½†æ³›åŒ–è¯¯å·®è¶Šå¤§ï¼Œè¿™è¢«ç§°ä¸ºåå·®-æ–¹å·®æƒè¡¡ã€‚å½“æ³›åŒ–è¯¯å·®å ä¸»å¯¼åœ°ä½æ—¶ï¼Œä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼Œä¼˜åŒ–è¯¯å·®$\mathcal{E}_{opt}$æ¥æºäºæŸå¤±å‡½æ•°çš„å¤æ‚æ€§å’Œä¼˜åŒ–è®¾ç½®ï¼Œå¦‚å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ã€‚ç„¶è€Œï¼Œç›®å‰è¿˜æ²¡æœ‰å…³äºPINNsçš„è¯¯å·®ä¼°è®¡ï¼Œç”šè‡³å¯¹ç›‘ç£å­¦ä¹ çš„ä¸‰ç§è¯¯å·®è¿›è¡Œé‡åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜[36,35,25]ã€‚

---

# 08-04

## [DeepXDEè®ºæ–‡é˜…è¯»(3)]

[DeepXDE- A Deep Learnin Library for Solving Differential Equations](../è®ºæ–‡èµ„æ–™/DeepXDE-%20A%20Deep%20Learning%20Library%20for%20Solving%20Differential%20Equations.pdf)

### **2.5 PINNsä¸FEMçš„æ¯”è¾ƒ**

<center>
    <img src="./Data/PINNs8.png" width="80%">
</center>

- åœ¨FEMä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªåˆ†æ®µå¤šé¡¹å¼æ¥è¿‘ä¼¼è§£uï¼Œè€Œåœ¨PINNsä¸­ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªç¥ç»ç½‘ç»œä½œä¸ºæ›¿ä»£æ¨¡å‹ï¼Œå‚æ•°ç”±æƒé‡å’Œåå·®ã€‚
- FEMé€šå¸¸éœ€è¦ç½‘æ ¼ç”Ÿæˆï¼Œè€ŒPINNsæ˜¯å®Œå…¨æ— ç½‘æ ¼çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç½‘æ ¼æˆ–éšæœºç‚¹ã€‚
- FEMåˆ©ç”¨åˆšåº¦çŸ©é˜µå’Œè´¨é‡çŸ©é˜µå°†åå¾®åˆ†æ–¹ç¨‹è½¬åŒ–ä¸ºä»£æ•°ç³»ç»Ÿï¼Œè€ŒPINNåˆ™å°†åå¾®åˆ†æ–¹ç¨‹å’Œè¾¹ç•Œæ¡ä»¶åµŒå…¥åˆ°æŸå¤±å‡½æ•°ä¸­ã€‚
- åœ¨æœ€åä¸€æ­¥ï¼Œæœ‰é™å…ƒä¸­çš„ä»£æ•°ç³»ç»Ÿæ˜¯ç”¨çº¿æ€§è§£ç®—å™¨ç²¾ç¡®æ±‚è§£çš„ï¼Œè€ŒPINNsä¸­çš„ç½‘ç»œæ˜¯ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å™¨å­¦ä¹ çš„ã€‚

åœ¨æ ¹æœ¬ä¸Šä¸åŒçš„æ˜¯ï¼ŒPINNsæä¾›äº†å‡½æ•°åŠå…¶å¯¼æ•°çš„éçº¿æ€§é€¼è¿‘ï¼Œè€ŒFEMè¡¨ç¤ºçº¿æ€§é€¼è¿‘ã€‚

### **2.6 PINNsæ±‚è§£ç§¯åˆ†å¾®åˆ†æ–¹ç¨‹**

åœ¨æ±‚è§£ç§¯åˆ†å¾®åˆ†æ–¹ç¨‹(IDEs)æ—¶ï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯æ¥è§£æåœ°å¯¼å‡ºæ•´æ•°é˜¶å¯¼æ•°ï¼Œè€Œæˆ‘ä»¬ä½¿ç”¨ç»å…¸æ–¹æ³•æ¥æ•°å€¼é€¼è¿‘ç§¯åˆ†ç®—å­ï¼Œå¦‚é«˜æ–¯æ±‚ç§¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬å››ä¸ªè¯¯å·®åˆ†é‡ï¼Œå³ç¦»æ•£åŒ–è¯¯å·®$\mathcal{E}_{dis}$ï¼Œè¿™æ˜¯ç”±é«˜æ–¯æ±‚ç§¯è¿‘ä¼¼å¾—åˆ°çš„ã€‚

For example, when solving

$$
\frac{d y}{d x}+y(x)=\int_{0}^{x} e^{t-x} y(t) d t
$$

we first use Gaussian quadrature of degree $n$ to approximate the integral

$$
\int_{0}^{x} e^{t-x} y(t) d t \approx \sum_{i=1}^{n} w_{i} e^{t_{i}(x)-x} y\left(t_{i}(x)\right)
$$

and then we use a PINN to solve the following PDE instead of the original equation:

$$
\frac{d y}{d x}+y(x) \approx \sum_{i=1}^{n} w_{i} e^{t_{i}(x)-x} y\left(t_{i}(x)\right)
$$

PINNs can also be easily extended to solve FDEs and SDEs , but we do not discuss such cases here due to page limit constraints.

<center>
    <img src="./Data/PINNs9.png" width="80%">
</center>

### **2.7 PINNsæ±‚è§£åé—®é¢˜**

åé—®é¢˜é€šå¸¸æ˜¯æŒ‡ï¼Œåœ¨PDEsä¸­æœ‰æœªçŸ¥çš„å‚æ•°$\lambda$,ç›¸å¯¹åœ°ï¼Œæˆ‘ä»¬æ‹¥æœ‰ä¸€äº›é¢å¤–çš„ä¿¡æ¯ï¼Œæ¯”å¦‚æŒ‡å®šæŸä¸ªå­åŒºåŸŸ$\mathcal{T}_i$çš„è§£æˆ–è€…ç›¸å…³ä¿¡æ¯ã€‚

PINNsæ±‚è§£åé—®é¢˜ï¼Œå®é™…ä¸Šä¸æ±‚è§£æ­£é—®é¢˜æ˜¯ä¸€æ ·çš„ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æ¨¡å‹ä¸­æ–°å¢$\lambda$å˜é‡ï¼Œæ ¹æ®é¢å¤–çš„ä¿¡æ¯åœ¨losså‡½æ•°ä¸­å¢é¡¹ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸€èµ·ä¼˜åŒ–$\theta å’Œ\lambda$å³å¯ã€‚

$$
\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\lambda} ; \mathcal{T})=w_{f} \mathcal{L}_{f}\left(\boldsymbol{\theta}, \boldsymbol{\lambda} ; \mathcal{T}_{f}\right)+w_{b} \mathcal{L}_{b}\left(\boldsymbol{\theta}, \boldsymbol{\lambda} ; \mathcal{T}_{b}\right)+w_{i} \mathcal{L}_{i}\left(\boldsymbol{\theta}, \boldsymbol{\lambda} ; \mathcal{T}_{i}\right)
$$

where

$$
\mathcal{L}_{i}\left(\theta, \lambda ; \mathcal{T}_{i}\right)=\frac{1}{\left|\mathcal{T}_{i}\right|} \sum_{\mathbf{x} \in \mathcal{T}_{i}}\|\mathcal{I}(\hat{u}, \mathrm{x})\|_{2}^{2}
$$

We then optimize $\boldsymbol{\theta}$ and $\boldsymbol{\lambda}$ together, and our solution is $\boldsymbol{\theta}^{*}, \boldsymbol{\lambda}^{*}=\arg \min _{\boldsymbol{\theta}, \boldsymbol{\lambda}} \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\lambda} ; \mathcal{T})$.

### **2.8 åŸºäºæ®‹å·®çš„è‡ªé€‚åº”ä¼˜åŒ–(RAR)**

æ­£å¦‚æˆ‘ä»¬åœ¨2.3å°èŠ‚ä¸­æ‰€è®¨è®ºçš„ï¼Œæ®‹å·®ç‚¹$\mathcal{T}$é€šå¸¸åœ¨å®šä¹‰åŸŸå†…éšæœºåˆ†å¸ƒã€‚è¿™åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½å¯ä»¥å¾ˆå¥½åœ°å·¥ä½œï¼Œä½†æ˜¯å¯¹äºæŸäº›æ˜¾ç¤ºå…·æœ‰é™¡å³­æ¢¯åº¦çš„è§£å†³æ–¹æ¡ˆçš„åå¾®åˆ†æ–¹ç¨‹æ¥è¯´ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ã€‚ä»¥Burgersæ–¹ç¨‹ä¸ºä¾‹;ç›´è§‚åœ°è¯´ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨é”‹é¢é™„è¿‘æ”¾ç½®æ›´å¤šçš„ç‚¹ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰åˆ°ä¸è¿ç»­æ€§ã€‚ç„¶è€Œï¼Œä¸€èˆ¬æ¥è¯´ï¼Œå¯¹äºè§£æœªçŸ¥çš„é—®é¢˜ï¼Œè®¾è®¡ä¸€ä¸ªå¥½çš„å‰©ä½™ç‚¹åˆ†å¸ƒæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ®‹å·®çš„è‡ªé€‚åº”ç»†åŒ–(RAR)æ–¹æ³•ï¼Œä»¥æ”¹å–„è®­ç»ƒè¿‡ç¨‹ä¸­æ®‹å·®ç‚¹çš„åˆ†å¸ƒï¼Œåœ¨æ¦‚å¿µä¸Šç±»ä¼¼äºæœ‰é™å…ƒç»†åŒ–æ–¹æ³•ã€‚

RARçš„æ€æƒ³æ˜¯ï¼š
åœ¨PDEæ®‹å·®æ›´å¤§çš„ä½ç½®ä¸Šæ·»åŠ æ›´å¤šçš„æ®‹å·®ç‚¹ï¼Œæˆ‘ä»¬åå¤åŠ ç‚¹ï¼Œç›´åˆ°æ•´ä¸ªåŒºåŸŸå†…çš„æ®‹å·®çš„å¹³å‡å€¼å°äºä¸€ä¸ªé˜ˆå€¼ã€‚

ç®—æ³•æµç¨‹å›¾å¦‚ä¸‹ï¼š

<center>
    <img src="./Data/PINNs10.png" width="80%">
</center>

---

# 08-05

## å„ç§ä¼˜åŒ–ç®—æ³•

æ¢¯åº¦ä¸‹é™ç®—æ³•æ˜¯ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹å¼ï¼Œåœ¨æœ€ä¼˜åŒ–é¢†åŸŸæ¢¯åº¦ä¸‹é™ä¹Ÿæ˜¯æœ€åŸºæœ¬çš„æ–¹æ³•ï¼Œæ‰€è°“çš„è®­ç»ƒç¥ç»ç½‘ç»œå°±æ˜¯æœ€ä¼˜åŒ–losså‡½æ•°ã€‚
ç°åœ¨çš„å„ç§ä¼˜åŒ–ç®—æ³•å¦‚Adamã€RMSpropç­‰ç­‰ï¼Œéƒ½æ˜¯åŸºäºæ¢¯åº¦ä¸‹é™çš„ç®—æ³•ã€‚
ç„¶è€Œï¼Œæ ‡å‡†çš„æ¢¯åº¦ä¸‹é™æ³•è¦æ±‚çš„è®¡ç®—é‡æ˜¯å¾ˆå¤§çš„ï¼Œéœ€è¦å¯¹å®ƒè¿›è¡Œä¼˜åŒ–ï¼Œç”¨æ›´å°‘çš„è®¡ç®—å®ç°å·®ä¸å¤šçš„æ•ˆæœã€‚ä¼˜åŒ–æ¢¯åº¦ä¸‹é™æ³•æœ‰ä¸¤ä¸ªæ€è·¯ï¼šä¼˜åŒ–ç¥ç»ç½‘ç»œç»“æ„å’Œä¼˜åŒ–æ¢¯åº¦ä¸‹é™æ³•æœ¬èº«ã€‚

## 1.å‡å°‘è®¡ç®—é‡

### 1.1 éšæœºæ¢¯åº¦ä¸‹é™æ³•

ä»¥äº¤å‰ç†µè®¡ç®—æŸå¤±å‡½æ•°ä¸ºä¾‹,$loss=- \sum_{i=1}^n(y_i*\log_2 \hat{y_i} + (1-y_i)*\log_2(1-\hat{y_i}))$ï¼Œå…¶ä¸­nä»£è¡¨è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œè®­ç»ƒé›†çš„æ•°é‡å¾ˆå¤§ï¼Œæ¯æ¬¡è®­ç»ƒå¦‚æœæŠŠæ‰€æœ‰æ ·æœ¬éƒ½è®¡ç®—ä¸€éçš„è®¡ç®—é‡å¤ªå¤§äº†ã€‚

ä¼˜åŒ–æ€è·¯ï¼š

1. å‡å°‘æ¯æ¬¡è®­ç»ƒè®¡ç®—é‡
2. ä¼˜åŒ–ä¸‹é™çš„è·¯å¾„ï¼Œæ›´å°‘çš„æ­¥æ•°æ›´å¿«åœ°è¾¾åˆ°æå€¼ç‚¹

éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼š
ä»æœŸæœ›çš„è§’åº¦ç†è§£æŸå¤±å‡½æ•°ï¼Œ$loss=\frac{- \sum_{i=1}^n(y_i*\log_2 \hat{y_i} + (1-y_i)*\log_2(1-\hat{y_i}))}{N}$ã€‚éšæœºè°ƒä¸€ä¸ªæ•°æ®ï¼Œç”¨è¿™ä¸ªæ•°æ®è®¡ç®—æ¢¯åº¦ï¼Œ ä¿®æ”¹å‚æ•°å€¼ï¼Œä¸‹æ¬¡è®­ç»ƒæ—¶ï¼Œå†éšæœºæŒ‘ä¸€ä¸ªæ•°æ®....

éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„æ”¶æ•›æ€§ï¼š å‡¸é—®é¢˜: $f(x^{(k)})-f^*= O(\frac{1}{\sqrt k}),kä»£è¡¨è¿­ä»£æ¬¡æ•°ï¼Œf^*ä»£è¡¨æå€¼ç‚¹$

### 1.2 Mini-batchæ–¹æ³•

mini-batchæ˜¯ç°åœ¨çš„éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„åˆ«ç§°ï¼Œæ¯æ¬¡ä¸æ­¢è°ƒä¸€ä¸ªæ•°æ®ï¼Œè€Œæ˜¯æŒ‘ä¸€ä¸ªbatchçš„æ•°æ®è®­ç»ƒã€‚

## 2. ä¼˜åŒ–ä¸‹é™è·¯å¾„

ä¸¥æ ¼æ¥è¯´ï¼Œç”¨ä¸€é˜¶taylorå±•å¼€ï¼Œæ¢¯åº¦æŒ‡å‘çš„æ˜¯ä¸Šå‡æœ€å¿«çš„æ–¹å‘ï¼Œè´Ÿæ¢¯åº¦æ‰æ˜¯ä¸‹é™æœ€å¿«çš„æ–¹å‘ã€‚è€Œæ¢¯åº¦æ˜¯æŸä¸€ä¸ªç‚¹çš„ä¸‹é™æœ€å¿«çš„æ–¹å‘ï¼Œå¦‚æœæƒ³è¦æŠŠæ•´ä¸ªä¸‹é™æœ€ä¼˜çš„è·¯å¾„ä¹Ÿæç»˜å‡ºæ¥ï¼Œæ¯æ¬¡è¿­ä»£çš„æ­¥é•¿è¦æ— çº¿å°æ‰è¡Œã€‚æ•…ï¼Œæ¯æ¬¡è¿­ä»£æœ‰ä¸€ä¸ªç¡®å®šçš„æ­¥é•¿ï¼Œè€Œæœ‰äº†è¿™ä¸ªæ­¥é•¿é‚£ä¹ˆä¸‹é™è·¯å¾„ä¸€å®šä¸ä¼šè·Ÿæœ€ä¼˜çš„ä¸‹é™è·¯çº¿å®Œå…¨é‡åˆã€‚

<img src="./Data/grad1.png">

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¦‚æœæ¯ä¸€ä¸ªä¸‹é™çš„æ­¥é•¿å¤ªé•¿ï¼Œå®ƒå¯èƒ½åç¦»æœ€ä¼˜çš„è·¯å¾„ã€‚å­¦ä¹ æ­¥é•¿ç”±å­¦ä¹ ç‡å†³å®šã€‚A->A'ç»è¿‡Bç‚¹ï¼Œè€ŒBç‚¹çš„æœ€é€Ÿä¸‹é™æ–¹å‘å·²ç»ä¸æ˜¯AA'äº†ã€‚

å¦‚ä½•ä¿è¯åœ¨ä¸€å®šçš„å­¦ä¹ æ­¥é•¿ï¼ŒåŒæ—¶åˆè¾ƒå¥½åœ°è´´è¿‘æœ€ä¼˜è·¯å¾„å‘¢ï¼Ÿ

### 2.1 Newtonæ³•

æ¢¯åº¦ä¸‹é™æ³•å®é™…ä¸Šæ˜¯ä¸€é˜¶æ³°å‹’å±•å¼€ï¼Œå¦‚æœæ­¥é•¿å¤ªå¤§ï¼Œåå·®ä¼šè¾ƒå¤§ã€‚è€ŒNewtonæ³•ä½¿ç”¨äºŒé˜¶æ³°å‹’å±•å¼€é€¼è¿‘ï¼ŒäºŒæ¬¡å‡½æ•°æœ‰é¡¶ç‚¹(æœ€å°å€¼ç‚¹)ï¼Œå½“å­¦ä¹ æ­¥é•¿å–å½“å‰xåˆ°è¯¥é¡¶ç‚¹çš„$\Delta x$æ—¶è®­ç»ƒæ•ˆæœæœ€å¥½ï¼Œä¸€æ—¦è¿‡äº†è¯¥é¡¶ç‚¹ï¼Œè¿‘ä¼¼æ•ˆæœå¯èƒ½è¿˜ä¸å¦‚ä¸€é˜¶å¯¼ã€‚åœ¨ä¸€ç»´çš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯å¾ˆæ˜æ˜¾çš„ï¼Œå¦‚å›¾æ‰€ç¤ºã€‚ç‰›é¡¿æ³•çš„å­¦ä¹ æ­¥é•¿æ˜¯ç¡®å®šçš„ã€‚

<img src="./Data/grad2.png" width="80%">

å¯¹äºå¤šä¸ªè‡ªå˜é‡çš„æƒ…å†µï¼Œæ ‡å‡†çš„Newtonæ³•æ¯æ¬¡è¦ç®—äºŒé˜¶å¯¼çš„é»‘å¡çŸ©é˜µï¼Œè®¡ç®—ä¸Šéš¾ä»¥æ¥å—ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç‰›é¡¿æ³•èƒ½ç»™äº†æˆ‘ä»¬ä¼˜åŒ–æ€è·¯ã€‚å› ä¸ºç‰›é¡¿æ³•æœ¬è´¨æ˜¯æŠŠä¸‹é™è·¯å¾„çš„æ‰€æœ‰ç»´åº¦æ”¾åœ¨ä¸€èµ·ï¼Œç»Ÿä¸€è€ƒè™‘(é»‘å¡çŸ©é˜µï¼Œæ‰€æœ‰ç»´åº¦çš„2é˜¶å¯¼ä¿¡æ¯)ï¼Œå¯»æ‰¾æ›´å¥½çš„è·¯å¾„ã€‚

ä¸‹é™è·¯å¾„çš„ç»´åº¦æ‹†åˆ†å¼€ï¼Œä¸€ä¸ªä¸€ä¸ªç»´åº¦è€ƒè™‘ã€‚

### 2.2 åŠ¨é‡æ³•(å†²é‡æ³•)

<img src="./Data/grad3.png" width="80%">

å¦‚å›¾æ‰€ç¤ºï¼Œæ©™è‰²çš„è·¯å¾„æŒ¯è¡åœ°è¶‹äºæå€¼ç‚¹ï¼ŒæŒ¯è¡æ˜¯æˆ‘ä»¬ä¸å¤ªæƒ³çœ‹åˆ°çš„ã€‚å¦‚æœæŠŠä¸‹é™æ–¹å‘æ‹†åˆ†æˆ æ¨ªè½´å’Œçºµè½´çš„åˆ†é‡ï¼Œå‘ç°æŒ¯è¡çš„åŸå› å°±çºµè½´çš„åˆ†é‡ä¸æ–­æ­£è´Ÿå˜åŒ–ï¼Œæ¨ªè½´åˆ†é‡æ˜¯ä¸€ç›´æŒ‡å‘æå€¼ç‚¹çš„ã€‚è€Œç»¿è‰²çš„è·¯å¾„ï¼Œçºµè½´ä¸Šçš„æŒ¯è¡å‡å°ï¼Œæ¨ªè½´ä¸Šè·¨åº¦åˆå¢åŠ ã€‚

å¦‚ä½•åšå¾—è¿™ä¸€ç‚¹çš„ï¼Ÿåˆ©ç”¨å†å²çš„æ¢¯åº¦ä¿®æ­£ã€‚å°†å›¾ä¸­æ©™è‰²è·¯å¾„ä¸Šä¸€æ­¥çš„æ¢¯åº¦å’Œå½“å‰æ­¥çš„æ¢¯åº¦ç›¸åŠ ï¼Œé‚£ä¹ˆç”±äºçºµè½´åˆ†é‡æ–¹å‘ç›¸åæŠµæ¶ˆï¼Œæ¨ªè½´åˆ†é‡æ–¹å‘ç›¸åŒï¼Œé‚£ä¹ˆç›¸åŠ åçš„æ–¹å‘å°±æ˜¯å›¾ä¸­ç»¿è‰²è·¯å¾„çš„æ–¹å‘ã€‚

<img src="./Data/grad4.png" width="80%">
<img src="./Data/grad5.png" width="80%">

$V_{(t)}$æ˜¯é€’å½’å®šä¹‰ï¼Œ$V_{(t)}$ç­‰äºä¸Šä¸€æ­¥çš„$V_{(t-1)}$+å½“å‰æ­¥çš„æ¢¯åº¦$\Delta W_{(t)i} \ å®ƒè¡¨ç¤ºç¬¬tæ­¥ï¼Œç¬¬iä¸ªå˜é‡W_içš„æ¢¯åº¦$ã€‚

è¿™ç§å®šä¹‰çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¦‚æœæ­¥æ•°å¤Ÿå¤šï¼Œæ‰€æœ‰å†å²æ•°æ®å°†ä¸€è§†åŒä»å…¨éƒ¨è€ƒè™‘ã€‚
æˆ‘ä»¬å¯ä»¥å¯¹Våšä¸€ä¸ª `æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡æ³•`ï¼Œä½¿å¾—è¶Šè¿‘çš„å†å²æ•°æ®æƒé‡è¶Šå¤§ï¼Œè¶Šè¿œçš„æ•°æ®æƒé‡è¶Šå°(è¶‹äº0)ã€‚

$V_{(t)} = \beta * V_{(t-1)} + (1-\beta)*\Delta W_{(t)i}$

<img src="./Data/grad6.png" width="80%">

### 2.3 Nesterovæ–¹æ³•

ä¸æ­¢è€ƒè™‘å†å²æ•°æ®ï¼Œè¿˜èƒ½è¶…å‰çš„è€ƒè™‘"æœªæ¥"çš„æ•°æ®ã€‚

<center>
    <img src="./Data/grad7.png" width="40%">
    <img src="./Data/grad8.png" width="40%">
</center>

ä¸Šå›¾ä¸­ï¼Œç»¿è‰²è™šçº¿ä»£è¡¨å†å²å†²é‡ï¼Œçº¢è‰²è™šçº¿ä»£è¡¨æ¢¯åº¦æ–¹æ³•ï¼Œçº¢è‰²å®çº¿ä»£è¡¨ä¸‹é™æ–¹å‘ã€‚ç»“åˆè¿™ä¸¤å¹…å›¾å¯ä»¥å‘ç°ï¼Œä¼˜åŒ–è·¯å¾„æ˜¯æœ‰æ›²æŠ˜çš„ï¼Œå‘å¤–ç§»åŠ¨ä¸€äº›ï¼Œç»•äº†ä¸€ç‚¹è¿œè·¯ã€‚

é‚£ä¹ˆï¼Œä»å·¦å›¾å¼€å§‹ï¼Œèƒ½å¦é€šè¿‡è°ƒæ•´ä¸‹é™æ–¹å‘ç›´æ¥å¾€é‡Œåç§»ï¼ŸNesterovæ–¹æ³•ã€‚

<center>
<img src="./Data/grad9.png" width="80%">
</center>

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒNesterovæ–¹æ³•å¯¹å½“å‰æ­¥çš„â€œæ¢¯åº¦â€åšäº†è°ƒæ•´(å¤§çº¢ç®­å¤´)ã€‚

åœ¨å†²é‡æ³•ä¸­ï¼Œè®¡ç®—æ¯æ­¥çš„ä¸‹é™æ–¹å‘ï¼Œéœ€è¦å…ˆè®¡ç®—å½“å‰ç‚¹çš„æ¢¯åº¦å€¼ï¼Œå†è°ƒæ•´$V_{(t-1)}->V_{(t)}$ã€‚è€ŒNesterovæ–¹æ³•ï¼Œåœ¨ç¬¬tæ­¥ä¸­ï¼Œç›´æ¥å°†ä¸Šä¸€æ­¥çš„å†²é‡$V_{(t-1)}$ä½œä¸ºä¸‹é™æ–¹æ³•ï¼Œè®¡ç®—ä¸€ä¸ªâ€œä¸´æ—¶çš„â€$W_{(t)i}=W_{(t-1)i}+\gamma *V_{(t-1)}$(è¿™å°±æ˜¯æ‰€è°“çš„è¶…å‰â€œè€ƒè™‘â€æœªæ¥çš„æ•°æ®)ã€‚

å¯¹è¿™ä¸ªâ€œä¸´æ—¶çš„â€$W_{(t)i}$æ±‚åå¯¼ï¼Œå¾—åˆ°ç¬¬tæ­¥çš„â€œæ¢¯åº¦â€$\Delta W_{(t)i}$ï¼ˆå·²ç»ä¸æ˜¯åŸæ¥æ„ä¹‰ä¸Šçš„æ¢¯åº¦äº†ï¼‰ã€‚ç„¶ååœ¨ä¿®æ­£$V_{(t-1)}->V_{(t)}$ï¼Œæ±‚å¾—ä¸‹ä¸€æ­¥çš„Wã€‚

Nesterovæ–¹æ³•ä¸­çš„â€œæ¢¯åº¦â€$\Delta W_{(t)i}$å°±æ˜¯ä¸Šå›¾ä¸­ç´«è‰²çš„è™šçº¿ï¼Œå®ƒå®é™…ä¸Šå°±æ˜¯ç»¿è‰²è™šçº¿ç®­å¤´æŒ‡å‘çš„ç‚¹çš„æ¢¯åº¦å‘é‡ï¼Œå¹³ç§»åˆ°ä¸Šé¢ã€‚

### 2.4 AdaGradæ–¹æ³•

å­¦ä¹ ç‡åº”è¯¥éšç€ä¼˜åŒ–è¿‡ç¨‹å‡å°‘ã€‚å¦‚æœå­¦ä¹ ç‡å›ºå®šä¸å˜ï¼Œå¾ˆå¯èƒ½ç”±äºæ— æ³•æ°å¥½è¾¾åˆ°æå€¼ç‚¹ï¼Œåœ¨æå€¼ç‚¹é™„è¿‘æŒ¯è¡ã€‚æœ€ç®€å•çš„æ–¹æ³•ï¼Œæ¯æ¬¡è¿­ä»£å­¦ä¹ å‡å°‘å›ºå®šæ•°å€¼ï¼Œä½†å‡å°‘çš„æ•°å€¼æ˜¯äººä¸ºç»™å®šçš„ã€‚å¦‚æœè®¾å®šå¾ˆå¤§ï¼Œå¯èƒ½è¿˜æ²¡è¾¾åˆ°æå€¼ç‚¹ï¼Œå­¦ä¹ ç‡å°±ä¸º0ï¼Œåœæ­¢è®­ç»ƒã€‚å¦‚æœè®¾å®šå¾—å¤ªå°ï¼Œå¯èƒ½å·²ç»åˆ°è¾¾æå€¼ç‚¹äº†ï¼Œè¿˜è¦æŒ¯è¡å¾ˆä¹…æ‰ä¼šåœæ­¢ï¼Œæµªè´¹è®¡ç®—ã€‚

**å¦‚ä½•è®©å­¦ä¹ ç‡è‡ªåŠ¨è°ƒæ•´ï¼Ÿ** ä¹Ÿæ˜¯åŸºäºå†å²æ•°æ®ã€‚

<img src="./Data/grad10.png" width="">

ä¸Šå›¾ä¸­ï¼Œ$S_{(t)}$æ ¹æ®å†å²æ•°æ®çš„å˜åŒ–é‡è€Œå®šï¼Œå½“å†å²æ•°æ®ä¿®æ­£çš„è¶Šå¤šï¼Œ$S_{(t)}$å°±æ›´å¤§ï¼Œä»è€Œå­¦ä¹ ç‡$\frac{\eta}{\sqrt S_{(t)}+\epsilon}$å‡å°‘çš„å°±è¶Šå¤šã€‚

æ¢¯åº¦çš„å†…ç§¯å¼€æ–¹ï¼Œå­¦ä¹ åˆ°çš„æ¢¯åº¦æ˜¯çœŸå®æ¢¯åº¦é™¤ä»¥æ¢¯åº¦å†…ç§¯çš„å¼€æ–¹ã€‚adagradæœ¬è´¨æ˜¯è§£å†³å„æ–¹å‘å¯¼æ•°æ•°å€¼é‡çº§çš„ä¸ä¸€è‡´è€Œå°†æ¢¯åº¦æ•°å€¼å½’ä¸€åŒ–ã€‚

AdaGradä¸­è‡ªé€‚åº”å­¦ä¹ ç‡çš„ç»“æœå’Œç‰›é¡¿æ³•ä¸­å¯¹hessiançŸ©é˜µåšnormal approximationæ˜¯ç±»ä¼¼çš„ï¼ˆé™¤äº†å¤šäº†å¼€æ ¹å·ï¼‰ã€‚è®¾Jä¸ºæ¢¯åº¦é˜µï¼Œå®¹æ˜“è¯æ˜JJ^Tæ˜¯å¯¹ç§°åŠæ­£å®šçš„çŸ©é˜µï¼ˆåŠ å’Œæ›´æ˜¯å¦‚æ­¤ï¼‰ï¼Œä¸ºä½¿æ•°å€¼ç¨³å®šåŠ ä¸Šäº†ä¸€ä¸ªè½»å¾®æ‰°åŠ¨ã€‚

AdaGradæ–¹æ³•å…¶å®ä¸æ­¢å¯ä»¥è®©å­¦ä¹ è¿‡ç¨‹æœ€ç»ˆåœä¸‹æ¥ï¼Œæ›´å¯ä»¥è°ƒæ•´ä¸åŒç»´åº¦ä¸Šæ¢¯åº¦æ•°é‡çº§ä¸ä¸€è‡´çš„é—®é¢˜ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç´«è‰²çš„çº¿ä»£è¡¨å†²é‡æ³•ï¼Œç°è‰²çš„çº¿ä»£è¡¨AdaGradæ–¹æ³•ã€‚ç”±äºåœ¨åˆå§‹ç‚¹ï¼Œçºµå‘çš„æ¢¯åº¦æ›´å¤§ï¼Œç´«è‰²çº¿(å†²é‡æ³•)æ²¿ç€çºµå‘ç–¯ç‹‚ç§»åŠ¨ã€‚è€ŒAdaGradæ–¹æ³•åšäº†å­¦ä¹ ç‡ä¸­ä¸åŒç»´åº¦æ•°é‡çº§çš„è°ƒæ•´($\eta æ˜¯ä¸€ä¸ªå‘é‡$)ï¼Œæ²¿ç€ç°è‰²çº¿ç§»åŠ¨ã€‚

<img src="./Data/grad11.png">

AdaGradæ–¹æ³•å¯ä»¥åšä¼˜åŒ–ï¼Œä¸è¦æŠŠæ‰€æœ‰çš„å†å²åŒ…è¢±éƒ½è€ƒè™‘è¿›æ¥ï¼Œåªè€ƒè™‘æ¯”è¾ƒå½“å‰æ¯”è¾ƒè¿‘çš„éƒ¨åˆ†å†å²ã€‚åšæ³•è¿˜æ˜¯ `æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡æ³•`ï¼Œä¸å†èµ˜è¿°ã€‚

ä¼˜åŒ–è¿‡åAdaGradçš„ç®—æ³•ç§°ä¸º `RMSpropæ–¹æ³•`ã€‚

### 2.5 Adamå’ŒNadamæ–¹æ³•

AdaGrad(RMSprop)æ–¹æ³•åªè€ƒè™‘ä¿®æ­£å­¦ä¹ ç‡ï¼Œå®é™…ä¸Šå¯ä»¥æŠŠAdaGradæ–¹æ³•å’ŒåŠ¨é‡æ³•ç»“åˆèµ·æ¥ï¼Œå› ä¸ºåŠ¨é‡æ³•åªè€ƒè™‘ä¿®æ­£äº†ä¸‹é™æ–¹å‘ã€‚

è¿™ä¸ªç»“åˆçš„ç®—æ³•å°±æ˜¯Adamç®—æ³•ã€‚

<img src="./Data/grad12.png">

è€ŒAdaGrad(RMSprop) ä¸ Nesterovç»“åˆå°±æ˜¯ Nadamæ–¹æ³•ã€‚

[éšæœºæ¢¯åº¦ä¸‹é™ã€ç‰›é¡¿æ³•ã€åŠ¨é‡æ³•ã€Nesterovã€AdaGradã€RMSpropã€Adamã€Nadam](https://www.bilibili.com/video/BV1r64y1s7fU)
