# è€¦åˆPINNã€æ­£åé—®é¢˜ã€3Dé—®é¢˜çš„å­¦ä¹ ç ”ç©¶



## è®°å½•æ—¶é—´ï¼š2022-06-29 

### ä»€ä¹ˆæ˜¯PINNï¼Ÿ

Raissiç­‰äººåœ¨2018å¹´ 

 [Physics-informed neural networks: A deep learning 
framework for solving forward and inverse problems involving 
nonlinear partial differential equations](https://github.com/maziarraissi/PINNs)

ä¸­æå‡ºäº†PINNï¼Œé€šè¿‡åœ¨æŸè€—å‡½æ•°ä¸­ç»“åˆç‰©ç†åœºï¼ˆå³åå¾®åˆ†æ–¹ç¨‹ï¼‰å’Œè¾¹ç•Œæ¡ä»¶æ¥è§£å†³åå¾®åˆ†æ–¹ç¨‹ã€‚æŸå¤±æ˜¯åå¾®åˆ†æ–¹ç¨‹çš„å‡æ–¹è¯¯å·®å’Œåœ¨åˆ†å¸ƒåœ¨åŸŸä¸­çš„â€œæ­é…ç‚¹â€ä¸Šæµ‹é‡çš„è¾¹ç•Œæ®‹å·®ã€‚



ä»–ä»¬åœ¨æ–‡ç« ä¸­åˆ—å‡ºäº†æ•°ä¸ªç»å…¸pdeç®—ä¾‹æ¥éªŒè¯PINNçš„æ­£ç¡®æ€§ã€‚åœ¨ä»–ä»¬çš„githubä¸»é¡µä¸Šæä¾›äº†æºç ï¼Œä½¿ç”¨äº†Tensorflow1.xçš„æ¡†æ¶ã€‚

æˆªæ­¢åˆ°2022-06-29ï¼ŒTensorFlowæ›´æ–°åˆ°2.9ï¼ŒTensorFlow1.xä»£ç é£æ ¼è·ŸTensorFlow2.xç›¸å·®ç”šè¿œï¼Œè€Œä¸”Raissiç­‰äººæä¾›çš„ä»£ç ä¹Ÿæœ‰è®¸å¤šå€¼å¾—ä¼˜åŒ–çš„åœ°æ–¹ã€‚

æˆ‘åœ¨githubä¸Šæ‰¾åˆ°ä¸€ä¸ªäº†PINNåœ¨TensorFlow2.xä¸Šçš„ä»£ç æ¡†æ¶ [omniscientoctopus/Physics-Informed-Neural-Networks: Investigating PINNs (github.com)](https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks),



### æœ¬æ–‡ç ”ç©¶ä»€ä¹ˆï¼Ÿ

åˆ©ç”¨æš‘å‡ç ”ç©¶ä¸€ä¸‹å¦‚ä¸‹é—®é¢˜ï¼š 

- PINNæ¨¡å‹åœ¨è€¦åˆé—®é¢˜ä¸Šçš„åº”ç”¨ï¼ˆè¿™æ–¹é¢çš„ç ”ç©¶ä¼¼ä¹è¿˜ä¸å¤šï¼‰ï¼Œ
- PINNæ­£è´Ÿé—®é¢˜æ±‚è§£ï¼ˆçœ‹äº†ä¸€äº›æ–‡ç« è¯´PINNçš„ä¼˜åŠ¿å…¶å®åœ¨äº **é«˜ç»´é—®é¢˜ & åé—®é¢˜æ±‚è§£**
- ä»£ç å®ç°è‹¥å¹²ç®—ä¾‹ï¼ˆä¹Ÿè®¸å¯èƒ½maybe Only oneğŸ˜ï¼‰ï¼Œæœ€å¥½èƒ½æŠŠæ¨¡å‹åº”ç”¨åˆ°3ç»´ã€‚



### PINNæ¨¡å‹æ­å»º

> å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨ï¼Œå…ˆæŒæ¡TensorFlowï¼Œå¹¶ä½¿ç”¨TensorFlowè‡ªå®šä¹‰PINNæ¨¡å‹æ˜¯å¿…è¦çš„ã€‚

å…ˆè®¾æƒ³ä¸€ä¸ªä»£ç æ¡†æ¶ã€‚

PINNçš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æœå…ˆä¸è€ƒè™‘losså‡½æ•°ï¼Œé‚£ä¹ˆPINNå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å…¨è¿æ¥åºåˆ—æ¨¡å‹ï¼Œå¯ä»¥ç”¨ **tf.keras.Sequential()**æ„å»ºã€‚

å‡è®¾Layer = [inputs,n1,n2,...nk,n_outputs]ï¼Œä»å·¦è‡³å³ä»£è¡¨æ¯å±‚ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚

```python
def createModel(layer):
    if len(layer) < 2:
        print("å±‚æ•°å°äº2ï¼, æ— æ³•æ„å»ºç¥ç»ç½‘ç»œ")
        return 
    model = keras.Sequential(name = "PINN")
    model.add(keras.Input(shape=(layer[0],)))
    for i in range(1,len(layer)-1):
        model.add(layers.Dense(layer[i], activation="relu", name="layer{}".format(i)))
    model.add(layers.Dense(layer[-1], name="outputs"))
    
    #  model.compile( loss= , metrics = [], optimizer= )
    #	...
    #
    #
    
    return model
```



#### <font color='purple'>åç»­è¦ä¸º model æ·»åŠ  lossï¼ˆPINNä¸»è¦éƒ¨åˆ†ï¼‰ã€metricï¼ˆå¯ä»¥ä¸è¦ï¼‰ã€optimizerï¼ˆä¼˜åŒ–å™¨ï¼Œå¿…è¦ï¼‰</font>

> ä»¥ä¸Šå†…å®¹äº 2022 -  06 - 29  markdownã€‚

---

## è®°å½•æ—¶é—´ï¼š2022-06-30

### <font color='blue'>1.PINNæ¨¡å‹æ­å»ºï¼ˆç»­ï¼‰</font>

TensorFlowä¸ºæˆ‘ä»¬æä¾›äº†å¤šç§å¤šæ ·çš„é«˜é˜¶APIå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿæ­å»ºæ¨¡å‹ã€‚**ä½†æ˜¯ï¼Œå¿«é€Ÿæ–¹ä¾¿çš„ä»£ä»·å°±æ˜¯çµæ´»æ€§é™ä½ã€‚**ç‰¹åˆ«æ˜¯PINNè·Ÿä¸€èˆ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ­¥éª¤ä¸åŒï¼Œä¸»è¦æ¥æºäºloss functionéœ€è¦å¯¹é¢„æµ‹å€¼è¿›è¡Œæ±‚å¯¼è¿ç®—ã€‚

ä»Šå¤©çš„ä»£ç ç¼–å†™é­é‡äº†ä¸€äº›å›°éš¾ï¼Œæˆ‘æ˜¨å¤©çš„æ„æ€å®é™…ä¸Šä¸å¯è¡Œï¼š

1. ä½¿ç”¨ **tf.keras.Sequential()**æˆ–è€…**tf.keras.Model()å‡½æ•°å¼API** æ­å»ºæ¨¡å‹ Modelã€‚(è¿™ä¸€æ­¥æ²¡é—®é¢˜ï¼‰

2. ç¼–å†™è‡ªå®šä¹‰losså‡½æ•°ï¼Œç„¶åå°† è‡ªå®šä¹‰çš„loss & ä¼˜åŒ–å™¨ & metrics ä¼ å…¥è‡ªå¸¦çš„æ–¹æ³• **Model.compile()**

3. è°ƒç”¨è‡ªå¸¦çš„æ–¹æ³• **Model.fit()** ï¼Œä¼ å…¥è®­ç»ƒæ•°æ®è®­ç»ƒå³å¯ã€‚



æ­¥éª¤1 å’Œ ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚è€ŒPINNæ¨¡å‹ç»“æ„ç®€å•ï¼Œä½¿ç”¨æ­¥éª¤1æ­å»ºæ¨¡å‹å®Œå…¨æ²¡æœ‰é—®é¢˜ã€‚

é—®é¢˜å‡ºåœ¨ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰ã€‚

è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å…¶ä¸­ä¸¤ä¸ªå…³é”®çš„æ–¹æ³• **Model.compile() & Model.fit()** æ–¹æ³•ç©¶ç«Ÿåšäº†ä»€ä¹ˆï¼Ÿ

å¦‚æœæ˜ç™½äº†Model.fit()çš„å·¥ä½œæ­¥éª¤ï¼Œå°±ä¼šçŸ¥é“è¿™ç§è‡ªå¸¦çš„è®­ç»ƒæ–¹å¼å±€é™æ€§ï¼Œä»¥åŠä¸ºä»€ä¹ˆåœ¨è®­ç»ƒPINNæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ã€‚



**Model.fit()å¤§è‡´å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š**

```python
def fit(self,X_train,Y_train,epoches,batch_size,**kwargs):
    #
    #		ä¸»è¦çš„è®­ç»ƒéƒ¨åˆ†ä»£ç 
    #
    for epoch in epoches: ## ä½¿ç”¨ X_train,Y_train è¿›è¡Œ epoch æ¬¡è®­ç»ƒ
        for x_batch,y_batch in dataset(X_train,Y_train,batch_size): ## dataset(X_train,Y_train,batch_size) æ•°æ®åˆ†ç»„
            train_step((x_batch,y_batch))  ## train_step(self,data) æ˜¯ ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥é‡è½½ï¼Œä»è€Œæ”¹å˜fit()

def train_step(self,data):
    
    # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.
    x, y = data

    with tf.GradientTape(persistent=True) as tape:
        y_pred = self(x, training=True)  # Forward pass
        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
        #è¿™é‡Œç”¨åˆ°çš„ self.compiled_loss() å°±æ˜¯ Model.compile(loss) ä¼ å…¥çš„loss
        #å³ Model.compile() ä½œç”¨æ˜¯è®© self.compiled_loss = loss
                       
	# Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)

    del tape

    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    
    #æŒ‡æ ‡è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    #....
```



å¯ä»¥çœ‹åˆ° **Model.fit()**åªæ˜¯ä¸ºæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªä¸€èˆ¬æ™®é€šçš„è®­ç»ƒæ­¥éª¤å‡½æ•°ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šã€‚

**Model.fit()** ä¼šåœ¨ train_step() ä¸­è°ƒç”¨ Model.compile() ä¼ å…¥çš„lossã€ä¼˜åŒ–å™¨ã€æŒ‡æ ‡ã€‚

```python
loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
```

ä¸Šè¿°ä»£ç ä¸­compiled_loss()é™å®šäº†å‚æ•° y , y_predã€‚yæ˜¯æ ‡ç­¾å€¼ï¼Œy_predæ˜¯é€šè¿‡æ¨¡å‹é¢„æµ‹çš„å€¼ã€‚



è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®˜æ–¹æ–‡æ¡£ã€ç½‘ä¸Šçš„å¸–å­è¯´ï¼š

**å¦‚æœæƒ³è‡ªå®šä¹‰lossï¼Œé‚£ä¹ˆä½ è‡ªå®šä¹‰çš„losså‡½æ•°ä¸€å®šè¦æœ‰ä¸¤ä¸ªè¾“å…¥å‚æ•°ï¼ˆyï¼Œy_pred)ã€‚**

**å› ä¸ºåœ¨ fit() â€”â€”ã€‹train_step()ä¸­ï¼Œè°ƒç”¨äº†self.compiled_loss()ï¼Œå¹¶è§„å®šäº†å®ƒçš„å‚å…¥å‚æ•°ä¸ºï¼ˆy,y_pred)**ã€‚



å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡è½½ train_step() ä½¿è‡ªå®šä¹‰losså‡½æ•°è‡ªç”±åº¦æ›´é«˜ï¼Œå¹¶ä»èƒ½ä½¿ç”¨fit()ã€‚

ç”šè‡³é‡è½½fit()ï¼Œä½†æ—¢ç„¶éƒ½é‡è½½fit()äº†ï¼Œå®ƒç­‰ä»·äºä»0ç¼–å†™è®­ç»ƒè¿‡ç¨‹ã€‚â€”â€”ä¸è¿‡ï¼Œè¿™æ­£æ˜¯PINNè®­ç»ƒéœ€è¦çš„ã€‚

**å› ä¸ºæˆ‘ä»¬æœ‰ä¸åŒæ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆå¸¦æ ‡ç­¾å’Œä¸å¸¦æ ‡ç­¾ï¼‰ã€å¤æ‚çš„losså‡½æ•°ï¼ˆä¸ä»…ä»…éœ€è¦yï¼Œy_predï¼Œè¿˜éœ€è¦y_predå¯¹xï¼Œtçš„å¯¼æ•°ï¼‰ï¼Œæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»ç„¶ä½¿ç”¨fitï¼ˆï¼‰ä»£ç æ¡†æ¶ï¼Œéœ€è¦å¤§åˆ€é˜”æ–§åœ°ä¿®æ”¹ã€‚è¿˜ä¸å¦‚è‡ªå·±å†™è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å»ä½¿ç”¨æ‰€è°“çš„é«˜é˜¶API~~**



### <font color='blue'>2.å­ç±»åŒ–Sequential() / Model() , å®šä¹‰ MyPinnï¼Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹</font>

ä¹‹å‰è®¨è®ºè¿‡**æ­¥éª¤1ï¼šä½¿ç”¨Sequentialæ­å»ºæ¨¡å‹** æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œæˆ‘ä»¬éœ€è¦åšçš„ä»…ä»…æ˜¯é‡æ–°å®šä¹‰**è®­ç»ƒè¿‡ç¨‹**ã€‚

é‚£ä¹ˆï¼Œå­ç±»åŒ–**class MyPinn(tf.keras.Sequential())**ï¼ŒMyPinnä¿ç•™Sequential()å¥½ç”¨çš„åŠŸèƒ½ï¼Œå†é™„åŠ ä¸Šæˆ‘ä»¬è‡ªå®šä¹‰çš„**è®­ç»ƒè¿‡ç¨‹**

```python
class MyPinn(keras.Sequential): ## ä»¥Burgers_Equationä¸ºä¾‹
    def __init__(self,name = None):
        
        super(MyPinn, self).__init__(name=name)
        self.nu = tf.constant(0.01/np.pi)
    
    @tf.function
    def test_gradient(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape() as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)
        u_x = tape.gradient(u,x)
        tf.print(u_x)
    
    @tf.function
    def loss_U(self,X_u_train,u_train):
        u_pred = self(X_u_train)
        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))
        return loss_u
    
    
    @tf.function
    def loss_PDE(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape(persistent=True) as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)  
            u_x = tape.gradient(u,x)         
            
        u_t = tape.gradient(u, t)     
        u_xx = tape.gradient(u_x, x)
        
        del tape
        
        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx

        loss_f = tf.reduce_mean(tf.square(f))

        return loss_f
    
    
    def loss_Total(self,X_u_train,u_train,X_f_train):
        loss_u = self.loss_U(X_u_train,u_train)
        loss_f = self.loss_PDE(X_f_train)
        
        loss_total = loss_u + loss_f
        
        return loss_total
    
    @tf.function
    def train_step(self,X_u_train,u_train,X_f_train):
        with tf.GradientTape(persistent=True) as tape:
            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)
                   
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss_total, trainable_vars)
        
        del tape
        
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        return loss_total
    
    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):
        for epoch in tf.range(1,epochs+1):
            loss_total = self.train_step(X_u_train,u_train,X_f_train)
            if epoch % 10 == 0:                
                print(
                    "Training loss (for per 10 epoches) at epoch %d: %.4f"
                    % (epoch, float(loss_total))
                )
```



<font color='purple'> **åœ¨å½“å‰æ–‡ä»¶å¤¹ myPINN.py è¿›è¡Œäº†train_modelæµ‹è¯•ï¼ŒæˆåŠŸè¿è¡Œ**</font>

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 6-30 markdown

---

## è®°å½•æ—¶é—´ï¼š2022-07-01

### <font color='blue'>ä¼˜åŒ–å™¨</font>

tf.keras.optimizersä¸ºæˆ‘ä»¬æä¾›äº†è®¸å¤šç°æˆçš„ä¼˜åŒ–å™¨ï¼Œæ¯”å¦‚SGDï¼ˆæœ€é€Ÿä¸‹é™ï¼‰ã€Adamã€RMSpropç­‰ç­‰ã€‚

å‡è®¾ï¼Œç°æœ‰æ¨¡å‹å¯¹è±¡ MyPinnã€‚

å¯ä»¥é€šè¿‡ tf.keras.optimizers.Optimizer() åˆ›å»ºä¼˜åŒ–å™¨å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œ MyPinn.optimizer = tf.keras.optimizers.SGD()

tf.keras.optimizers.Optimizer()ä¸»è¦æä¾›äº†ä¸¤ç§Methodsï¼Œä¸ºæˆ‘ä»¬çš„å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚

1. **apply_gradients(**
       **grads_and_vars, name=None, experimental_aggregate_gradients=True**
   **)**

â€‹	ä¹‹å‰å®šä¹‰çš„MyPinn.train_step()ä¸­å°±ä½¿ç”¨äº†è¿™ç§Methodã€‚

â€‹	æˆ‘ä»¬å…ˆè®¡ç®—å‡ºgradsï¼Œå†ä½¿ç”¨apply_gradient()ï¼Œè¿›è¡Œå‚æ•°ä¼˜åŒ–ã€‚

2. **minimize(**
       **loss, var_list, grad_loss=None, name=None, tape=None**
   **)**

   minimize()æ–¹æ³•å…ˆä½¿ç”¨tf.GradientTape()è®¡ç®—å‡ºlossï¼Œå†è°ƒç”¨apply_gradients()ã€‚ç›¸å½“äºæŠŠcompute gradientså’Œapply gradients å°è£…åœ¨ä¸€èµ·ã€‚

å¯ä»¥å‘ç°ï¼Œapply_gradients()å°±æ˜¯minimize()ä¸­çš„ç¬¬äºŒæ­¥ã€‚

**ä¸ºäº†ç²¾å‡†åœ°æ§åˆ¶ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ ä¸Šä¸€äº›åˆ«çš„æ“ä½œï¼Œæˆ‘ä»¬ä½¿ç”¨ ç¬¬1ç§æ–¹æ³• å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚**



### <font color='blue'>  Adam & L-BFGS </font>

Adamä¼˜åŒ–å™¨åœ¨deep neural networkä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¹‹å‰ä¹Ÿè¯´è¿‡ï¼Œtf.keras.optimizersé‡Œå†…ç½®äº†Adamä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨å°±å¥½ã€‚

åœ¨PINNåŸä½œè€…çš„ä»£ç ä¸­(tensorflow1.x)ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸¤ç§ä¼˜åŒ–å™¨ ï¼š Adam & L-BFGSã€‚

åœ¨ training model è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬å…ˆä½¿ç”¨ Adam è¿›è¡Œä¼˜åŒ–ï¼Œåä½¿ç”¨ L- BFGS è¿›è¡Œä¼˜åŒ–ã€‚



L-BFGS æ˜¯ ç§©2çš„æ‹Ÿç‰›é¡¿æ–¹æ³•(è¿™å­¦æœŸ â€œæœ€ä¼˜åŒ–æ–¹æ³•â€ è¯¾ä¸Šåˆšå¥½å­¦è¿‡)ï¼Œå®ƒæ˜¯åŸºæœ¬ç‰›é¡¿æ–¹æ³•çš„ä¸€ç§å˜å½¢ã€‚ç‰›é¡¿æ–¹æ³•åœ¨æå€¼ç‚¹é™„è¿‘æ—¶ï¼Œæ”¶æ•›é€Ÿåº¦å¿«ï¼Œè€Œæ‹Ÿç‰›é¡¿æ–¹æ³•åœ¨ä¿æŒè¿™ä¸ªä¼˜ç§€æ€§è´¨çš„åŸºç¡€ä¸Šï¼Œæ”¹è¿›äº†ç‰›é¡¿æ–¹æ³•è¿‡ç¨‹çš„ä¸€äº›ç¼ºç‚¹ï¼Œæ¯”å¦‚è®¡ç®—äºŒé˜¶å¯¼ã€çŸ©é˜µæ±‚é€†å’ŒGä¸æ­£å®šç­‰é—®é¢˜ã€‚



ç„¶è€Œåœ¨TensorFlow1.xä¸­ å¹¶æ²¡æœ‰å†…ç½®çš„ L-BFGSï¼Œä½œè€…å®é™…æ˜¯ä½¿ç”¨tensoflow1.x æä¾›çš„ä¸€ä¸ªæ¥å£ï¼Œä½¿ç”¨ Scipy åº“ä¸­çš„ L-BFGSã€‚

Scipyä¸­è°ƒç”¨L-BFGSçš„æ ¼å¼æ˜¯ï¼š

```python
scipy.optimize.minimize(fun, 
				x0, args=(),
                method='L-BFGS-B', 
                jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)
```

 å…¶ä¸­funæ˜¯ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œè¿”å›ç›®æ ‡å‡½æ•°å€¼ã€‚

> ```
> fun(x, *args) -> float
> ```

where `x` **is a 1-D array with shape (n,)** and `args` is a tuple of the fixed parameters needed to completely specify the function.

> å½“ jac = True æ—¶ï¼Œ fun()  retrun fval , gradients



è¿™æ—¶ï¼Œå†çœ‹ä¸€ä¸‹ä½œè€…è°ƒç”¨L-BFGSçš„ä»£ç ï¼Œå°±çŸ¥é“æ˜¯ä»€ä¹ˆæ„æ€äº†ã€‚ã€‚

```python
self.optimizer =tf.contrib.opt.ScipyOptimizerInterface (self.loss, 
                                                        method = 'L-BFGS-B', 
                                                        options = {'maxiter': 3000,
                                                                   'maxfun': 3000,
                                                                   'maxcor': 50,
                                                                   'maxls': 50,
                                                                   'ftol' : 1.0 * np.finfo(float).eps})
```



è¿™æ—¶å¦‚æœæˆ‘ä»¬å†™ self.optimizer.minimize() å®é™…ä¸Šå°±ä¼šè°ƒç”¨ scipy.optimize.minimize( args ) ï¼Œargs=ä¸Šè¿°ä»£ç ä¸­ä¼ å…¥çš„å‚æ•°ï¼Œself.loss ç›¸å½“äº funã€‚



é—æ†¾çš„æ˜¯ï¼Œåœ¨TensorFlow2.xä¸­ï¼Œè¯¥æ¥å£å·²ç»åˆ é™¤ã€‚

å½“ç„¶æˆ‘ä»¬ä»èƒ½æƒ³åŠæ³•ä½¿ç”¨Scipyä¸­çš„L-BFGSï¼Œ

æ— éå°±æ˜¯æŒ‰ç…§scipy.optimize.miminze()çš„è°ƒç”¨æ ¼å¼ï¼Œåœ¨MyPinnå†…éƒ¨å®šä¹‰ä¸€ä¸ªloss_fun(x) ï¼Œx is 1-D array with shape = (n,)ï¼Œ

ä½œä¸ºscipy.optimize.miminze(fun,...)ä¸­çš„funï¼Œä½†è¿™æ„å‘³ç€éœ€è¦æŠŠMyPinnçš„weightså’Œbias "æ‰å¹³åŒ–" æ”¾åœ¨ä¸€ä¸ª1ç»´æ•°ç»„ä¸­ï¼Œåœ¨ä¼˜åŒ–å®Œæ¯•åï¼Œè¿˜è¦æŠŠç»“æœå†å˜æˆåŸæ¥çš„å½¢çŠ¶ï¼Œæ”¾å›MyPinné‡Œã€‚ã€‚ã€‚æ„Ÿè§‰æœ‰ç‚¹éº»çƒ¦ã€‚



åˆä½†æ˜¯ï¼Œè™½ç„¶æ¥å£æ²¡äº†ï¼Œä½†TensorFlow2.0ä¸­ tfp åº“ä¸­æœ‰å®ç° L-BFGS ç®—æ³•ã€‚ğŸ˜



**ä¸‹é¢é“¾æ¥ä¸­ï¼Œæé—®è€…(åŒæ—¶ä½œä¸ºå›ç­”è€…ï¼Œä»–è‡ªé—®è‡ªç­”)è®¨è®ºäº†åœ¨TensorFlow2.xä¸­ä½¿ç”¨ Scipyçš„L-BFGS å’Œ è‡ªå¸¦çš„L-BFGS è®¡ç®—å·®åˆ«ã€‚**

[python - Use Scipy Optimizer with Tensorflow 2.0 for Neural Network training - Stack Overflow](https://stackoverflow.com/questions/59029854/use-scipy-optimizer-with-tensorflow-2-0-for-neural-network-training)

<img src="./L-BFGS in scipy and tfp.png" />



**å¯ä»¥å‘ç°ä½¿ç”¨TensorFlow2.0 tfpä¸­çš„L-BFGSè®¡ç®—é€Ÿåº¦æ›´å¿«**

**ä¸è¿‡tfpä¸­çš„L-BFGSè®¡ç®—ç»“æœç•¥é€ŠäºScipyä¸­çš„L-BFGSï¼Œå¯èƒ½æ˜¯TensorFlowé»˜è®¤float32ï¼Œè€ŒScipyæ˜¯float64ï¼Œä»¥åŠScipyä¸­L-BFGSç®—æ³•çš„å®ç°æ¯”tfpçš„æ›´å¥½ã€‚**



[Optimize TensorFlow & Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

æ³¨æ„å¦‚æœæƒ³ä½¿ç”¨tfpçš„L-BFGSä¹Ÿæ˜¯è¦æ±‚è¾“å…¥å˜é‡æ˜¯1-Dçš„ã€‚è€Œæˆ‘ä»¬çš„PiNNæ¨¡å‹ä¸­çš„weightså’Œbiaséƒ½æ˜¯ä»¥å¤šç»´çš„å½¢å¼ä¿å­˜ï¼Œæ‰€ä»¥è¦å…ˆå°†å®ƒä»¬è¿›è¡Œâ€œæ‰å¹³åŒ–â€ï¼Œå†ä¼ å…¥L-BFGSå‡½æ•°ä¸­ã€‚ï¼ˆæ­£åœ¨å­¦ä¹ ã€‚ã€‚ã€‚ï¼‰

ä¸Šé¢çš„é“¾æ¥è®¨è®ºäº†å¦‚ä½•å°†modelä¸­çš„å˜é‡â€œæ‰å¹³åŒ–â€ã€‚

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-1 markdown
