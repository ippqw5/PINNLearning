{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc0a382",
   "metadata": {},
   "source": [
    "# 6-29 记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ba80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(layer):\n",
    "    if len(layer) < 2:\n",
    "        print(\"层数小于2！, 无法构建神经网络\")\n",
    "        return \n",
    "    model = keras.Sequential(name = \"PINN\")\n",
    "    model.add(keras.Input(shape=(layer[0],)))\n",
    "    for i in range(1,len(layer)-1):\n",
    "        model.add(layers.Dense(layer[i], activation=\"relu\", name=\"layer{}\".format(i)))\n",
    "    model.add(layers.Dense(layer[-1], name=\"outputs\"))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  测试\n",
    "layer = [2,4,4,3]\n",
    "model = createModel(layer)\n",
    "inputs = tf.random.normal([1,2])\n",
    "print(model(inputs))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ed250",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def myloss(y_predict,y_true):\n",
    "    return tf.reduce_sum(tf.square(y_predict-y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(),  # Optimizer\n",
    "    loss = myloss\n",
    "    )\n",
    "model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([10,2])\n",
    "y = tf.random.normal([10,3])\n",
    "model.fit(x,y,batch_size=2,epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307f646",
   "metadata": {},
   "source": [
    "## 后续要为model 添加 loss、metric、optimizer\n",
    "\n",
    "\n",
    "model.compile(\n",
    "\n",
    "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "    \n",
    "    # Loss function to minimize\n",
    "    \n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # List of metrics to monitor\n",
    "    \n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    \n",
    ")\n",
    "\n",
    "> 以上内容截止至 6-29 markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca7eda",
   "metadata": {},
   "source": [
    "# 6-30 记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9558e",
   "metadata": {},
   "source": [
    "train_step()是fit()将会调用的核心函数，\n",
    "\n",
    "而train_step()将会使用 我们之前调用compile(optimizer,loss,metrics)时传入的优化器、损失函数、指标。\n",
    "\n",
    "实际上compile()做的事情就是将这些部件加入模型，方便train_step()使用。\n",
    "\n",
    "故，当我们想要自定义loss、更精准地控制训练过程，同时又想要保留keras.Model或keras.Sequential的方便方法。\n",
    "\n",
    "我们可以自定义一个子类，继承keras.Model或keras.Sequential。\n",
    "\n",
    "由于PINN的复杂性，需要对预测值求导再组成loss函数，仅仅修改train_step()是不够的，这意味着需要重载fit()，实际上就是抛弃已有的fit()框架，自己定义训练循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPinn(keras.Sequential): ## 正在编写\n",
    "    def __init__(self,name = None):\n",
    "        \n",
    "        super(MyPinn, self).__init__(name=name)\n",
    "        self.nu = tf.constant(0.01/np.pi)\n",
    "    \n",
    "    @tf.function\n",
    "    def test_gradient(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)\n",
    "        u_x = tape.gradient(u,x)\n",
    "        tf.print(u_x)\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_U(self,X_u_train,u_train):\n",
    "        u_pred = self(X_u_train)\n",
    "        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))\n",
    "        return loss_u\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def loss_PDE(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)  \n",
    "            u_x = tape.gradient(u,x)         \n",
    "            \n",
    "        u_t = tape.gradient(u, t)     \n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx\n",
    "\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "\n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss_Total(self,X_u_train,u_train,X_f_train):\n",
    "        loss_u = self.loss_U(X_u_train,u_train)\n",
    "        loss_f = self.loss_PDE(X_f_train)\n",
    "        \n",
    "        loss_total = loss_u + loss_f\n",
    "        \n",
    "        return loss_total\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self,X_u_train,u_train,X_f_train):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)\n",
    "                   \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss_total, trainable_vars)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        return loss_total\n",
    "    \n",
    "    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):\n",
    "        for epoch in tf.range(1,epochs+1):\n",
    "            loss_total = self.train_step(X_u_train,u_train,X_f_train)\n",
    "            if epoch % 10 == 0:                \n",
    "                print(\n",
    "                    \"Training loss (for per 10 epoches) at epoch %d: %.4f\"\n",
    "                    % (epoch, float(loss_total))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(layer,Model):\n",
    "    if len(layer) < 2:\n",
    "        print(\"层数小于2！, 无法构建神经网络\")\n",
    "        return \n",
    "    model = Model(name = \"PINN\")\n",
    "    model.add(keras.Input(shape=(layer[0],)))\n",
    "    for i in range(1,len(layer)-1):\n",
    "        model.add(layers.Dense(layer[i], activation=\"relu\", name=\"layer{}\".format(i)))\n",
    "    model.add(layers.Dense(layer[-1], name=\"outputs\"))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train = tf.random.normal([1000,2])\n",
    "u_train = tf.random.normal([1000,1])\n",
    "X_f_train = tf.random.normal([1000,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [2,20,20,20,20,1]\n",
    "m1= createModel(layer,MyPinn)\n",
    "m1.compile(keras.optimizers.SGD(learning_rate=0.1))\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.train_model(X_u_train,u_train,X_f_train,epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试PINN (请无视)\n",
    "class MyPINN(keras.Sequential):\n",
    "    \n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x)\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss_u = tf.reduce_mean(tf.square(y-y_pred)) \n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            # loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            loss = loss_u\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736790d4",
   "metadata": {},
   "source": [
    "> 以上内容截止至 6-30 markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2e39f",
   "metadata": {},
   "source": [
    "# 7-1 记录\n",
    "\n",
    "### Adam优化器 & tfp中L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(layer,Model):\n",
    "    if len(layer) < 2:\n",
    "        print(\"层数小于2！, 无法构建神经网络\")\n",
    "        return \n",
    "    model = Model(name = \"PINN\")\n",
    "    model.add(keras.Input(shape=(layer[0],)))\n",
    "    for i in range(1,len(layer)-1):\n",
    "        model.add(layers.Dense(layer[i], activation=\"relu\", name=\"layer{}\".format(i)))\n",
    "    model.add(layers.Dense(layer[-1], name=\"outputs\"))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPinn(keras.Sequential): ## 正在编写\n",
    "    def __init__(self,name = None):\n",
    "        \n",
    "        super(MyPinn, self).__init__(name=name)\n",
    "        self.nu = tf.constant(0.01/np.pi)\n",
    "    \n",
    "    @tf.function\n",
    "    def test_gradient(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)\n",
    "        u_x = tape.gradient(u,x)\n",
    "        tf.print(u_x)\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_U(self,X_u_train,u_train):\n",
    "        u_pred = self(X_u_train)\n",
    "        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))\n",
    "        return loss_u\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def loss_PDE(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)  \n",
    "            u_x = tape.gradient(u,x)         \n",
    "            \n",
    "        u_t = tape.gradient(u, t)     \n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx\n",
    "\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "\n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss_Total(self,X_u_train,u_train,X_f_train):\n",
    "        loss_u = self.loss_U(X_u_train,u_train)\n",
    "        loss_f = self.loss_PDE(X_f_train)\n",
    "        \n",
    "        loss_total = loss_u + loss_f\n",
    "        \n",
    "        return loss_total\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self,X_u_train,u_train,X_f_train):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)\n",
    "                   \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss_total, trainable_vars)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        return loss_total\n",
    "    \n",
    "    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):\n",
    "        for epoch in tf.range(1,epochs+1):\n",
    "            loss_total = self.train_step(X_u_train,u_train,X_f_train)\n",
    "            if epoch % 10 == 0:                \n",
    "                print(\n",
    "                    \"Training loss (for per 10 epoches) at epoch %d: %.4f\"\n",
    "                    % (epoch, float(loss_total))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train = tf.random.normal([1000,2])\n",
    "u_train = tf.random.normal([1000,1])\n",
    "X_f_train = tf.random.normal([1000,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6675b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [2,20,20,1]\n",
    "m1= createModel(layer,MyPinn)\n",
    "m1.compile(keras.optimizers.Adam())\n",
    "m1.train_model(X_u_train,u_train,X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_factory(model, loss, X_u_train,u_train,X_f_train):\n",
    "    \"\"\"A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    Args:\n",
    "        model [in]: an instance of `tf.keras.Model` or its subclasses.\n",
    "        loss [in]: a loss function in model\n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(model_parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "    # prepare required information first\n",
    "    count = 0\n",
    "    idx = [] # stitch indices\n",
    "    part = [] # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = np.product(shape)\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "\n",
    "    part = tf.constant(part)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # now create a function that will be returned by this factory\n",
    "    @tf.function\n",
    "    def f(params_1d):\n",
    "        \"\"\"A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "        This function is created by function_factory.\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "\n",
    "        # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            # update the parameters in the model\n",
    "            assign_new_model_parameters(params_1d)\n",
    "            # calculate the loss\n",
    "            loss_value = loss(X_u_train,u_train,X_f_train)\n",
    "\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "        # print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "        tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        # store loss value so we can retrieve later\n",
    "        tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "        return loss_value, grads\n",
    "\n",
    "    # store these information as members so we can use them outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_model_parameters = assign_new_model_parameters\n",
    "    f.history = []\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a721e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = function_factory(m1, m1.loss_Total, X_u_train,u_train,X_f_train)\n",
    "init_params = tf.dynamic_stitch(func.idx, m1.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tfp.optimizer.lbfgs_minimize(\n",
    "    value_and_gradients_function=func, initial_position=init_params, max_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aaf03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.loss_Total(X_u_train,u_train,X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33256101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
