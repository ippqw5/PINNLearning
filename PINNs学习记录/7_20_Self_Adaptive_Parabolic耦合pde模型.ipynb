{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"./Data/公式.png\" style=\"zoom:50%;\" />"
      ],
      "metadata": {
        "id": "spInIFVwzLva"
      },
      "id": "spInIFVwzLva"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "01l-S8EOdodM"
      },
      "id": "01l-S8EOdodM"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyDOE"
      ],
      "metadata": {
        "id": "cdRFQVRmo2Yw"
      },
      "id": "cdRFQVRmo2Yw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ft0nI8-EznSF"
      },
      "id": "ft0nI8-EznSF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5e2de7",
      "metadata": {
        "id": "4d5e2de7"
      },
      "outputs": [],
      "source": [
        "from sympy import*\n",
        "import numpy as np\n",
        "from pyDOE import lhs         #Latin Hypercube Sampling\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D   \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "yAt6asqSbzHy"
      },
      "id": "yAt6asqSbzHy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298178bf",
      "metadata": {
        "id": "298178bf"
      },
      "outputs": [],
      "source": [
        "\n",
        "x = symbols('x')\n",
        "y = symbols('y')\n",
        "t = symbols('t')\n",
        "\n",
        "a = v1 = v2 = k =1 #a=4 ,v1=5,v2=10,k=1/4\n",
        "c1 = 1 + v1/k ;c2 = -v1/v2;c3 = c2 - c1\n",
        "g1 = g2 =0\n",
        "\n",
        "u1 = a*x*(1-x)*(1-y)*exp(-t)\n",
        "u2 = a*x*(1-x)*(c1 + c2*y + c3*y**2)*exp(-t)\n",
        "\n",
        "f1 = diff(u1,t) - v1*( diff(u1,x,2) + diff(u1,y,2))\n",
        "f2 = diff(u2,t) - v2*( diff(u2,x,2) + diff(u2,y,2))\n",
        "\n",
        "u1 = lambdify((x,y,t),u1)\n",
        "u2 = lambdify((x,y,t),u2)\n",
        "\n",
        "f1 = lambdify((x,y,t),f1,'tensorflow')\n",
        "f2 = lambdify((x,y,t),f2,'tensorflow')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TrainingData & TestData"
      ],
      "metadata": {
        "id": "6ihipZqLb1m3"
      },
      "id": "6ihipZqLb1m3"
    },
    {
      "cell_type": "code",
      "source": [
        "def trainingdata(N_u,N_f,N_i,omga,u,interface):    \n",
        "    '''\n",
        "      x1 = x,  x2 = y\n",
        "      interface = 0, 则x2_range[0] 作为 interface ； interfacer = 1 则 x2_range[1]作为interface\n",
        "    \n",
        "      Return: X_u_train (N_u x 3),u_train (N_u x 1), X_f_train (N_f x 3), X_i_train (N_i x 3)\n",
        "    '''\n",
        "    x1_range = omga[0]\n",
        "    x2_range = omga[1]\n",
        "    t_range = [0.,1.]\n",
        "    \n",
        "    # Initial Condition t=0 , x1_range[0]<= x1 <= x_range[1] , x2_range[0] <= x2 <= x2_range[1]\n",
        "    initial_X = np.ones((N_u,3))\n",
        "    initial_X[:,0] = np.random.rand(N_u) * (x1_range[1] - x1_range[0]) + x1_range[0]\n",
        "    initial_X[:,1] = np.random.rand(N_u) * (x2_range[1] - x2_range[0]) + x2_range[0]\n",
        "    initial_X[:,2] = initial_X[:,2] * t_range[0]\n",
        "    #initial_u = np.ones((N_u,1))\n",
        "    initial_u = u(initial_X[:,0], initial_X[:,1], initial_X[:,2])\n",
        "    initial_u = initial_u.reshape(-1,1)\n",
        "\n",
        "\n",
        "    # BC_left: x1=x1_range[0], 0<=t<=1, x2_range[0] <= x2 <= x2_range[1]\n",
        "    leftedge_X = np.ones(shape=(N_u,3))\n",
        "    leftedge_X[:,0] = leftedge_X[:,0] * x1_range[0]\n",
        "    leftedge_X[:,1] = np.random.rand(N_u) * (x2_range[1] - x2_range[0]) + x2_range[0]\n",
        "    leftedge_X[:,2] = np.random.rand(N_u) * (t_range[1] - t_range[0]) + t_range[0]\n",
        "    leftedge_u = np.ones(shape=(N_u,1))\n",
        "    leftedge_u = u(leftedge_X[:,0], leftedge_X[:,1], leftedge_X[:,2])\n",
        "    leftedge_u = leftedge_u.reshape(-1,1)\n",
        "    \n",
        "    # BC_right: x1=x1_range[1], 0<=t<=1, x2_range[0] <= x2 <= x2_range[1]\n",
        "    rightedge_X = np.ones(shape=(N_u,3))\n",
        "    rightedge_X[:,0] = rightedge_X[:,0] * x1_range[1]\n",
        "    rightedge_X[:,1] = np.random.rand(N_u) * (x2_range[1] - x2_range[0]) + x2_range[0]\n",
        "    rightedge_X[:,2] = np.random.rand(N_u) * (t_range[1] - t_range[0]) + t_range[0]\n",
        "    rightedge_u = np.ones(shape=(N_u,1))\n",
        "    rightedge_u = u(rightedge_X[:,0], rightedge_X[:,1], rightedge_X[:,2])\n",
        "    rightedge_u = rightedge_u.reshape(-1,1)\n",
        "\n",
        "    # BC_top or BC_bottom ,depends on interface. x2 = x2_range[bc], 0<=t<=1 ,x1_range[0]<= x1 <= x_range[1]\n",
        "    bc = (interface+1)%2\n",
        "    BC_X = np.ones(shape=(N_u,3))\n",
        "    BC_X[:,0] = np.random.rand(N_u) * (x1_range[1] - x1_range[0]) + x1_range[0]\n",
        "    BC_X[:,1] = BC_X[:,1] * x2_range[bc]\n",
        "    BC_X[:,2] = np.random.rand(N_u) * (t_range[1] - t_range[0]) + t_range[0]\n",
        "    BC_u = np.ones(shape=(N_u,1))\n",
        "    BC_u = u(BC_X[:,0], BC_X[:,1], BC_X[:,2])\n",
        "    BC_u = BC_u.reshape(-1,1)\n",
        "    \n",
        "    all_X_u_train =  np.vstack([initial_X,leftedge_X,rightedge_X,BC_X])\n",
        "    all_u_train = np.vstack([initial_u,leftedge_u,rightedge_u,BC_u])\n",
        "    \n",
        "    idx = np.random.choice(all_X_u_train.shape[0],all_X_u_train.shape[0], replace=False) \n",
        "    \n",
        "    X_u_train = all_X_u_train[idx,:]\n",
        "    u_train = all_u_train[idx,:]\n",
        "    \n",
        "    # BC_interface: x2 = x2_range[interface], 0<=t<=1 ,x1_range[0]<= x1 <= x_range[1]\n",
        "    X_i_train = np.ones(shape=(N_i,3))\n",
        "    X_i_train[:,0] = np.random.rand(N_i) * (x1_range[1] - x1_range[0]) + x1_range[0]\n",
        "    X_i_train[:,1] = X_i_train[:,1] * x2_range[interface]\n",
        "    X_i_train[:,2] = np.random.rand(N_i) * (t_range[1] - t_range[0]) + t_range[0]     \n",
        "            \n",
        "            \n",
        "    '''Collocation Points'''\n",
        "    # Latin Hypercube sampling for collocation points \n",
        "    # N_f sets of tuples(x1,x2,t)\n",
        "    lb = np.array([x1_range[0],x2_range[0],t_range[0]])\n",
        "    ub = np.array([x1_range[1],x2_range[1],t_range[1]])\n",
        "    X_f_train = lb + (ub-lb)*lhs(3,N_f) \n",
        "    #X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
        "    \n",
        "    return X_u_train,u_train,X_f_train,X_i_train\n",
        "\n",
        "def ds_train_n_batch(X_u1_train,u1_train,X_f1_train,X_u2_train,u2_train,X_f2_train,X_i_train,n = 5):\n",
        "    batch_u1 = int(len(X_u1_train)/n) # = int(len(u1_train)/n)\n",
        "    batch_f1 = int(len(X_f1_train)/n)\n",
        "\n",
        "    batch_u2 = int(len(X_u2_train)/n) # = int(len(u2_train)/n)\n",
        "    batch_f2 = int(len(X_f2_train)/n)\n",
        "    \n",
        "    batch_i = int(len(X_i_train)/n)\n",
        "    \n",
        "    ds_u1 = tf.data.Dataset.from_tensor_slices( ((X_u1_train,u1_train)) ).batch( batch_u1 )\n",
        "    ds_f1 = tf.data.Dataset.from_tensor_slices( X_f1_train ).batch( batch_f1 )\n",
        "    \n",
        "    ds_u2 = tf.data.Dataset.from_tensor_slices( ((X_u2_train,u2_train)) ).batch( batch_u2 )\n",
        "    ds_f2 = tf.data.Dataset.from_tensor_slices( X_f2_train ).batch( batch_f2 )\n",
        "    \n",
        "    ds_i = tf.data.Dataset.from_tensor_slices(X_i_train).batch(batch_i)\n",
        "\n",
        "    ds = tf.data.Dataset.zip((ds_u1,ds_f1,ds_u2,ds_f2,ds_i))\n",
        "    return ds"
      ],
      "metadata": {
        "id": "FDzpcxyG95lQ"
      },
      "id": "FDzpcxyG95lQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "omga1 = [[0.,1.],[0.,1.]]  # (x,y)的取值范围 \n",
        "omga2 = [[0.,1.],[-1.,0.]] \n",
        "def test_data(N,omga,u,t=1.):\n",
        "  # Output: X,Y,T,U shape=(N,N)\n",
        "  x_range = tf.linspace(omga[0][0],omga[0][1],N)\n",
        "  y_range = tf.linspace(omga[1][0],omga[1][1],N)\n",
        "\n",
        "  X,Y = tf.meshgrid(x_range,y_range)\n",
        "  T = tf.ones(shape=X.shape)*t\n",
        "  U = u(X,Y,T)\n",
        "  return X,Y,T,U\n",
        "\n",
        "def pred_data(X,Y,T,model):\n",
        "  #Input: X,Y,T shape=(N,N)\n",
        "  #Output: U_pred shape=(N,N)\n",
        "  Shape = X.shape\n",
        "  dim = len(Shape)\n",
        "  X = tf.reshape(X,shape=(Shape[0]**dim,1))\n",
        "  Y = tf.reshape(Y,shape=(Shape[0]**dim,1))\n",
        "  T = tf.reshape(T,shape=(Shape[0]**dim,1))\n",
        "\n",
        "  Inputs = tf.concat([X,Y,T],axis=1)\n",
        "  U_pred = model(Inputs)\n",
        "  U_pred = tf.reshape(U_pred,shape=Shape)\n",
        "  return U_pred\n"
      ],
      "metadata": {
        "id": "n5j7LzvsZbpC"
      },
      "id": "n5j7LzvsZbpC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PINN Model"
      ],
      "metadata": {
        "id": "F3SCiS-sZKg_"
      },
      "id": "F3SCiS-sZKg_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c73f62",
      "metadata": {
        "id": "83c73f62"
      },
      "outputs": [],
      "source": [
        "class MyPinn(keras.Sequential): \n",
        "    def __init__(self,Layers,v,n,k,f,name = None): #Layers=[3,20,20,1]\n",
        "        super(MyPinn, self).__init__(name=name)\n",
        "        self.add(keras.Input(shape=(Layers[0],) ,dtype=tf.float64))\n",
        "        for i in range(1,len(Layers)-1):\n",
        "            self.add(keras.layers.Dense(Layers[i], dtype=tf.float64, activation='tanh'))\n",
        "        self.add(keras.layers.Dense(Layers[-1],dtype=tf.float64, name=\"outputs\"))\n",
        "        self.v = tf.constant(v,dtype=tf.float64)\n",
        "        self.n = tf.constant(n,dtype=tf.float64)\n",
        "        self.k = tf.constant(k,dtype=tf.float64)\n",
        "        self.f = f\n",
        "        \n",
        "    @tf.function\n",
        "    def loss_U(self,X_u_train,u_train):\n",
        "        u= self(X_u_train)\n",
        "        loss_u = tf.reduce_mean(tf.square(u_train - u))\n",
        "        return loss_u\n",
        "        \n",
        "    @tf.function\n",
        "    def loss_PDE(self,X_f_train):\n",
        "        x = X_f_train[:,0:1]\n",
        "        y = X_f_train[:,1:2]\n",
        "        t = X_f_train[:,2:3]\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch([x,y,t])\n",
        "            X = tf.stack([x[:,0],y[:,0],t[:,0]],axis=1)\n",
        "            u = self(X)  \n",
        "            u_x = tape.gradient(u,x)\n",
        "            u_y = tape.gradient(u,y)\n",
        "            #u_t = tape.gradient(u,t)\n",
        "        #tf.print(u_x)    \n",
        "        u_t = tape.gradient(u, t)     \n",
        "        u_xx = tape.gradient(u_x, x)\n",
        "        u_yy = tape.gradient(u_y, y)\n",
        "\n",
        "        del tape\n",
        "      \n",
        "        loss_f = u_t - self.v *(u_xx + u_yy) - self.f(x,y,t)\n",
        "        loss_f = tf.reduce_mean(tf.square(loss_f))\n",
        "        return loss_f\n",
        "    \"\"\"以下为单区域PINN 训练方法\"\"\"    \n",
        "    @tf.function\n",
        "    def train_loop(self,X_u_train,u_train,X_f_train):\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss = self.loss_U(X_u_train,u_train) + self.loss_PDE(X_f_train)\n",
        "      gradients = tape.gradient(loss,self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "      return loss\n",
        "\n",
        "    def train(self,X_u_train,u_train,X_f_train,epochs=200):\n",
        "      for epoch in tf.range(1,epochs+1):\n",
        "          loss = self.train_loop(X_u_train,u_train,X_f_train)\n",
        "          if epoch % 50 == 0:                \n",
        "              tf.print(\n",
        "                  \"Training loss (for per 10 epoches) at epoch \",epoch,\":\",loss\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x9obOkjDmwOp"
      },
      "id": "x9obOkjDmwOp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CouplePinn(keras.Model):\n",
        "  def __init__(self,model_1,model_2,name=None):\n",
        "    super(CouplePinn,self).__init__(name=name)\n",
        "    self.m1 = model_1 \n",
        "    self.m2 = model_2\n",
        "    \n",
        "    # Metrics\n",
        "    self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "    self.loss_adaptive = keras.metrics.Mean(name=\"loss_adaptive\")\n",
        "    self.loss_m1 = keras.metrics.Mean(name=\"loss_m1\")\n",
        "    self.loss_m2 = keras.metrics.Mean(name=\"loss_m2\")\n",
        "    self.loss_i = keras.metrics.Mean(name=\"loss_i\")\n",
        "\n",
        "    # Self-Adaptive factors     \n",
        "    self.alpha1 = tf.Variable(0.0, dtype = tf.float64, trainable = True)     \n",
        "    self.alpha2 = tf.Variable(0.0, dtype = tf.float64, trainable = True)\n",
        "    #self.alpha_i = tf.Variable(1.0, dtype = tf.float64, trainable = True)\n",
        "\n",
        "    self.optimizer_alpha = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    #self.optimizer_alpha_m2 = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    #self.optimizer_alpha_i = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  @property\n",
        "  def metrics(self):\n",
        "      # We list our `Metric` objects here so that `reset_states()` can be\n",
        "      # called automatically at the start of each epoch\n",
        "      # or at the start of `evaluate()`.\n",
        "      # If you don't implement this property, you have to call\n",
        "      # `reset_states()` yourself at the time of your choosing.\n",
        "      return [self.loss_adaptive,self.loss_tracker,self.loss_m1,self.loss_m2,self.loss_i] \n",
        "\n",
        "\n",
        "  \"\"\" 交界处损失函数 \"\"\"\n",
        "  @tf.function\n",
        "  def loss_I(self,X_i_train):\n",
        "      x = X_i_train[:,0:1]\n",
        "      y = X_i_train[:,1:2]\n",
        "      t = X_i_train[:,2:3]\n",
        "      \n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "          tape.watch([x,y,t])\n",
        "          X = tf.stack([x[:,0],y[:,0],t[:,0]],axis=1)\n",
        "          U1 = self.m1(X)\n",
        "          U2 = self.m2(X)\n",
        "      \n",
        "      U1_x = tape.gradient(U1, x)\n",
        "      U1_y = tape.gradient(U1, y)\n",
        "      U2_x = tape.gradient(U2, x)\n",
        "      U2_y = tape.gradient(U2, y)\n",
        "\n",
        "      del tape\n",
        "      \n",
        "      loss_i1 = -self.m1.v * ( U1_x*self.m1.n[0] + U1_y*self.m1.n[1]) - self.m1.k * (U1-U2)\n",
        "      loss_i2 = -self.m2.v * ( U2_x*self.m2.n[0] + U2_y*self.m2.n[1]) - self.m2.k * (U2-U1)\n",
        "\n",
        "      loss_i1 = tf.reduce_mean(tf.square(loss_i1)) \n",
        "      loss_i2 = tf.reduce_mean(tf.square(loss_i2))\n",
        "\n",
        "      return loss_i1,loss_i2\n",
        "\n",
        "  \"\"\"耦合区域 PINN训练方法\"\"\"\n",
        "  @tf.function\n",
        "  def train_step(self,data):\n",
        "      (X_u1_train,u1_train),X_f1_train,(X_u2_train,u2_train),X_f2_train,X_i_train = data #unpack\n",
        "      with tf.GradientTape(persistent=True) as tape: \n",
        "          loss_m1 = self.m1.loss_U(X_u1_train,u1_train) + self.m1.loss_PDE(X_f1_train)\n",
        "          loss_m2 = self.m2.loss_U(X_u2_train,u2_train) + self.m2.loss_PDE(X_f2_train)\n",
        "          loss_i1,loss_i2 = self.loss_I(X_i_train)\n",
        "\n",
        "          a1 = tf.math.sigmoid(self.alpha1) * 2\n",
        "          a2 = tf.math.sigmoid(self.alpha2) * 2\n",
        "          loss_adaptive = a1*(loss_m1+loss_i1) + a2*(loss_m2+loss_i2)\n",
        "\n",
        "      loss = loss_m1 + loss_m2 + loss_i1 + loss_i2\n",
        "      # Compute gradients\n",
        "      gradients_m1 = tape.gradient(loss_adaptive,self.m1.trainable_variables)\n",
        "      gradients_m2 = tape.gradient(loss_adaptive,self.m2.trainable_variables)\n",
        "      gradient_a1pha1 = -tape.gradient(loss_adaptive,self.alpha1)\n",
        "      gradient_alpha2 = -tape.gradient(loss_adaptive,self.alpha2)\n",
        "\n",
        "      del tape\n",
        "\n",
        "      # Updata parameters\n",
        "      self.m1.optimizer.apply_gradients(zip(gradients_m1, self.m1.trainable_variables))\n",
        "      self.m2.optimizer.apply_gradients(zip(gradients_m2, self.m2.trainable_variables))\n",
        "      self.optimizer_alpha.apply_gradients( zip([gradient_a1pha1,gradient_alpha2], [self.alpha1,self.alpha2]) )\n",
        "\n",
        "      # Updata Metric\n",
        "      self.loss_adaptive.update_state(loss_adaptive)\n",
        "      self.loss_tracker.update_state(loss)\n",
        "      self.loss_m1.update_state(loss_m1)\n",
        "      self.loss_m2.update_state(loss_m2)\n",
        "      self.loss_i.update_state(loss_i1+loss_i2)\n",
        "\n",
        "      return {m.name : m.result() for m in self.metrics}\n",
        "  "
      ],
      "metadata": {
        "id": "uc1lMJpzP6WW"
      },
      "id": "uc1lMJpzP6WW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "BVntbsKwcKf3"
      },
      "id": "BVntbsKwcKf3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb08a40",
      "metadata": {
        "scrolled": true,
        "id": "cfb08a40"
      },
      "outputs": [],
      "source": [
        "\"\"\" Training Data \"\"\"\n",
        "n1 = [0.,-1.]\n",
        "n2 = [0.,1.]\n",
        "\n",
        "omga1 = [[0.,1.],[0.,1.]]  # (x,y)的取值范围 \n",
        "omga2 = [[0.,1.],[-1.,0.]] \n",
        "\n",
        "N_i = 500\n",
        "\n",
        "N_u1 = 500; N_f1 = 2000  ;omga = omga1; interface=0; u = u1\n",
        "X_u1_train,u1_train,X_f1_train,X_i_train= trainingdata(N_u1,N_f1,N_i,omga,u,interface)\n",
        "\n",
        "\n",
        "N_u2 = 500; N_f2= 2000 ; omga = omga2; interface=1; u = u2\n",
        "X_u2_train,u2_train,X_f2_train,X_i_train= trainingdata(N_u2,N_f2,N_i,omga,u,interface)\n",
        "\n",
        "\"\"\" Test Data \"\"\"\n",
        "N = 100\n",
        "X_u1,Y_u1,T_u1,U1 = test_data(N,omga1,u1,t=1.0)\n",
        "X_u2,Y_u2,T_u2,U2 = test_data(N,omga2,u2,t=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e46f3c2",
      "metadata": {
        "id": "5e46f3c2"
      },
      "outputs": [],
      "source": [
        "Layers=[3,20,20,20,20,1]\n",
        "model_u1 = MyPinn(Layers,v1,n1,k,f1,name=\"model_1\")\n",
        "model_u1.compile(optimizer=keras.optimizers.Adam())\n",
        "\n",
        "model_u2 = MyPinn(Layers,v2,n2,k,f2,name=\"model_2\")\n",
        "model_u2.compile(optimizer=keras.optimizers.Adam())\n",
        "\n",
        "ds = ds_train_n_batch(X_u1_train, u1_train , X_f1_train , X_u2_train , u2_train , X_f2_train , X_i_train ,n = 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_u1.train(X_u1_train,u1_train,X_f1_train,epochs=1000) #单区域PINN训练\n",
        "model_u2.train(X_u2_train,u2_train,X_f2_train,epochs=1000) #单区域PINN训练"
      ],
      "metadata": {
        "id": "MZITNwuOyxPW"
      },
      "id": "MZITNwuOyxPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "02d3a630",
      "metadata": {
        "id": "02d3a630"
      },
      "outputs": [],
      "source": [
        "couple_model = CouplePinn(model_u1,model_u2) \n",
        "couple_model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "couple_model.fit(ds,epochs=100)#耦合PINN训练"
      ],
      "metadata": {
        "id": "H41deEkHaFvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57885fc5-9941-46bf-f700-7fe40c546c68"
      },
      "id": "H41deEkHaFvt",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 46s 36ms/step - loss_adaptive: 0.2616 - loss: 0.2612 - loss_m1: 0.0454 - loss_m2: 0.0716 - loss_i: 0.1442\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.1334 - loss: 0.1326 - loss_m1: 0.0495 - loss_m2: 0.0470 - loss_i: 0.0361\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0893 - loss: 0.0884 - loss_m1: 0.0237 - loss_m2: 0.0355 - loss_i: 0.0293\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0733 - loss: 0.0724 - loss_m1: 0.0181 - loss_m2: 0.0322 - loss_i: 0.0221\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0647 - loss: 0.0638 - loss_m1: 0.0148 - loss_m2: 0.0300 - loss_i: 0.0190\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0576 - loss: 0.0566 - loss_m1: 0.0138 - loss_m2: 0.0287 - loss_i: 0.0141\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 37ms/step - loss_adaptive: 0.0531 - loss: 0.0521 - loss_m1: 0.0119 - loss_m2: 0.0269 - loss_i: 0.0133\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0493 - loss: 0.0483 - loss_m1: 0.0112 - loss_m2: 0.0262 - loss_i: 0.0109\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0463 - loss: 0.0452 - loss_m1: 0.0103 - loss_m2: 0.0252 - loss_i: 0.0098\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0437 - loss: 0.0426 - loss_m1: 0.0097 - loss_m2: 0.0245 - loss_i: 0.0084\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss_adaptive: 0.0417 - loss: 0.0405 - loss_m1: 0.0091 - loss_m2: 0.0238 - loss_i: 0.0077\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0399 - loss: 0.0388 - loss_m1: 0.0086 - loss_m2: 0.0232 - loss_i: 0.0069\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0385 - loss: 0.0373 - loss_m1: 0.0082 - loss_m2: 0.0227 - loss_i: 0.0064\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 34ms/step - loss_adaptive: 0.0372 - loss: 0.0360 - loss_m1: 0.0079 - loss_m2: 0.0222 - loss_i: 0.0059\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.0362 - loss: 0.0349 - loss_m1: 0.0077 - loss_m2: 0.0217 - loss_i: 0.0055\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 25ms/step - loss_adaptive: 0.0352 - loss: 0.0339 - loss_m1: 0.0074 - loss_m2: 0.0213 - loss_i: 0.0052\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 34ms/step - loss_adaptive: 0.0343 - loss: 0.0330 - loss_m1: 0.0072 - loss_m2: 0.0209 - loss_i: 0.0049\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 36ms/step - loss_adaptive: 0.0336 - loss: 0.0322 - loss_m1: 0.0070 - loss_m2: 0.0205 - loss_i: 0.0047\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 24ms/step - loss_adaptive: 0.0329 - loss: 0.0315 - loss_m1: 0.0068 - loss_m2: 0.0201 - loss_i: 0.0046\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 25ms/step - loss_adaptive: 0.0322 - loss: 0.0308 - loss_m1: 0.0066 - loss_m2: 0.0197 - loss_i: 0.0045\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss_adaptive: 0.0316 - loss: 0.0301 - loss_m1: 0.0064 - loss_m2: 0.0194 - loss_i: 0.0044\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0310 - loss: 0.0295 - loss_m1: 0.0062 - loss_m2: 0.0190 - loss_i: 0.0043\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0304 - loss: 0.0289 - loss_m1: 0.0060 - loss_m2: 0.0187 - loss_i: 0.0042\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.0299 - loss: 0.0283 - loss_m1: 0.0058 - loss_m2: 0.0184 - loss_i: 0.0041\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0293 - loss: 0.0278 - loss_m1: 0.0056 - loss_m2: 0.0181 - loss_i: 0.0040\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 25ms/step - loss_adaptive: 0.0288 - loss: 0.0272 - loss_m1: 0.0055 - loss_m2: 0.0178 - loss_i: 0.0040\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0283 - loss: 0.0267 - loss_m1: 0.0053 - loss_m2: 0.0175 - loss_i: 0.0039\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0278 - loss: 0.0262 - loss_m1: 0.0051 - loss_m2: 0.0172 - loss_i: 0.0039\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0274 - loss: 0.0257 - loss_m1: 0.0050 - loss_m2: 0.0169 - loss_i: 0.0038\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0269 - loss: 0.0252 - loss_m1: 0.0048 - loss_m2: 0.0167 - loss_i: 0.0037\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss_adaptive: 0.0265 - loss: 0.0248 - loss_m1: 0.0047 - loss_m2: 0.0164 - loss_i: 0.0037\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.0260 - loss: 0.0243 - loss_m1: 0.0045 - loss_m2: 0.0161 - loss_i: 0.0036\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss_adaptive: 0.0256 - loss: 0.0239 - loss_m1: 0.0044 - loss_m2: 0.0159 - loss_i: 0.0036\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss_adaptive: 0.0252 - loss: 0.0235 - loss_m1: 0.0043 - loss_m2: 0.0156 - loss_i: 0.0036\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0248 - loss: 0.0231 - loss_m1: 0.0042 - loss_m2: 0.0154 - loss_i: 0.0035\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss_adaptive: 0.0245 - loss: 0.0227 - loss_m1: 0.0041 - loss_m2: 0.0151 - loss_i: 0.0035\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0241 - loss: 0.0223 - loss_m1: 0.0040 - loss_m2: 0.0149 - loss_i: 0.0035\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0238 - loss: 0.0220 - loss_m1: 0.0039 - loss_m2: 0.0147 - loss_i: 0.0034\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss_adaptive: 0.0235 - loss: 0.0216 - loss_m1: 0.0038 - loss_m2: 0.0144 - loss_i: 0.0034\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0231 - loss: 0.0213 - loss_m1: 0.0037 - loss_m2: 0.0142 - loss_i: 0.0034\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 27ms/step - loss_adaptive: 0.0228 - loss: 0.0210 - loss_m1: 0.0036 - loss_m2: 0.0140 - loss_i: 0.0034\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss_adaptive: 0.0225 - loss: 0.0207 - loss_m1: 0.0035 - loss_m2: 0.0138 - loss_i: 0.0033\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0223 - loss: 0.0204 - loss_m1: 0.0035 - loss_m2: 0.0136 - loss_i: 0.0033\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0220 - loss: 0.0201 - loss_m1: 0.0034 - loss_m2: 0.0134 - loss_i: 0.0033\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 22ms/step - loss_adaptive: 0.0217 - loss: 0.0198 - loss_m1: 0.0033 - loss_m2: 0.0131 - loss_i: 0.0033\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 34ms/step - loss_adaptive: 0.0215 - loss: 0.0195 - loss_m1: 0.0033 - loss_m2: 0.0129 - loss_i: 0.0033\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0212 - loss: 0.0192 - loss_m1: 0.0032 - loss_m2: 0.0127 - loss_i: 0.0033\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 36ms/step - loss_adaptive: 0.0210 - loss: 0.0190 - loss_m1: 0.0032 - loss_m2: 0.0125 - loss_i: 0.0033\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 36ms/step - loss_adaptive: 0.0207 - loss: 0.0187 - loss_m1: 0.0031 - loss_m2: 0.0123 - loss_i: 0.0033\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0205 - loss: 0.0185 - loss_m1: 0.0031 - loss_m2: 0.0121 - loss_i: 0.0033\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0202 - loss: 0.0182 - loss_m1: 0.0031 - loss_m2: 0.0119 - loss_i: 0.0032\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0200 - loss: 0.0180 - loss_m1: 0.0030 - loss_m2: 0.0117 - loss_i: 0.0032\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 27ms/step - loss_adaptive: 0.0198 - loss: 0.0178 - loss_m1: 0.0030 - loss_m2: 0.0116 - loss_i: 0.0032\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0195 - loss: 0.0175 - loss_m1: 0.0029 - loss_m2: 0.0114 - loss_i: 0.0032\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0193 - loss: 0.0173 - loss_m1: 0.0029 - loss_m2: 0.0112 - loss_i: 0.0032\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.0191 - loss: 0.0171 - loss_m1: 0.0029 - loss_m2: 0.0110 - loss_i: 0.0032\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0188 - loss: 0.0168 - loss_m1: 0.0028 - loss_m2: 0.0108 - loss_i: 0.0032\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0186 - loss: 0.0166 - loss_m1: 0.0028 - loss_m2: 0.0106 - loss_i: 0.0032\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0184 - loss: 0.0164 - loss_m1: 0.0028 - loss_m2: 0.0104 - loss_i: 0.0032\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss_adaptive: 0.0182 - loss: 0.0161 - loss_m1: 0.0027 - loss_m2: 0.0103 - loss_i: 0.0031\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0179 - loss: 0.0159 - loss_m1: 0.0027 - loss_m2: 0.0101 - loss_i: 0.0031\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0177 - loss: 0.0157 - loss_m1: 0.0027 - loss_m2: 0.0099 - loss_i: 0.0031\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 34ms/step - loss_adaptive: 0.0174 - loss: 0.0154 - loss_m1: 0.0026 - loss_m2: 0.0097 - loss_i: 0.0031\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0172 - loss: 0.0152 - loss_m1: 0.0026 - loss_m2: 0.0096 - loss_i: 0.0030\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss_adaptive: 0.0170 - loss: 0.0149 - loss_m1: 0.0026 - loss_m2: 0.0094 - loss_i: 0.0030\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 34ms/step - loss_adaptive: 0.0167 - loss: 0.0147 - loss_m1: 0.0025 - loss_m2: 0.0092 - loss_i: 0.0029\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0165 - loss: 0.0145 - loss_m1: 0.0025 - loss_m2: 0.0091 - loss_i: 0.0029\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.0162 - loss: 0.0142 - loss_m1: 0.0025 - loss_m2: 0.0089 - loss_i: 0.0028\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0160 - loss: 0.0140 - loss_m1: 0.0024 - loss_m2: 0.0088 - loss_i: 0.0028\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0157 - loss: 0.0137 - loss_m1: 0.0024 - loss_m2: 0.0087 - loss_i: 0.0027\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0155 - loss: 0.0135 - loss_m1: 0.0024 - loss_m2: 0.0085 - loss_i: 0.0026\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 27ms/step - loss_adaptive: 0.0152 - loss: 0.0133 - loss_m1: 0.0023 - loss_m2: 0.0084 - loss_i: 0.0026\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss_adaptive: 0.0150 - loss: 0.0130 - loss_m1: 0.0023 - loss_m2: 0.0083 - loss_i: 0.0025\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 27ms/step - loss_adaptive: 0.0147 - loss: 0.0128 - loss_m1: 0.0023 - loss_m2: 0.0082 - loss_i: 0.0024\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0145 - loss: 0.0126 - loss_m1: 0.0022 - loss_m2: 0.0081 - loss_i: 0.0023\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 27ms/step - loss_adaptive: 0.0143 - loss: 0.0124 - loss_m1: 0.0022 - loss_m2: 0.0080 - loss_i: 0.0022\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0141 - loss: 0.0122 - loss_m1: 0.0022 - loss_m2: 0.0079 - loss_i: 0.0022\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 31ms/step - loss_adaptive: 0.0139 - loss: 0.0120 - loss_m1: 0.0021 - loss_m2: 0.0078 - loss_i: 0.0021\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 45ms/step - loss_adaptive: 0.0137 - loss: 0.0118 - loss_m1: 0.0021 - loss_m2: 0.0077 - loss_i: 0.0020\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss_adaptive: 0.0135 - loss: 0.0117 - loss_m1: 0.0021 - loss_m2: 0.0076 - loss_i: 0.0020\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss_adaptive: 0.0134 - loss: 0.0115 - loss_m1: 0.0020 - loss_m2: 0.0076 - loss_i: 0.0019\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.0132 - loss: 0.0114 - loss_m1: 0.0020 - loss_m2: 0.0075 - loss_i: 0.0019\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0131 - loss: 0.0112 - loss_m1: 0.0020 - loss_m2: 0.0074 - loss_i: 0.0018\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss_adaptive: 0.0129 - loss: 0.0111 - loss_m1: 0.0019 - loss_m2: 0.0074 - loss_i: 0.0018\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0128 - loss: 0.0110 - loss_m1: 0.0019 - loss_m2: 0.0073 - loss_i: 0.0017\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0127 - loss: 0.0109 - loss_m1: 0.0019 - loss_m2: 0.0073 - loss_i: 0.0017\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0126 - loss: 0.0108 - loss_m1: 0.0019 - loss_m2: 0.0072 - loss_i: 0.0017\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss_adaptive: 0.0125 - loss: 0.0107 - loss_m1: 0.0018 - loss_m2: 0.0072 - loss_i: 0.0017\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss_adaptive: 0.0124 - loss: 0.0106 - loss_m1: 0.0018 - loss_m2: 0.0071 - loss_i: 0.0016\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss_adaptive: 0.0123 - loss: 0.0105 - loss_m1: 0.0018 - loss_m2: 0.0071 - loss_i: 0.0016\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0122 - loss: 0.0104 - loss_m1: 0.0018 - loss_m2: 0.0070 - loss_i: 0.0016\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 34ms/step - loss_adaptive: 0.0121 - loss: 0.0103 - loss_m1: 0.0018 - loss_m2: 0.0070 - loss_i: 0.0016\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0121 - loss: 0.0102 - loss_m1: 0.0017 - loss_m2: 0.0069 - loss_i: 0.0016\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 33ms/step - loss_adaptive: 0.0120 - loss: 0.0102 - loss_m1: 0.0017 - loss_m2: 0.0069 - loss_i: 0.0016\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0119 - loss: 0.0101 - loss_m1: 0.0017 - loss_m2: 0.0068 - loss_i: 0.0016\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0118 - loss: 0.0100 - loss_m1: 0.0017 - loss_m2: 0.0068 - loss_i: 0.0015\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0118 - loss: 0.0099 - loss_m1: 0.0016 - loss_m2: 0.0068 - loss_i: 0.0015\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 34ms/step - loss_adaptive: 0.0117 - loss: 0.0099 - loss_m1: 0.0016 - loss_m2: 0.0067 - loss_i: 0.0015\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 32ms/step - loss_adaptive: 0.0117 - loss: 0.0098 - loss_m1: 0.0016 - loss_m2: 0.0067 - loss_i: 0.0015\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss_adaptive: 0.0116 - loss: 0.0097 - loss_m1: 0.0016 - loss_m2: 0.0066 - loss_i: 0.0015\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8170321f50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "u1_i = u1(X_i_train[:,0],X_i_train[:,1],X_i_train[:,2])\n",
        "u2_i = u2(X_i_train[:,0],X_i_train[:,1],X_i_train[:,2])\n",
        "u1_i = u1_i.reshape((u1_i.shape[0],1))\n",
        "u2_i = u1_i.reshape((u2_i.shape[0],1))\n",
        "\n",
        "X_u1_train_ = tf.concat([X_u1_train,X_i_train],axis=0)\n",
        "X_u2_train_ = tf.concat([X_u2_train,X_i_train],axis=0)\n",
        "u1_train_ = tf.concat([u1_train,u1_i],axis=0)\n",
        "u2_train_ = tf.concat([u2_train,u2_i],axis=0) \n",
        "'''"
      ],
      "metadata": {
        "id": "4f5fpeqYjU3N"
      },
      "id": "4f5fpeqYjU3N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"history_u1 = model_u1.fit(\n",
        "    X_u1_train,\n",
        "    u1_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    #validation_data=(x_val, y_val),\n",
        ")\n",
        "history_u2 = model_u2.fit(\n",
        "    X_u2_train,\n",
        "    u2_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    #validation_data=(x_val, y_val),\n",
        ")\"\"\""
      ],
      "metadata": {
        "id": "uVQWLp9Mnlvl"
      },
      "id": "uVQWLp9Mnlvl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "L6Mnir9gbM6T"
      },
      "id": "L6Mnir9gbM6T"
    },
    {
      "cell_type": "code",
      "source": [
        "U1_pred = pred_data(X_u1,Y_u1,T_u1,model_u1)\n",
        "U2_pred = pred_data(X_u2,Y_u2,T_u2,model_u2)\n",
        "####################################################\n",
        "plt.figure(1) \n",
        "ax1 = plt.axes(projection=\"3d\")\n",
        "ax1.patch.set_facecolor(\"white\")   #设置 axes 背景颜色\n",
        "\n",
        "ax1.plot_surface(X_u1,Y_u1,U1,alpha=0.9) #生成曲面z，alpha调节透明度，cmap调节曲面色调\n",
        "ax1.plot_surface(X_u2,Y_u2,U2,alpha=0.9)\n",
        "\n",
        "ax1.set_xlabel(\"X\")   \n",
        "ax1.set_ylabel(\"Y\")\n",
        "ax1.set_zlabel(\"U\")\n",
        "ax1.set_title(\"Real U\")\n",
        "######################################################\n",
        "plt.figure(2) \n",
        "ax2 = plt.axes(projection=\"3d\")\n",
        "ax2.patch.set_facecolor(\"white\") \n",
        "\n",
        "### u1\n",
        "ax2.plot_surface(X_u1,Y_u1,U1_pred,alpha=0.9)\n",
        "\n",
        "###u2\n",
        "ax2.plot_surface(X_u2,Y_u2,U2_pred,alpha=0.9)\n",
        "\n",
        "ax2.set_xlabel(\"X\")   \n",
        "ax2.set_ylabel(\"Y\")\n",
        "ax2.set_zlabel(\"U\")\n",
        "ax2.set_title(\"Pred U\")\n",
        "###############################\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XQZfCqv297I1"
      },
      "id": "XQZfCqv297I1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u1_real = U1.numpy().flatten()\n",
        "u1_pred = U1_pred.numpy().flatten()\n",
        "u2_real = U2.numpy().flatten()\n",
        "u2_pred = U2_pred.numpy().flatten()\n",
        "\n",
        "error_u1 = np.linalg.norm((u1_real-u1_pred),2)/np.linalg.norm(u1_real,2)  \n",
        "print('Test Error For u1:  %.5f'  % (error_u1))\n",
        "\n",
        "error_u2 = np.linalg.norm((u2_real-u2_pred),2)/np.linalg.norm(u2_real,2)  \n",
        "print('Test Error For u2:  %.5f'  % (error_u2))"
      ],
      "metadata": {
        "id": "TbBGSi9j1AQU"
      },
      "id": "TbBGSi9j1AQU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "7_20_Self-Adaptive-Parabolic耦合pde模型.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "01l-S8EOdodM",
        "yAt6asqSbzHy",
        "6ihipZqLb1m3"
      ],
      "toc_visible": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}