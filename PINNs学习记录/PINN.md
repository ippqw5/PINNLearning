# è€¦åˆPINNã€æ­£åé—®é¢˜ã€3Dé—®é¢˜çš„å­¦ä¹ ç ”ç©¶



# 06-29 

### ä»€ä¹ˆæ˜¯PINNï¼Ÿ

Raissiç­‰äººåœ¨2018å¹´ 

 [Physics-informed neural networks: A deep learning 
framework for solving forward and inverse problems involving 
nonlinear partial differential equations](https://github.com/maziarraissi/PINNs)

ä¸­æå‡ºäº†PINNï¼Œé€šè¿‡åœ¨æŸè€—å‡½æ•°ä¸­ç»“åˆç‰©ç†åœºï¼ˆå³åå¾®åˆ†æ–¹ç¨‹ï¼‰å’Œè¾¹ç•Œæ¡ä»¶æ¥è§£å†³åå¾®åˆ†æ–¹ç¨‹ã€‚æŸå¤±æ˜¯åå¾®åˆ†æ–¹ç¨‹çš„å‡æ–¹è¯¯å·®å’Œåœ¨åˆ†å¸ƒåœ¨åŸŸä¸­çš„â€œæ­é…ç‚¹â€ä¸Šæµ‹é‡çš„è¾¹ç•Œæ®‹å·®ã€‚



ä»–ä»¬åœ¨æ–‡ç« ä¸­åˆ—å‡ºäº†æ•°ä¸ªç»å…¸pdeç®—ä¾‹æ¥éªŒè¯PINNçš„æ­£ç¡®æ€§ã€‚åœ¨ä»–ä»¬çš„githubä¸»é¡µä¸Šæä¾›äº†æºç ï¼Œä½¿ç”¨äº†Tensorflow1.xçš„æ¡†æ¶ã€‚

æˆªæ­¢åˆ°2022-06-29ï¼ŒTensorFlowæ›´æ–°åˆ°2.9ï¼ŒTensorFlow1.xä»£ç é£æ ¼è·ŸTensorFlow2.xç›¸å·®ç”šè¿œï¼Œè€Œä¸”Raissiç­‰äººæä¾›çš„ä»£ç ä¹Ÿæœ‰è®¸å¤šå€¼å¾—ä¼˜åŒ–çš„åœ°æ–¹ã€‚

æˆ‘åœ¨githubä¸Šæ‰¾åˆ°ä¸€ä¸ªäº†PINNåœ¨TensorFlow2.xä¸Šçš„ä»£ç æ¡†æ¶ [omniscientoctopus/Physics-Informed-Neural-Networks: Investigating PINNs (github.com)](https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks),



### æœ¬æ–‡ç ”ç©¶ä»€ä¹ˆï¼Ÿ

åˆ©ç”¨æš‘å‡ç ”ç©¶ä¸€ä¸‹å¦‚ä¸‹é—®é¢˜ï¼š 

- PINNæ¨¡å‹åœ¨è€¦åˆé—®é¢˜ä¸Šçš„åº”ç”¨ï¼ˆè¿™æ–¹é¢çš„ç ”ç©¶ä¼¼ä¹è¿˜ä¸å¤šï¼‰ï¼Œ
- PINNæ­£è´Ÿé—®é¢˜æ±‚è§£ï¼ˆçœ‹äº†ä¸€äº›æ–‡ç« è¯´PINNçš„ä¼˜åŠ¿å…¶å®åœ¨äº **é«˜ç»´é—®é¢˜ & åé—®é¢˜æ±‚è§£**
- ä½¿ç”¨tensorflow2.0ï¼Œä»£ç å®ç°è‹¥å¹²ç®—ä¾‹ï¼ˆä¹Ÿè®¸å¯èƒ½maybe Only oneğŸ˜ï¼‰ï¼Œæœ€å¥½èƒ½æŠŠæ¨¡å‹åº”ç”¨åˆ°3ç»´ã€‚



### PINNæ¨¡å‹æ­å»º

> å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨ï¼Œå…ˆæŒæ¡TensorFlowï¼Œå¹¶ä½¿ç”¨TensorFlowè‡ªå®šä¹‰PINNæ¨¡å‹æ˜¯å¿…è¦çš„ã€‚

å…ˆè®¾æƒ³ä¸€ä¸ªä»£ç æ¡†æ¶ã€‚

PINNçš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æœå…ˆä¸è€ƒè™‘losså‡½æ•°ï¼Œé‚£ä¹ˆPINNå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å…¨è¿æ¥åºåˆ—æ¨¡å‹ï¼Œå¯ä»¥ç”¨ **tf.keras.Sequential()**æ„å»ºã€‚

å‡è®¾Layer = [inputs,n1,n2,...nk,n_outputs]ï¼Œä»å·¦è‡³å³ä»£è¡¨æ¯å±‚ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚

```python
def createModel(layer):
    if len(layer) < 2:
        print("å±‚æ•°å°äº2ï¼, æ— æ³•æ„å»ºç¥ç»ç½‘ç»œ")
        return 
    model = keras.Sequential(name = "PINN")
    model.add(keras.Input(shape=(layer[0],)))
    for i in range(1,len(layer)-1):
        model.add(layers.Dense(layer[i], activation="relu", name="layer{}".format(i)))
    model.add(layers.Dense(layer[-1], name="outputs"))
    
    #  model.compile( loss= , metrics = [], optimizer= )
    #	...
    #
    #
    
    return model
```

**<font color='purple'>åç»­è¦ä¸º model æ·»åŠ  lossï¼ˆPINNä¸»è¦éƒ¨åˆ†ï¼‰ã€metricï¼ˆå¯ä»¥ä¸è¦ï¼‰ã€optimizerï¼ˆä¼˜åŒ–å™¨ï¼Œå¿…è¦ï¼‰</font>**

> ä»¥ä¸Šå†…å®¹äº 2022 -  06 - 29  markdownã€‚

---

# 06-30

### <font color='blue'>1.PINNæ¨¡å‹æ­å»ºï¼ˆç»­ï¼‰</font>

TensorFlowä¸ºæˆ‘ä»¬æä¾›äº†å¤šç§å¤šæ ·çš„é«˜é˜¶APIå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿæ­å»ºæ¨¡å‹ã€‚**ä½†æ˜¯ï¼Œå¿«é€Ÿæ–¹ä¾¿çš„ä»£ä»·å°±æ˜¯çµæ´»æ€§é™ä½ã€‚**ç‰¹åˆ«æ˜¯PINNè·Ÿä¸€èˆ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ­¥éª¤ä¸åŒï¼Œä¸»è¦æ¥æºäºloss functionéœ€è¦å¯¹é¢„æµ‹å€¼è¿›è¡Œæ±‚å¯¼è¿ç®—ã€‚

ä»Šå¤©çš„ä»£ç ç¼–å†™é­é‡äº†ä¸€äº›å›°éš¾ï¼Œæˆ‘æ˜¨å¤©çš„æ„æ€å®é™…ä¸Šä¸å¯è¡Œï¼š

1. ä½¿ç”¨ **tf.keras.Sequential()**æˆ–è€…**tf.keras.Model()å‡½æ•°å¼API** æ­å»ºæ¨¡å‹ Modelã€‚(è¿™ä¸€æ­¥æ²¡é—®é¢˜ï¼‰

2. ç¼–å†™è‡ªå®šä¹‰losså‡½æ•°ï¼Œç„¶åå°† è‡ªå®šä¹‰çš„loss & ä¼˜åŒ–å™¨ & metrics ä¼ å…¥è‡ªå¸¦çš„æ–¹æ³• **Model.compile()**

3. è°ƒç”¨è‡ªå¸¦çš„æ–¹æ³• **Model.fit()** ï¼Œä¼ å…¥è®­ç»ƒæ•°æ®è®­ç»ƒå³å¯ã€‚



æ­¥éª¤1 å’Œ ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚è€ŒPINNæ¨¡å‹ç»“æ„ç®€å•ï¼Œä½¿ç”¨æ­¥éª¤1æ­å»ºæ¨¡å‹å®Œå…¨æ²¡æœ‰é—®é¢˜ã€‚

é—®é¢˜å‡ºåœ¨ï¼ˆæ­¥éª¤2ï¼Œæ­¥éª¤3ï¼‰ã€‚

è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å…¶ä¸­ä¸¤ä¸ªå…³é”®çš„æ–¹æ³• **Model.compile() & Model.fit()** æ–¹æ³•ç©¶ç«Ÿåšäº†ä»€ä¹ˆï¼Ÿ

å¦‚æœæ˜ç™½äº†Model.fit()çš„å·¥ä½œæ­¥éª¤ï¼Œå°±ä¼šçŸ¥é“è¿™ç§è‡ªå¸¦çš„è®­ç»ƒæ–¹å¼å±€é™æ€§ï¼Œä»¥åŠä¸ºä»€ä¹ˆåœ¨è®­ç»ƒPINNæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ã€‚



**Model.fit()å¤§è‡´å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š**

```python
def fit(self,X_train,Y_train,epoches,batch_size,**kwargs):
    #
    #		ä¸»è¦çš„è®­ç»ƒéƒ¨åˆ†ä»£ç 
    #
    for epoch in epoches: ## ä½¿ç”¨ X_train,Y_train è¿›è¡Œ epoch æ¬¡è®­ç»ƒ
        for x_batch,y_batch in dataset(X_train,Y_train,batch_size): ## dataset(X_train,Y_train,batch_size) æ•°æ®åˆ†ç»„
            train_step((x_batch,y_batch))  ## train_step(self,data) æ˜¯ ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥é‡è½½ï¼Œä»è€Œæ”¹å˜fit()

def train_step(self,data):
    
    # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.
    x, y = data

    with tf.GradientTape(persistent=True) as tape:
        y_pred = self(x, training=True)  # Forward pass
        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
        #è¿™é‡Œç”¨åˆ°çš„ self.compiled_loss() å°±æ˜¯ Model.compile(loss) ä¼ å…¥çš„loss
        #å³ Model.compile() ä½œç”¨æ˜¯è®© self.compiled_loss = loss
                       
	# Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)

    del tape

    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    
    #æŒ‡æ ‡è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    #....
```



å¯ä»¥çœ‹åˆ° **Model.fit()**åªæ˜¯ä¸ºæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªä¸€èˆ¬æ™®é€šçš„è®­ç»ƒæ­¥éª¤å‡½æ•°ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šã€‚

**Model.fit()** ä¼šåœ¨ train_step() ä¸­è°ƒç”¨ Model.compile() ä¼ å…¥çš„lossã€ä¼˜åŒ–å™¨ã€æŒ‡æ ‡ã€‚

```python
loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
```

ä¸Šè¿°ä»£ç ä¸­compiled_loss()é™å®šäº†å‚æ•° y , y_predã€‚yæ˜¯æ ‡ç­¾å€¼ï¼Œy_predæ˜¯é€šè¿‡æ¨¡å‹é¢„æµ‹çš„å€¼ã€‚





è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®˜æ–¹æ–‡æ¡£ã€ç½‘ä¸Šçš„å¸–å­è¯´ï¼š

**å¦‚æœæƒ³è‡ªå®šä¹‰lossï¼Œé‚£ä¹ˆä½ è‡ªå®šä¹‰çš„losså‡½æ•°ä¸€å®šè¦æœ‰ä¸¤ä¸ªè¾“å…¥å‚æ•°ï¼ˆyï¼Œy_pred)ã€‚**

**å› ä¸ºåœ¨ fit() â€”â€”ã€‹train_step()ä¸­ï¼Œè°ƒç”¨äº†self.compiled_loss()ï¼Œå¹¶è§„å®šäº†å®ƒçš„å‚å…¥å‚æ•°ä¸ºï¼ˆy,y_pred)**ã€‚





å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡è½½ train_step() ä½¿è‡ªå®šä¹‰losså‡½æ•°è‡ªç”±åº¦æ›´é«˜ï¼Œå¹¶ä»èƒ½ä½¿ç”¨fit()ã€‚

ç”šè‡³é‡è½½fit()ï¼Œä½†æ—¢ç„¶éƒ½é‡è½½fit()äº†ï¼Œå®ƒç­‰ä»·äºä»0ç¼–å†™è®­ç»ƒè¿‡ç¨‹ã€‚â€”â€”ä¸è¿‡ï¼Œè¿™æ­£æ˜¯PINNè®­ç»ƒéœ€è¦çš„ã€‚

**å› ä¸ºæˆ‘ä»¬æœ‰ä¸åŒæ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆå¸¦æ ‡ç­¾å’Œä¸å¸¦æ ‡ç­¾ï¼‰ã€å¤æ‚çš„losså‡½æ•°ï¼ˆä¸ä»…ä»…éœ€è¦yï¼Œy_predï¼Œè¿˜éœ€è¦y_predå¯¹xï¼Œtçš„å¯¼æ•°ï¼‰ï¼Œæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»ç„¶ä½¿ç”¨fitï¼ˆï¼‰ä»£ç æ¡†æ¶ï¼Œéœ€è¦å¤§åˆ€é˜”æ–§åœ°ä¿®æ”¹ã€‚è¿˜ä¸å¦‚è‡ªå·±å†™è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å»ä½¿ç”¨æ‰€è°“çš„é«˜é˜¶API~~**



### <font color='blue'>2.å­ç±»åŒ–Sequential() / Model() , å®šä¹‰ MyPinnï¼Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹</font>

ä¹‹å‰è®¨è®ºè¿‡**æ­¥éª¤1ï¼šä½¿ç”¨Sequentialæ­å»ºæ¨¡å‹** æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œæˆ‘ä»¬éœ€è¦åšçš„ä»…ä»…æ˜¯é‡æ–°å®šä¹‰**è®­ç»ƒè¿‡ç¨‹**ã€‚

é‚£ä¹ˆï¼Œå­ç±»åŒ–**class MyPinn(tf.keras.Sequential())**ï¼ŒMyPinnä¿ç•™Sequential()å¥½ç”¨çš„åŠŸèƒ½ï¼Œå†é™„åŠ ä¸Šæˆ‘ä»¬è‡ªå®šä¹‰çš„**è®­ç»ƒè¿‡ç¨‹**

```python
class MyPinn(keras.Sequential): ## ä»¥Burgers_Equationä¸ºä¾‹
    def __init__(self,name = None):
        
        super(MyPinn, self).__init__(name=name)
        self.nu = tf.constant(0.01/np.pi)
    
    @tf.function
    def test_gradient(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape() as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)
        u_x = tape.gradient(u,x)
        tf.print(u_x)
    
    @tf.function
    def loss_U(self,X_u_train,u_train):
        u_pred = self(X_u_train)
        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))
        return loss_u
    
    
    @tf.function
    def loss_PDE(self,X_f_train):
        x = X_f_train[:,0]
        t = X_f_train[:,1]
        with tf.GradientTape(persistent=True) as tape:
            tape.watch([x,t])
            X = tf.stack([x,t],axis=-1)
            u = self(X)  
            u_x = tape.gradient(u,x)         
            
        u_t = tape.gradient(u, t)     
        u_xx = tape.gradient(u_x, x)
        
        del tape
        
        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx

        loss_f = tf.reduce_mean(tf.square(f))

        return loss_f
    
    
    def loss_Total(self,X_u_train,u_train,X_f_train):
        loss_u = self.loss_U(X_u_train,u_train)
        loss_f = self.loss_PDE(X_f_train)
        
        loss_total = loss_u + loss_f
        
        return loss_total
    
    @tf.function
    def train_step(self,X_u_train,u_train,X_f_train):
        with tf.GradientTape(persistent=True) as tape:
            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)
                   
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss_total, trainable_vars)
        
        del tape
        
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        return loss_total
    
    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):
        for epoch in tf.range(1,epochs+1):
            loss_total = self.train_step(X_u_train,u_train,X_f_train)
            if epoch % 10 == 0:                
                print(
                    "Training loss (for per 10 epoches) at epoch %d: %.4f"
                    % (epoch, float(loss_total))
                )
```



<font color='purple'> **åœ¨å½“å‰æ–‡ä»¶å¤¹ myPINN.py è¿›è¡Œäº†train_modelæµ‹è¯•ï¼ŒæˆåŠŸè¿è¡Œ**</font>

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 6-30 markdown

---

# 07-01

### <font color='blue'>ä¼˜åŒ–å™¨</font>

tf.keras.optimizersä¸ºæˆ‘ä»¬æä¾›äº†è®¸å¤šç°æˆçš„ä¼˜åŒ–å™¨ï¼Œæ¯”å¦‚SGDï¼ˆæœ€é€Ÿä¸‹é™ï¼‰ã€Adamã€RMSpropç­‰ç­‰ã€‚



å‡è®¾ï¼Œç°æœ‰æ¨¡å‹å¯¹è±¡ MyPinnã€‚

å¯ä»¥é€šè¿‡ tf.keras.optimizers.Optimizer() åˆ›å»ºä¼˜åŒ–å™¨å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œ MyPinn.optimizer = tf.keras.optimizers.SGD()

tf.keras.optimizers.Optimizer()ä¸»è¦æä¾›äº†ä¸¤ç§Methodsï¼Œä¸ºæˆ‘ä»¬çš„å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚

1. **apply_gradients(**
       **grads_and_vars, name=None, experimental_aggregate_gradients=True**
   **)**

â€‹	ä¹‹å‰å®šä¹‰çš„MyPinn.train_step()ä¸­å°±ä½¿ç”¨äº†è¿™ç§Methodã€‚

â€‹	æˆ‘ä»¬å…ˆè®¡ç®—å‡ºgradsï¼Œå†ä½¿ç”¨apply_gradient()ï¼Œè¿›è¡Œå‚æ•°ä¼˜åŒ–ã€‚

2. **minimize(**
       **loss, var_list, grad_loss=None, name=None, tape=None**
   **)**

   minimize()æ–¹æ³•å…ˆä½¿ç”¨tf.GradientTape()è®¡ç®—å‡ºlossï¼Œå†è°ƒç”¨apply_gradients()ã€‚ç›¸å½“äºæŠŠcompute gradientså’Œapply gradients å°è£…åœ¨ä¸€èµ·ã€‚

å¯ä»¥å‘ç°ï¼Œapply_gradients()å°±æ˜¯minimize()ä¸­çš„ç¬¬äºŒæ­¥ã€‚

**ä¸ºäº†ç²¾å‡†åœ°æ§åˆ¶ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ ä¸Šä¸€äº›åˆ«çš„æ“ä½œï¼Œæˆ‘ä»¬ä½¿ç”¨ ç¬¬1ç§æ–¹æ³• å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚**



### <font color='blue'>  Adam & L-BFGS </font>

Adamä¼˜åŒ–å™¨åœ¨deep neural networkä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¹‹å‰ä¹Ÿè¯´è¿‡ï¼Œtf.keras.optimizersé‡Œå†…ç½®äº†Adamä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨å°±å¥½ã€‚

åœ¨PINNåŸä½œè€…çš„ä»£ç ä¸­(tensorflow1.x)ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸¤ç§ä¼˜åŒ–å™¨ ï¼š Adam & L-BFGSã€‚

åœ¨ training model è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬å…ˆä½¿ç”¨ Adam è¿›è¡Œä¼˜åŒ–ï¼Œåä½¿ç”¨ L- BFGS è¿›è¡Œä¼˜åŒ–ã€‚



L-BFGS æ˜¯ ç§©2çš„æ‹Ÿç‰›é¡¿æ–¹æ³•(è¿™å­¦æœŸ â€œæœ€ä¼˜åŒ–æ–¹æ³•â€ è¯¾ä¸Šåˆšå¥½å­¦è¿‡)ï¼Œå®ƒæ˜¯åŸºæœ¬ç‰›é¡¿æ–¹æ³•çš„ä¸€ç§å˜å½¢ã€‚ç‰›é¡¿æ–¹æ³•åœ¨æå€¼ç‚¹é™„è¿‘æ—¶ï¼Œæ”¶æ•›é€Ÿåº¦å¿«ï¼Œè€Œæ‹Ÿç‰›é¡¿æ–¹æ³•åœ¨ä¿æŒè¿™ä¸ªä¼˜ç§€æ€§è´¨çš„åŸºç¡€ä¸Šï¼Œæ”¹è¿›äº†ç‰›é¡¿æ–¹æ³•è¿‡ç¨‹çš„ä¸€äº›ç¼ºç‚¹ï¼Œæ¯”å¦‚è®¡ç®—äºŒé˜¶å¯¼ã€çŸ©é˜µæ±‚é€†å’ŒGä¸æ­£å®šç­‰é—®é¢˜ã€‚





ç„¶è€Œåœ¨TensorFlow1.xä¸­ å¹¶æ²¡æœ‰å†…ç½®çš„ L-BFGSï¼Œä½œè€…å®é™…æ˜¯ä½¿ç”¨tensoflow1.x æä¾›çš„ä¸€ä¸ªæ¥å£ï¼Œä½¿ç”¨ Scipy åº“ä¸­çš„ L-BFGSã€‚

Scipyä¸­è°ƒç”¨L-BFGSçš„æ ¼å¼æ˜¯ï¼š

```python
scipy.optimize.minimize(fun, 
				x0, args=(),
                method='L-BFGS-B', 
                jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)
```

 å…¶ä¸­funæ˜¯ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œè¿”å›ç›®æ ‡å‡½æ•°å€¼ã€‚

> ```
> fun(x, *args) -> float
> ```

where `x` **is a 1-D array with shape (n,)** and `args` is a tuple of the fixed parameters needed to completely specify the function.

> å½“ jac = True æ—¶ï¼Œ fun()  retrun fval , gradients



è¿™æ—¶ï¼Œå†çœ‹ä¸€ä¸‹ä½œè€…è°ƒç”¨L-BFGSçš„ä»£ç ï¼Œå°±çŸ¥é“æ˜¯ä»€ä¹ˆæ„æ€äº†ã€‚ã€‚

```python
self.optimizer =tf.contrib.opt.ScipyOptimizerInterface (self.loss, 
                                                        method = 'L-BFGS-B', 
                                                        options = {'maxiter': 3000,
                                                                   'maxfun': 3000,
                                                                   'maxcor': 50,
                                                                   'maxls': 50,
                                                                   'ftol' : 1.0 * np.finfo(float).eps})
```

è¿™æ—¶å¦‚æœæˆ‘ä»¬å†™ self.optimizer.minimize() å®é™…ä¸Šå°±ä¼šè°ƒç”¨ scipy.optimize.minimize( args ) ï¼Œargs=ä¸Šè¿°ä»£ç ä¸­ä¼ å…¥çš„å‚æ•°ï¼Œself.loss ç›¸å½“äº funã€‚



é—æ†¾çš„æ˜¯ï¼Œåœ¨TensorFlow2.xä¸­ï¼Œè¯¥æ¥å£å·²ç»åˆ é™¤ã€‚

å½“ç„¶æˆ‘ä»¬ä»èƒ½æƒ³åŠæ³•ä½¿ç”¨Scipyä¸­çš„L-BFGSï¼Œ

æ— éå°±æ˜¯æŒ‰ç…§scipy.optimize.miminze()çš„è°ƒç”¨æ ¼å¼ï¼Œåœ¨MyPinnå†…éƒ¨å®šä¹‰ä¸€ä¸ªloss_fun(x) ï¼Œx is 1-D array with shape = (n,)ï¼Œ

ä½œä¸ºscipy.optimize.miminze(fun,...)ä¸­çš„funï¼Œä½†è¿™æ„å‘³ç€éœ€è¦æŠŠMyPinnçš„weightså’Œbias "æ‰å¹³åŒ–" æ”¾åœ¨ä¸€ä¸ª1ç»´æ•°ç»„ä¸­ï¼Œåœ¨ä¼˜åŒ–å®Œæ¯•åï¼Œè¿˜è¦æŠŠç»“æœå†å˜æˆåŸæ¥çš„å½¢çŠ¶ï¼Œæ”¾å›MyPinné‡Œã€‚



åˆä½†æ˜¯ï¼Œè™½ç„¶æ¥å£æ²¡äº†ï¼Œä½†TensorFlow2.0ä¸­ tfp åº“ä¸­æœ‰å®ç° L-BFGS ç®—æ³•ã€‚ğŸ˜



**ä¸‹é¢é“¾æ¥ä¸­ï¼Œé«˜èµå›ç­”è®¨è®ºäº†åœ¨TensorFlow2.xä¸­ä½¿ç”¨ Scipyçš„L-BFGS å’Œ è‡ªå¸¦çš„L-BFGS è®¡ç®—å·®åˆ«ã€‚**

[python - Use Scipy Optimizer with Tensorflow 2.0 for Neural Network training - Stack Overflow](https://stackoverflow.com/questions/59029854/use-scipy-optimizer-with-tensorflow-2-0-for-neural-network-training)

<img src="./Data/L-BFGS in scipy and tfp.png" />



**å¯ä»¥å‘ç°ä½¿ç”¨TensorFlow2.0 tfpä¸­çš„L-BFGSè®¡ç®—é€Ÿåº¦æ›´å¿«**

**ä¸è¿‡tfpä¸­çš„L-BFGSè®¡ç®—ç»“æœç•¥é€ŠäºScipyä¸­çš„L-BFGSï¼Œå¯èƒ½æ˜¯TensorFlowé»˜è®¤float32ï¼Œè€ŒScipyæ˜¯float64ï¼Œä»¥åŠScipyä¸­L-BFGSç®—æ³•çš„å®ç°æ¯”tfpçš„æ›´å¥½ã€‚**



[Optimize TensorFlow & Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

æ³¨æ„å¦‚æœæƒ³ä½¿ç”¨tfpçš„L-BFGSä¹Ÿæ˜¯è¦æ±‚è¾“å…¥å˜é‡æ˜¯1-Dçš„ã€‚è€Œæˆ‘ä»¬çš„PiNNæ¨¡å‹ä¸­çš„weightså’Œbiaséƒ½æ˜¯ä»¥å¤šç»´çš„å½¢å¼ä¿å­˜ï¼Œæ‰€ä»¥è¦å…ˆå°†å®ƒä»¬è¿›è¡Œâ€œæ‰å¹³åŒ–â€ï¼Œå†ä¼ å…¥L-BFGSå‡½æ•°ä¸­ã€‚

ä¸Šé¢çš„é“¾æ¥è®¨è®ºäº†å¦‚ä½•å®šä¹‰å°†modelä¸­çš„å˜é‡â€œæ‰å¹³åŒ–â€ï¼Œå˜å›æ¥ ä»¥åŠ å¦‚ä½•å®šä¹‰â€œfunction_factory"è¿”å›ä¸€ä¸ªLBFGSéœ€è¦çš„functionã€‚



æˆ‘ä»¬çœ‹ä¸‹tfpä¸­L-BFGSçš„è°ƒç”¨æ ¼å¼ï¼š

```python
tfp.optimizer.lbfgs_minimize(
    value_and_gradients_function,
    initial_position,
    num_correction_pairs=10,
    tolerance=1e-08,
    x_tolerance=0,
    f_relative_tolerance=0,
    initial_inverse_hessian_estimate=None,
    max_iterations=50,
    parallel_iterations=1,
    stopping_condition=None,
    name=None
)
# value_and_gradients_function æ˜¯ä¸€ä¸ªå‡½æ•°, 
# Input: paramters with shape = 1-D ; Output: loss and gradients with paramters, gradients are also 1-D.

# initial_position: initial paramters 
```

<font color='purple' >**å¯¹MyPinnæ¨¡å‹(keras.Sequentialæ¨¡å‹)ä½¿ç”¨tfp L-BFGS()è¿›è¡Œå‚æ•°ä¼˜åŒ–æµç¨‹å¦‚ä¸‹ï¼š** </font>

1. æå–MyPinnä¸­çš„weights å’Œ bias (å³éœ€è¦ä¼˜åŒ–çš„parameters)ï¼Œæ­¤æ—¶å®ƒä»¬å°±æ˜¯initial_position(æœªæ‰å¹³åŒ–)ã€‚

2. åˆ›å»ºä¸¤ä¸ªåˆ—è¡¨ idx=[],part=[]

3. æŠŠMyPinnæ¯å±‚å‚æ•°çš„shapeç­‰è‹¥å¹²ä¿¡æ¯ï¼Œç”¨å¾ªç¯appendåˆ°idxå’Œpartã€‚

   idxå¸®åŠ©æˆ‘ä»¬è°ƒç”¨ tf.dynamic_stitch()å°†weightså’Œbias"æ‰å¹³åŒ–"æˆparams_1dã€‚

   partå¸®åŠ©æˆ‘ä»¬params_1då˜å›weightså’Œbias,å¹¶æ›´æ–°MyPinnä¸­çš„å‚æ•°ã€‚

4. å®šä¹‰ä¸€ä¸ªfuncå‡½æ•°,func(params_1d)

â€‹		Input:  params_1d

â€‹		Output: loss , gradients

â€‹		Insideï¼šå…ˆæŠŠ params_1d è½¬å˜å› MyPinn ä¸­ weights,bias çš„shapeï¼Œå¹¶æ›´æ–°å®ƒä»¬ã€‚

â€‹					  ä½¿ç”¨MyPinnä¸­å·²å®šä¹‰çš„loss_Total()æ–¹æ³•è®¡ç®—loss å’Œ gradientsã€‚

â€‹				      æ³¨æ„ï¼šéœ€è¦å°† gradients ä¹Ÿæ‰å¹³åŒ– å†returnã€‚( gradients.shape = [weights,bias].shape,æ•…ä¹Ÿå¯ä»¥ç”¨idxæ‰å¹³åŒ– )

5.  å°†ç¬¬ä¸€æ­¥æå–å‡ºæ¥çš„weights å’Œ bias æ‰å¹³åŒ–å¤„ç†ï¼Œä½œä¸ºinitial_position
6. è°ƒç”¨tfp.optimizer.lbfgs_minimize(func,initial_position)å³å¯ï¼

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-1 markdown

---



# 07-04

â€‹	ä»Šå¤©ä¸»è¦æ·»åŠ äº† Data Preparation å’Œ Plot çš„ä»£ç ã€‚å¹¶ä¸”ï¼ŒæŒ‰æ—¥æœŸå‘½åï¼Œå°†ä»£ç åˆ†å¼€åœ¨ä¸åŒçš„NoteBookã€‚

â€‹    ä½¿ç”¨ä¸‹é¢é“¾æ¥ä¸­çš„è®­ç»ƒæ•°æ®ã€‚

â€‹	[Optimize TensorFlow & Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

â€‹	ä¸è¿‡ï¼Œæˆ‘åœ¨ä½¿ç”¨MyPinnè®­ç»ƒBurgers Equationï¼Œè®­ç»ƒç»“æœä¸å¤ªç†æƒ³ã€‚ä¸€å¼€å§‹ï¼Œæˆ‘æ€€ç–‘æ˜¯ï¼Œfloat32æ ¼å¼ **And** tfpä¸­lbfgsä¸Scipyä¸­lbfgsçš„å·®åˆ«ã€‚debugäº†å¾ˆä¹…ï¼Œå‘ç°ä¸æ˜¯è¿™äº›åŸå› ã€‚

â€‹	å› ä¸ºä¸Šé¢çš„é“¾æ¥ä¸­ï¼Œä½œè€…ä¹Ÿæœ‰ç”¨tfpä¸­çš„lbfgsè®­ç»ƒæ¨¡å‹ï¼Œæˆ‘è¿è¡Œäº†ä¸€éï¼Œä»ç„¶å¯ä»¥è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœï¼Œçœ‹æ¥è¿˜éœ€è¦debug(æ¼ã€‚



â€‹	é¢˜å¤–è¯ï¼Œä½¿ç”¨Google colabå¯ä»¥ç™½å«–ç®—åŠ›ï¼Œå°†.ipynbæ–‡ä»¶ä¸Šä¼ ï¼Œå¯ä»¥åœ¨äº‘ç«¯è®¡ç®—ï¼Œè¿˜å…è´¹ï¼Œè€Œä¸”æˆ‘çš„ç”µè„‘å†…å­˜æœ‰æ—¶å€™ä¸å¤ªå¤Ÿç”¨ï¼Œæ‰€ä»¥colabå°±å¾ˆniceã€‚



> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-4 markdown

---



# 07-05 Debug

ä»Šå¤©èŠ±äº†ä¸å°‘æ—¶é—´ï¼Œæ€»ç®—è®©æˆ‘å‘ç°äº†è¿™ä¸ªæ‰€è°“çš„â€œbugâ€ã€‚**â€œæ‰¾bugæ—¶é—´ï¼Œæ¯”å†™ä»£ç æ—¶é—´è¦é•¿â€è¿™æ¬¡çœŸçš„å°è¯äº†è¿™å¥è¯å§ã€‚**

è¿‡ç¨‹ä¸­ï¼Œæˆ‘ç”šè‡³ä¸€åº¦æƒ³è¦æ”¾å¼ƒè‡ªå·±çš„MyPinnæ¨¡å‹ï¼Œä½¿ç”¨åˆ«äººçš„æ„å»ºæ¨¡å‹ä»£ç ï¼ˆæ¯”å¦‚ä¸Šé¢çš„é“¾æ¥ï¼‰ ã€‚

**åœ¨è§£é‡Šbugä¹‹å‰**ï¼Œæˆ‘éœ€è¦è¯´æ˜ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆæˆ‘è‡ªå·±è¦ç”¨æ„å»ºä¸€å¥—TensorFlow2.xçš„Pinn classï¼Œè€Œä¸æ˜¯ç”¨åˆ«äººçš„ã€‚

[Optimize TensorFlow & Keras models with L-BFGS from TensorFlow Probability | import pyChao](https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/)

**ä¸ºæ–¹ä¾¿è¯´æ˜ï¼Œæˆ‘å°†ä¸Šè¿°é“¾æ¥ä¸­çš„pinnä»£ç ç§°ä¸º code**ã€‚

1. TensorFlow2.xé»˜è®¤æ˜¯eageråŠ¨æ€å›¾æ¨¡å¼ï¼Œè¿™ç§æ¨¡å¼è®¡ç®—é€Ÿåº¦æ¯”è¾ƒæ…¢ï¼Œä½†æ–¹ä¾¿è°ƒè¯•ã€‚ä½¿ç”¨@tf.functionå‡½æ•°è£…é¥°å™¨ï¼Œèƒ½å°† eagerè½¬ä¸º**AutoGraphåŠ¨æ€å›¾æ¨¡å¼**è®¡ç®—ï¼Œæ•ˆç‡å ªæ¯”TensorFlow1.xçš„é™æ€å›¾ï¼ˆtf1.xåªæ”¯æŒé™æ€å›¾ï¼‰ï¼ŒåŒæ—¶ä¹Ÿæ–¹ä¾¿è°ƒè¯•ã€‚**ä½†æ˜¯å‘¢ï¼Œ@tf.function æœ‰ä¸€å®šçš„ç¼–ç¨‹è§„èŒƒã€‚**

   è€Œ **code**ï¼Œæ²¡æœ‰ä½¿ç”¨TensorFlow2.0çš„@tf.functionåŠŸèƒ½ã€‚

   è¿™æ˜¯åŸå› ä¸€ã€‚

2. **é«˜é˜¶API**ã€‚tf.keraså·²ç»ä¸ºæˆ‘ä»¬æä¾›äº†å„ç§å±‚ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰ ä»¥åŠ **2ç§ä¸»è¦æ„å»ºæ¨¡å‹çš„æ–¹å¼**ï¼šSequential()ï¼Œå‡½æ•°å¼APIã€‚çµæ´»ä½¿ç”¨å‡½æ•°å¼APIï¼Œç†è®ºä¸Šå¯ä»¥æ„å»ºä»»ä½•æ¨¡å‹ï¼Œä¸å•å•æ˜¯ç¥ç»ç½‘ç»œã€‚è€ŒPinnä½¿ç”¨Sequential()å³å¯ã€‚æ¯”å¦‚ï¼Œæˆ‘çš„ä»£ç å°±æ˜¯è®©MyPinnç»§æ‰¿keras.Sequential

   è€Œåœ¨**code**ä¸­ï¼ŒPinnç»§æ‰¿çš„æ˜¯keras.Moduleï¼ŒModuleå®é™…ä¸Šæ˜¯Sequentialç­‰é«˜é˜¶APIçš„åŸºç±»ï¼Œå®ƒæä¾›çš„åŠŸèƒ½æ˜¯è®°å½•åœ¨ç±»ä¸­å‡ºç°çš„Variablesï¼Œè€Œæ²¡æœ‰Sequentialä¸­æ›´å¤šå¥½ç”¨æ–¹ä¾¿çš„åŠŸèƒ½ï¼Œæ¯”å¦‚ç›´æ¥ä½¿ç”¨addæ–¹æ³•ï¼Œæ­é…keras.layers.Denseï¼Œä¸ºMyPinnæ·»åŠ å±‚(æœ¬è´¨ä¸Šä¹Ÿæ˜¯æ·»åŠ Variables)ã€‚

   è‹¥ä½¿ç”¨**code**ä¸­çš„æ–¹æ³•ï¼Œè®©Pinnå•å•ç»§æ‰¿Moduleï¼Œåˆ™éœ€è¦è‡ªå·±åšå˜é‡åˆå§‹åŒ–ï¼Œå¹¶ä¸”æ²¡æœ‰åˆ«çš„å¥½ç”¨çš„åŠŸèƒ½ã€‚å®ƒåªèƒ½è®°å½•classä¸­å‡ºç°çš„å˜é‡è€Œå·²ã€‚

   è¿™æ˜¯åŸå› äºŒã€‚

3. **ä¼˜åŒ–å™¨**ã€‚åœ¨codeä¸­ä¸ºäº†ä½¿ç”¨LBFGSï¼Œåœ¨ç±»ä¸­å®šä¹‰äº† â€œå˜é‡ä¸€ç»´åŒ–ã€ä¸€ç»´å˜é‡è½¬ä¸ºåŸæ¥å½¢çŠ¶â€ç­‰ ä¸“é—¨ä¸ºä½¿ç”¨LBFGSçš„å‡½æ•°ã€‚æˆ‘ä¸å–œæ¬¢è¿™æ ·å­ã€‚

   **ä¸ºäº†æ¸…æ™°æ€§ï¼Œæˆ‘æ›´å€¾å‘äºå°†æ¨¡å‹å®šä¹‰å’Œä¼˜åŒ–æ–¹æ³•åˆ†ç¦»å¼€ï¼Œè€Œä¸æ˜¯å…¨åœ¨å†™åœ¨classä¸­ã€‚**

    â€œå˜é‡ä¸€ç»´åŒ–ã€ä¸€ç»´å˜é‡è½¬ä¸ºåŸæ¥å½¢çŠ¶â€è¿™æ ·çš„å‡½æ•°ï¼Œåªæ˜¯ä¸ºä½¿ç”¨LBFGSæœåŠ¡çš„ï¼Œä»–åº”è¯¥å®šä¹‰åœ¨å¤–é¢ã€‚å¦‚æœä½¿ç”¨å†…ç½®çš„Adamã€SDGç­‰ä¼˜åŒ–å™¨ï¼Œå®Œå…¨ä¸éœ€è¦å®šä¹‰è¿™äº›å‡½æ•°ï¼Œç›´æ¥åœ¨train_step()ä¸­ï¼Œoptimizer.apply_gradientï¼ˆï¼‰å³å¯ã€‚

   è¿™æ˜¯åŸå› ä¸‰ã€‚



æ‰€ä»¥bugå‡ºç°åœ¨å“ªå‘¢ï¼Ÿ@tf.function ç¼–ç¨‹ä¸è§„èŒƒï¼Ÿ å˜é‡åå†™é”™ï¼Ÿ

**åœ¨loss_PDEçš„å®šä¹‰ä¸­ã€‚**



ä¸‹é¢æˆ‘åˆ†åˆ«ç»™å‡ºæˆ‘è‡ªå·±çš„å®šä¹‰ä»¥åŠcodeçš„å®šä¹‰

```python
def loss_PDE(self,X_f_train): #æˆ‘çš„å®šä¹‰
    
      x = X_f_train[:,0]  # x.shape =(nums,)
      t = X_f_train[:,1]  # t.shape =(nums,)
      with tf.GradientTape(persistent=True) as tape:
          tape.watch([x,t])
          X = tf.stack([x,t],axis=-1) # X.shape = (nums,2)
          u = self(X)  # predict u
          u_x = tape.gradient(u,x)
```

```python
def loss_PDE(self,X_f_train): #codeçš„å®šä¹‰
      
      x = X_f_train[:,0:1] # x.shape = (nums,2)
      t = X_f_train[:,1:2] # t.shape = (nums,2)
      with tf.GradientTape(persistent=True) as tape:
          tape.watch([x,t])
          X = tf.stack([x[:,0],t[:,0]],axis=1) #X.shape = (nums,2)
          u = self(X)  # predict u
          u_x = tape.gradient(u,x)
```

ä¸¤ç§ä»£ç çš„ç¬¬3è¡Œå’Œç¬¬4è¡Œï¼Œå¹²çš„éƒ½æ˜¯åŒä¸€ä»¶äº‹ï¼ŒæŠŠX_f_trainçš„ x å’Œ t åˆ†ç¦»å‡ºæ¥ï¼ˆåé¢è¦å¯¹xï¼Œtæ±‚å¯¼ï¼‰ã€‚

```python
with tf.GradientTape(persistent=True) as tape:
	....
```

ä¸Šè¿°å°†è®¡ç®—è¿‡ç¨‹è®°å½•åœ¨â€œtapeâ€ä¸­ï¼Œåé¢å¯ä»¥æ±‚tapeä¸­å‡ºç°å˜é‡ï¼ˆåŒ…æ‹¬ä¸­é—´å˜é‡ï¼‰çš„å¯¼æ•°ã€‚

tape.watch([x,t])å°†xï¼Œtä¹Ÿè®°å½•åˆ°tapeä¸­ï¼Œå› ä¸ºé»˜è®¤åªè®°å½•variablesï¼Œè€Œæ­¤å¤„xå’Œfæ˜¯constantã€‚

æˆ‘ä»¬å°†x,té€šè¿‡tf.stack()æ‹¼æ¥åˆ°ä¸€èµ·ç»„æˆæ¨¡å‹çš„inputï¼šXï¼Œè°ƒç”¨self(X) predict uã€‚tf.stackä¹Ÿä¼šè¢«çœ‹åšä¸€ç§ç®—å­ï¼Œè¢«è®°å½•åˆ°tapeä¸­ã€‚

ä½ æˆ–è®¸å‘ç°Xå’Œloss_PDEå‡½æ•°ä¼ å…¥çš„X_f_train**æ•°å€¼ä¸Šä¸€æ ·**ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥self(X_f_trian)å‘¢ï¼Ÿ**å› ä¸ºå®ƒä»…ä»…æ˜¯æ•°å€¼ä¸Šä¸€æ ·ã€‚ã€‚ã€‚**

å¯ä»¥è¯•ä¸€ä¸‹æ”¹ç”¨u=self(X_f_train)ï¼Œé‚£ä¹ˆu_xè¿”å›çš„Noneã€‚



ä»¤äººæˆ‘æ„Ÿåˆ°å¥‡æ€ªçš„æ˜¯ï¼Œä¸¤ç§ä»£ç çš„ç¬¬7æ­¥ï¼Œæœ€åè¿”å›çš„Xæ˜¯ä¸€æ ·çš„ï¼Œä¸ºä»€ä¹ˆåªæœ‰ç¬¬äºŒç§ï¼ˆcodeçš„å®ç°ï¼‰OKï¼Ÿè€Œæˆ‘çš„å°±ä¸è¡Œã€‚

å…¶å®æˆ‘åœ¨å†™ä»£ç çš„æ—¶å€™ï¼Œå‚è€ƒäº†codeçš„loss_PDEï¼Œè§‰å¾—ä»–çš„æ“ä½œæœ‰ç‚¹**å¤šä½™**ï¼Œå°±ç”¨äº†ä¸€ç§æ›´æ¸…æ¥šçš„å†™æ³•ï¼Œæ­£å¦‚ç¬¬ä¸€ç§å†™æ³•é‚£æ ·ï¼Œç„¶è€Œè®­ç»ƒçš„ç»“æœå°±æ˜¯ï¼Œç¬¬ä¸€ç§ä¸è¡Œï¼ï¼Ÿ



**ä¸‹é¢æˆ‘å†éªŒè¯ä¸€ä¸‹ä¸¤ç§å†™æ³•çš„(ç¬¬7è¡Œ)Xæ˜¯å¦ä¸€è‡´?** 

<img src="./Data/bug.png" style="zoom:75%;" />

ç»“æœæ˜¾ç¤º g å’Œ gg ï¼Œç¡®å®æ˜¯ä¸€æ ·çš„ï¼ˆå…¨æ˜¯Trueï¼Œæ²¡æœ‰æˆªå®Œå…¨)ï¼Œä»¤äººç–‘æƒ‘ã€‚

**ä¸ç®¡äº†ã€‚åé¢æˆ‘ä½¿ç”¨äº†codeçš„å†™æ³•.æ¨¡å‹çš„losså‡½æ•°èƒ½ä¸æ–­ä¸‹é™äº†ï¼Œæ•ˆæœè§7_4-7_5MyPINN_Burgers.ipynb**

> ä»¥ä¸Šå†…å®¹æˆªæ­¢è‡³ 7-5 markdown

---

# 07-06

ä»Šå¤©é˜…è¯»å­¦ä¹ äº†ä¸€äº›NNåœ¨PDEæ±‚è§£é—®é¢˜ä¸­çš„å¤šç§æ–¹æ³•ã€‚

### ç¥ç»ç½‘ç»œå¸¸è§ç»“æ„

<img src='./Data/NN_structures.png' style="zoom:50%;"  >



**ç›®å‰PINNå¤§å¤šä¸ºå…¨è¿æ¥å‰å‘ç½‘ç»œ(tensorflowä¸­çš„Denseå±‚)ï¼Œå°è¯•ä½¿ç”¨å¦‚å·ç§¯å±‚ã€å¾ªç¯ç½‘ç»œã€åé¦ˆç½‘ç»œï¼Ÿ**



### ç¥ç»ç½‘ç»œå¯¹è¾¹ç•Œæ¡ä»¶çš„å¤„ç†æ–¹å¼

ANN: artificial neural network

1. **çº¯ç½‘ç»œå‹**
1. 1  Loss = $\lambda_1 * MSE_{pde} + \lambda_2 * MSE_{BC}$  ï¼Œ åŸå§‹çš„PINNæ¨¡å‹å°±ä½¿ç”¨çš„æ˜¯è¿™ç§æ–¹å¼ï¼Œåˆ©ç”¨è¾¹ç•Œæ¡ä»¶å’Œé¢„æµ‹å€¼è®¡ç®—$MSE_{BC}$,çº³å…¥lossä¸­ã€‚
   
1. 2  å°†è¾¹ç•Œæ¡ä»¶å¸¦å…¥ç¥ç»ç½‘ç»œè¡¨è¾¾å¼â€”â€”å–ä»£éƒ¨åˆ†weightsï¼Œé€šè¿‡å¾®åˆ†æ–¹ç¨‹æ®‹å·®$MSE_{pde}$ä¼˜åŒ–å‰©ä½™æƒå€¼ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src='./Data/BC_solution1.png' style="zoom:70%;" >

â€‹			**ä½†æˆ‘è®¤ä¸ºï¼Œå¯¹äºæŸä¸€ä¸ªå¤æ‚pdeï¼Œä»£ç å®ç°è¿™ç§æ€æƒ³å¹¶ä¸å®¹æ˜“ã€‚**

2. **è¾¹ç•Œå¸æ”¶å‹**

â€‹	è¾¹ç•Œå¸æ”¶å‹çš„æ€æƒ³æ˜¯ï¼šæŠŠç¥ç»ç½‘ç»œçœ‹åšå‡½æ•° ANN(X)ï¼›æ„é€ è¾¹ç•Œå‡½æ•°BC(X)ï¼šå½“Xâˆˆè¾¹ç•Œæ—¶ï¼ŒBCä¸ºè¾¹ç•Œå€¼ï¼Œå¦åˆ™ä¸º0ï¼›æ„é€  L(X)ï¼Œå½“Xâˆˆè¾¹ç•Œæ—¶ï¼ŒL(X)=0.

ä»¤è¯•è§£  $y_t = BC(X) + L(X) * ANN(X) $, æ­¤å‡½æ•°ä¸¥æ ¼æ»¡è¶³è¾¹ç•Œæ¡ä»¶ã€‚å†é€šè¿‡åŸŸå†…ç‚¹è®¡ç®—$MSE_{pde}$ï¼Œæ›´æ–°ANNã€‚

â€‹	**BC(X),L(X)çš„æ„é€ æ–¹æ³•ï¼š**

â€‹	<img src='./Data/BC_ODE.png'>



<img src='./Data/BC_PDE.png'>

**åœ¨PDEä¸­å¦‚ä½•è§£é‡ŠANN(X)çš„ä½œç”¨ï¼Ÿ** é€šç”¨BC(X),L(X)çš„æ„é€ æ–¹æ³•ï¼Ÿ



è¿›ä¸€æ­¥ï¼Œä½¿ç”¨ç‹¬ç«‹ç½‘ç»œ$ANN_{BC}$ä»£æ›¿BC(X)ã€‚

<img src='./Data/Duo_NN.png' style="zoom:50%;" >

æœ‰äº†è¿™ç§ç‹¬ç«‹ç½‘ç»œçš„æ€æƒ³ï¼Œä¹Ÿæœ‰äººæå‡ºå¤šç½‘ç»œçš„æ¨¡å‹ã€‚å³ç”¨ç‹¬ç«‹çš„ç½‘ç»œåˆ†åˆ«é¢„æµ‹PDEä¸­çš„å„ç§åå¯¼ï¼Œæœ€åæŠŠlossåŠ åœ¨ä¸€èµ·ã€‚

---

# 07-07

ä»Šå¤©ä¸»è¦é˜…è¯»å­¦ä¹ äº†å¼ å­¦é•¿çš„è®ºæ–‡ï¼Œå…³äºä½¿ç”¨PINNæ±‚è§£Navier-Stokes/Darcyè€¦åˆæ¨¡å‹çš„æ­£åé—®é¢˜ã€‚

## æ­£é—®é¢˜ä¸åé—®é¢˜

æ­£é—®é¢˜ï¼š

å·²çŸ¥pdeæ–¹ç¨‹ç»„ï¼Œåˆè¾¹å€¼æ¡ä»¶ï¼Œæ±‚è§£æè§£æˆ–è€…æ•°å€¼è§£ã€‚

åé—®é¢˜ï¼š

å·²çŸ¥pdeæ–¹ç¨‹ç»„ï¼Œæˆ–ä¸çŸ¥é“åˆè¾¹å€¼æ¡ä»¶ï¼Œæˆ–ä¸çŸ¥é“æ¨¡å‹ä¸­æŸäº›å‚æ•°çš„å€¼ï¼Œå–è€Œä»£ä¹‹ï¼ŒçŸ¥é“åŒºåŸŸå†…éƒ¨çš„ä¸€äº›æ•°å€¼çœŸè§£ï¼Œåæ¨æ•´ä¸ªåŒºåŸŸçš„è§£æˆ–è€…æ¨¡å‹å‚æ•°ã€‚

---

## æ±‚è§£æ€è·¯

**å¯¹äºæ­£é—®é¢˜ï¼š**

æ„å»ºä¸¤ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œ$NN_1 å’Œ NN_2$ï¼Œä½†æ˜¯ä»–ä»¬çš„è¾“å…¥å±‚inputæ˜¯ç›¸åŒçš„ï¼Œ$NN_1$ç”¨äºæ±‚è§£Naiver-Stokesæ–¹ç¨‹$U_1$ï¼Œ$NN_2$ç”¨äºæ±‚è§£$U_2$,æ ¹æ®Navier-Stokes/Darcyè€¦åˆæ¨¡å‹ä¸­çš„pdeæ–¹ç¨‹å’Œåˆè¾¹å€¼æ¡ä»¶ï¼Œç”¨$U_1 å’Œ U_2$æ„é€ æ®‹å·®ï¼ŒåŠ åœ¨ä¸€èµ·ï¼Œç»„æˆlossã€‚è®­ç»ƒç¥ç»ç½‘ç»œã€‚

**å¯¹äºåé—®é¢˜ï¼š**

å‡è®¾ï¼Œåˆè¾¹å€¼æ¡ä»¶æœªçŸ¥ï¼Œä½†çŸ¥é“ä¸€äº›å†…éƒ¨åŒºåŸŸçš„æ•°å€¼è§£ã€‚åªéœ€è¦æŠŠlossä¸­çš„åˆè¾¹å€¼MSEå»æ‰ï¼Œæ”¹æˆå…³äºå†…éƒ¨åŒºåŸŸçš„æ•°å€¼å€¼çš„MSEã€‚å…¶å®åœ¨PINNä¸­ï¼Œåˆè¾¹å€¼å’Œå†…éƒ¨åŒºåŸŸçš„æ•°å€¼è§£ å¯ä»¥çœ‹åšæ˜¯åŒç­‰ä½ä½çš„ã€‚

å‡è®¾ï¼Œæ–¹ç¨‹ç»„ä¸­æŸäº›ç‰©ç†å‚æ•°å€¼æœªçŸ¥ã€‚åªéœ€è¦æŠŠå®ƒä»¬è®¾ç½®ä¸ºVariablesï¼Œçº³å…¥lossï¼Œä¸NNçš„å‚æ•°ä¸€èµ·è®­ç»ƒã€‚



ä¸å¾—ä¸è¯´ï¼ŒNative-stokes/Darcyè€¦åˆæ¨¡å‹çš„æ–¹ç¨‹ç»„è¿˜æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œå¼ å­¦é•¿çš„è®ºæ–‡ä¸­ï¼Œä½¿ç”¨2då…·æœ‰è§£æè§£çš„ç®—ä¾‹ï¼Œä½¿ç”¨æœ€åŸå§‹çš„PINNæ¨¡å‹ç»“æ„æ±‚è§£ï¼Œæ•ˆæœè¿˜ç®—ä¸é”™ã€‚

ä½†å½“é—®é¢˜å˜æˆ3dï¼Œlossæ›´åŠ å¤æ‚æ—¶ï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œç”±äºé«˜é˜¶å¯¼æ•°çš„å­˜åœ¨ï¼Œä½¿ç”¨è¿™ç§æœ€åŸå§‹çš„PINNæ¨¡å‹è§£Navier-Stokes/Darcyè€¦åˆæ¨¡å‹ï¼Œåº”è¯¥å¾ˆæ…¢ã€‚

ä»NNçš„æ¨¡å‹ç»“æ„å’Œè®­ç»ƒç­–ç•¥ä¸Šåšä¸€äº›è°ƒæ•´ï¼Œåº”è¯¥ç®—æ˜¯æ¯”è¾ƒè‡ªç„¶çš„ä¼˜åŒ–ç­–ç•¥ã€‚

---

# 07-08

##  	å°æ‰¹é‡è®­ç»ƒæ¨¡å¼å®éªŒ

â€‹	é˜…è¯»ä¸€ç¯‡è®ºæ–‡ï¼Œå…³äºpdeè€¦åˆæ¨¡å‹çš„æ•°å€¼æ±‚è§£æ–¹æ³•ã€‚

â€‹	PARTITIONED TIMESTEPPING FOR A PARABOLIC TWO DOMA.pdf

â€‹    è¯•äº†ä¸‹å°æ‰¹é‡è®­ç»ƒã€‚å°†å…¨éƒ¨è®­ç»ƒæ•°æ® **nç­‰åˆ†å**ï¼Œè¿›è¡Œå°æ‰¹é‡è®­ç»ƒã€‚åœ¨ç›¸åŒçš„epochsä¸‹ï¼Œå°æ‰¹é‡è®­ç»ƒæ•ˆæœæ¯”åŸæ¥å¥½ã€‚æ•ˆæœè§"7_8_myPINN_Burgers.ipynb"

---

# 07-11 

## PINNæ±‚è§£parabolicè€¦åˆpdeæ¨¡å‹

â€‹	ç”¨PINNæ±‚è§£æœ€ç®€å•çš„parabolicè€¦åˆpdeæ¨¡å‹â€”â€”PARTITIONED TIMESTEPPING FOR A PARABOLIC TWO DOMA.pdf

<img src='./Data/å…¬å¼0.png'>

<img src='./Data/å…¬å¼1.png'>



æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒæ¨¡å‹çš„ä»£ç å†™å¥½äº†ï¼Œç”»å›¾çš„è¿˜æ²¡å†™ã€‚è®­ç»ƒæ•°æ®çš„ç”Ÿæˆä»£ç ï¼Œå†™çš„æœ‰äº›å†—é•¿ï¼Œåé¢ä¼šä¼˜åŒ–ä¸€ä¸‹ã€‚

ä»£ç è§ **7_11_Parabolicè€¦åˆpdeæ¨¡å‹.ipynb**



**ç½‘ç»œç»“æ„:** 

ä¸¤ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œ$NN_1 , NN_2$ï¼Œåˆ†åˆ«ç”¨äºé¢„æµ‹$u_1,u_2$, æ„é€  $ loss = loss_{u1} + loss_{u2} + loss_{interface}$ï¼Œæ¯æ¬¡è®­ç»ƒåŒæ—¶è®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼Œè€¦åˆæ€§ä½“ç°åœ¨$loss_{interface}$.

<img src='./Data/coupleNN.png' style='zoom:50%'>

æ¯æ¬¡è®­ç»ƒæ¨¡å‹ï¼Œ$NN_1 å’Œ NN_2 $å„è‡ªä¼ å…¥ä¸€æ‰¹"ä¸åŒçš„" å†…éƒ¨ç‚¹è®­ç»ƒé›† å’Œ è¾¹ç•Œç‚¹è®­ç»ƒé›†(ä¸åŒ…æ‹¬interface)ã€‚ $NN_få’ŒNN_p$åœ¨interfaceå¤„çš„è®­ç»ƒé›†ã€‚

---

# 07-12

##  è®­ç»ƒparabolicè€¦åˆpdeçš„PINNæ¨¡å‹

â€‹	u1æ‹Ÿåˆçš„æ¯”è¾ƒå¥½ã€‚u2æ‹Ÿåˆæ•ˆæœå¾ˆå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾¹ç•Œå¤„ã€‚

â€‹	æ­£åœ¨ç ”ç©¶ï¼Œä¸çŸ¥æ˜¯ä»£ç æœ‰é”™ï¼Œè¿˜æ˜¯è¯´å› ä¸ºu2è¡¨è¾¾å¼æ¯”è¾ƒå¤æ‚ï¼Œæœ‰yçš„äºŒæ¬¡é¡¹ã€‚

â€‹	è®­ç»ƒä»£ç è§  **7_11_Parabolicè€¦åˆpdeæ¨¡å‹.ipynb**

---

# 07-13 

## **ä¸2ä½å­¦é•¿ä¼šè®®äº¤æµï¼Œè®¨è®ºPINN**

è§£å†³äº†ä¸å°‘ç–‘é—®ï¼ŒPINNåœ¨è¾¹ç•Œå¤„çš„æ‹Ÿåˆæ•ˆæœç¡®å®ä¸€èˆ¬ã€‚

<img src = './Data/ä¼šè®®.png'>

---

# 07-15â€”â€”07-17 

## **æ”¹è¿›parabolicè€¦åˆpdeçš„ä»£ç ã€‚**

æ”¹è¿›æ–¹æ¡ˆå¦‚ä¸‹ï¼š

1. ä¸åŒå­¦è®¨è®ºå‘ç°ï¼Œä¹‹å‰çš„é‡‡æ ·ç‚¹ä¸ºç­‰åˆ†ï¼Œä¸å¤Ÿâ€œéšæœºâ€ï¼Œä½¿ç”¨normalæˆ–è€…æ‹‰ä¸é«˜æ¬¡æ–¹é‡‡æ ·æ•ˆæœæ›´å¥½
2. è®­ç»ƒæ¬¡æ•°ä¸è¶³ï¼Œå¢åŠ è®­ç»ƒæ¬¡æ•°
3. å°†è¾¹ç•Œç‚¹ä¹ŸåŒæ—¶çº³å…¥å†…éƒ¨ç‚¹è®­ç»ƒï¼Œå¯¼è‡´åœ¨è¾ƒå°‘è®­ç»ƒæ¬¡æ•°ä¸‹ï¼Œè¾¹ç•Œå¤„æ•ˆæœä¸å¥½
4. è€¦åˆé˜¶æ®µè®­ç»ƒç»“æŸåï¼Œç»§ç»­å¯¹ä¸¤ä¸ªåŒºåŸŸåˆ†å¸ƒè¿›è¡Œå•ç‹¬çš„PINNè®­ç»ƒï¼Œä½¿å¾—ç²¾å‡†åº¦è¿›ä¸€æ­¥æå‡

æ•ˆæœè§ **7_15_æ”¹è¿›ç‰ˆParabolicè€¦åˆpde.ipynb**



---

# 07-18 

## 	ä»£ç  & è®ºæ–‡é˜…è¯»

â€‹	å­¦ä¹ TensorFlow2.0 Metricè¯„ä¼°å‡½æ•° ï¼Œä»£ç è§â€œ **tensorflowå­¦ä¹ è®°å½•/12_Metric.ipynb**â€

â€‹	é˜…è¯» [Deep Learning-An Introduction](../è®ºæ–‡èµ„æ–™/Deep Learning-An Introduction.pdf )ã€‚è¿™ç¯‡æ–‡ç« ä»æ•°å­¦è§’åº¦ï¼Œä»é›¶å¼€å§‹ä»‹ç»Deep Learningï¼Œæ˜¯ä¸€ç¯‡ä»‹ç»æ€§çš„æ–‡ç« ã€‚	

---

# 07-20

## Self-Adaptive-Weight c-PINN

**ä»Šå¤©å¯¹parabolicè€¦åˆpdeçš„PINNæ¨¡å‹è¿›ä¸€æ­¥æ”¹è¿›**ã€‚ä¸»è¦æœ‰å¦‚ä¸‹ä¸¤ä¸ªæ–¹é¢ï¼š

1. é¢„è®­ç»ƒæ¨¡å¼
2. è‡ªé€‚åº”losså‡½æ•°å› å­

**é¢„è®­ç»ƒæ¨¡å¼**ï¼šåœ¨è®­ç»ƒè€¦åˆæ¨¡å‹å‰ï¼Œå…ˆå„ç§å•åŒºåŸŸè¿›è¡ŒPINNè®­ç»ƒã€‚æœ‰åˆ©äºè€¦åˆæ¨¡å‹è¾¹ç•Œè®­ç»ƒï¼Œä»¥åŠè®­ç»ƒæ•ˆç‡ã€‚

**è‡ªé€‚åº”losså‡½æ•°å› å­ï¼š**
$$
Loss = \alpha_1 * loss_{u1} + \alpha_2 * loss_{u2} \\
loss_{u1} := loss_{u1}^{bc} + loss_{u1}^{f} + loss_{u1}^{i} \\
loss_{u2} := loss_{u2}^{bc} + loss_{u2}^{f} + loss_{u2}^{i} \\
$$
â€‹	å…¶ä¸­$\alpha_i$å°±æ˜¯è‡ªé€‚åº”å› å­ã€‚

â€‹	è€ƒè™‘åˆ°å®é™…è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œu1å’Œu2çš„losså¤§å°ä¸ä¸€æ ·ï¼Œ**â€ä¼˜å…ˆâ€œ**è®­ç»ƒlossè¾ƒå¤§çš„ä¸€æ–¹ï¼Œå³åœ¨$loss_{ui}$å‰ä¹˜ä¸Šä¸€ä¸ªè¾ƒå¤§çš„å› å­ï¼Œä½¿å…¶åœ¨æ•´ä¸ª**$Loss$**ä¸­å æ¯”æ›´å¤§ï¼Œä»è€Œè¾¾åˆ°ä¼˜å…ˆè®­ç»ƒçš„ç›®æ ‡ã€‚



### **ä»€ä¹ˆæ˜¯è‡ªé€‚åº”æƒé‡ Self-Adaptive-Weightï¼Ÿ**

â€‹	æŠŠ$\alpha_1,\alpha_2$ä¹Ÿçœ‹åšå˜é‡ã€‚åœ¨è®­ç»ƒæ¨¡å‹å‚æ•°çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ˜¯åŸºäºâ€œè´Ÿæ¢¯åº¦â€ã€‚

â€‹	å¦‚æœä½¿ç”¨**â€œæ­£æ¢¯åº¦â€**å»æ”¹å˜$\alpha_1,\alpha_2$ï¼Œèƒ½å¤Ÿä½¿å¾—$loss_{ui}$å¯¹åº”çš„$\alpha_{i}$æ›´å¤§ã€‚

â€‹	å®é™…ä¸Šï¼Œä½¿ç”¨è¿™ç§ç­–ç•¥ï¼Œä¸æ–­åœ°è®­ç»ƒä¼šä½¿å¾—$\alpha$ä¸€ç›´å¢å¤§ï¼ŒåŒæ—¶ä¸ºäº†æ§åˆ¶$\alpha$çš„å€¼ï¼Œå¯ä»¥å¥—ä¸€å±‚sigmoidå‡½æ•°ï¼Œä½¿å¾—$\alpha$æ§åˆ¶åœ¨0-1ä¹‹é—´ã€‚$åˆå§‹åŒ–\alpha=0ï¼Œ\alpha=tf.math.sigmoid(\alpha)$, 



### åŠ æƒç­–ç•¥

â€‹		å¯¹$loss_{u1} å’Œ loss_{u2}$ åŠ æƒçš„**ç›®çš„**ï¼šä½¿å¾—æŸå¤±è¾ƒå¤§çš„ä¸€æ–¹åœ¨æ•´ä¸ªlossä¸­çš„è´¡çŒ®æ›´å¤§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå€¾å‘äºè®­ç»ƒæŸå¤±æ›´å¤§çš„ä¸€æ–¹ã€‚

â€‹		å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœä¸¤ä¸ªç¥ç»ç½‘ç»œä¹‹é—´æ²¡æœ‰è”ç³»ï¼Œå³ $loss_{u1}(x_1;\theta_1) , loss_{u2}(x_2;\theta_2) $çš„è‡ªå˜é‡$(x_1,\theta_1),(x_2,\theta_2)$ä¹‹é—´æ²¡æœ‰é‡åˆçš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆå¯¹$loss_{u1} å’Œ loss_{u2}$ åŠ æƒå®é™…ä¸Šæ˜¯æ²¡æœ‰"æ•ˆæœ"çš„ã€‚

â€‹		åŸå› æ˜¯ï¼Œå¦‚æœä¸¤ä¸ªç¥ç»ç½‘ç»œä¹‹é—´æ²¡æœ‰è”ç³»æ—¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯¹ $Loss = \alpha_1 * loss_{u1} + \alpha_2 * loss_{u2}$æ±‚å…³äº$\theta_{1}$çš„å¯¼æ•°ï¼Œ$\frac{\partial loss}{\partial \theta_1} =\alpha_1 *  \frac{\partial loss_{u1}}{\partial \theta_1}$ï¼Œå¯ä»¥å‘ç°ä¸$\theta_2$æ— å…³ï¼Œå³è·Ÿç¬¬äºŒä¸ªç¥ç»ç½‘ç»œæ— å…³ï¼Œåªæ˜¯åœ¨è®­ç»ƒå•ä¸ªç¥ç»ç½‘ç»œè€Œå·²ï¼Œè€Œå¯¹å•ä¸ªç¥ç»ç½‘ç»œçš„lossä¹˜ä»¥ä¸€ä¸ªæ•°ï¼Œå®é™…æ˜¯æ²¡æœ‰ç”¨çš„ï¼Œç›¸å½“äºå¯¹ä¼˜åŒ–é—®é¢˜ä¸­ç›®æ ‡å‡½æ•°ä¹˜ä¸Šä¸€ä¸ªå¸¸æ•°ï¼Œæ˜¾ç„¶ä¸å½±å“æˆ‘ä»¬å¯»æ‰¾æœ€ä¼˜è§£ã€‚

â€‹		å› æ­¤ï¼Œæ­¤å¤„çš„ $loss_{u1},loss_{u2}$å…·ä½“ä¸ºï¼š
$$
loss_{u1} := loss_{u1}^{bc} + loss_{u1}^{f} + loss_{u1}^{i} \\
loss_{u2} := loss_{u2}^{bc} + loss_{u2}^{f} + loss_{u2}^{i} \\
$$
å…¶ä¸­$loss_{u1}^{i}$æ˜¯u1åœ¨äº¤ç•Œå¤„Interface çš„æŸå¤±å‡½æ•°ï¼Œä¸u1,u2æœ‰å…³ï¼Œå³ä¸$\theta_{1},\theta_{2}$æœ‰å…³ï¼Œå®ƒä½¿å¾—ä¸¤ä¸ªç¥ç»ç½‘ç»œè”ç³»åœ¨ä¸€èµ·ã€‚å½“$loss_{u1}^{i}$æƒé‡æ›´å¤§æ—¶ï¼Œæ¨¡å‹é€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°ä¸¤ä¸ªç½‘ç»œçš„å‚æ•°$\theta_{1},\theta_{2}$,ä¼šæ›´å€¾å‘äºä½¿å¾—$loss_{u1}$æ›´å°ã€‚



åœ¨å®é™…æ“ä½œä¸­ï¼Œæ— è®ºæ˜¯å•åŒºåŸŸçš„PINNå’Œè€¦åˆçš„PINNï¼Œåœ¨è¾¹ç•Œå¤„çš„æ‹Ÿåˆæ•ˆæœç›¸è¾ƒäºå†…éƒ¨çš„æ‹Ÿåˆæ•ˆæœæ›´å·®ã€‚

å¾€å¾€åœ¨$loss_u^{bc}$å‰ä¹˜ä¸Šä¸€ä¸ªå¸¸æ•°kï¼Œæ¯”å¦‚k=10ï¼š
$$
Loss = \alpha_1 * loss_{u1} + \alpha_2 * loss_{u2} \\
loss_{u1} := 10* loss_{u1}^{bc} + loss_{u1}^{f} + loss_{u1}^{i} \\
loss_{u2} := 10* loss_{u2}^{bc} + loss_{u2}^{f} + loss_{u2}^{i} \\
$$

### å¤šç§è‡ªé€‚åº”åŠ æƒç­–ç•¥

é™¤äº†é‚£ç§è‡ªé€‚åº”æƒé‡ Self-Adaptive-Weightä¹‹å¤–ï¼Œæˆ‘è¿˜æ„é€ äº†ä¸€ç§æ–°çš„ã€‚

ä»¤: 
$$
Loss^{(k)} = loss_{u1}^{(k)} + \alpha^{(k)} loss_{u2}^{(k)} ,\alpha^{(0)} = 1 \\

 \alpha^{(k+1)} = \frac{loss_{u2}^{(k)} }{loss_{u1}^{(k)} + eps}, å…¶ä¸­ epsæ˜¯ä¸€ä¸ªå¾ˆå°çš„æ­£æ•°ï¼Œé˜²æ­¢åˆ†æ¯ä¸º0\\
$$
æƒ³æ³•å°±æ˜¯ï¼Œå½“å‰æ­¥çš„æƒé‡$\alpha^{(k)}$æ˜¯æ ¹æ®ä¸Šä¸€æ­¥$loss_{u1}^{(k-1)},loss_{u2}^{(k-1)}$çš„æ¯”å€¼æ¥è°ƒæ•´,å½“$\alpha^{(k)}$>1æ—¶ï¼Œè¡¨ç¤ºä¸Šä¸€æ­¥loss_u2çš„å€¼æ›´å¤§ï¼Œæ•…åœ¨å½“å‰æ­¥ï¼Œè®©loss_u2ä¹˜ä¸Š$a^{(k)}>1$,ä½¿å¾—å½“å‰æ­¥loss_u2ä¸‹é™å¾—æ›´å¿«ã€‚



è¿™ç§â€è‡ªé€‚åº”â€œç­–ç•¥æœ‰å¾ˆå¤šï¼Œæƒ³æ€ä¹ˆæ„é€ å°±æ€ä¹ˆæ„é€ ï¼Œè¦æŠ“çš„ç‚¹å°±æ˜¯æ ¹æ®ä»¥å‰çš„lossï¼ŒåŠ¨æ€åœ°è°ƒæ•´æƒé‡ï¼Œä½¿å¾—å½“å‰æ­¥å€¾å‘è®­ç»ƒäºå‰Næ­¥ä¸­è¾ƒå¤§çš„ **å­loss** ã€‚



â€‹	

---

# 07-21

## ä¼˜åŒ–è®­ç»ƒæ­¥éª¤

1. é¢„è®­ç»ƒâ€”â€”å•åŒºåŸŸè®­ç»ƒ
2. è€¦åˆè®­ç»ƒâ€”â€”Adamç®—æ³•
3. è€¦åˆè®­ç»ƒâ€”â€”LBFGSç®—æ³•

**ä»£ç è§0721_è‡ªé€‚åº”&LBFGS_Parabolicè€¦åˆæ¨¡å‹.ipynb**<br />æ¯”è¾ƒAdamç®—æ³•å’ŒLBFGSç®—æ³•çš„è®­ç»ƒè¡¨ç°ã€‚<br />ï¼ˆæœ‰å¿…è¦æ·±å…¥äº†è§£Adamçš„æ€§è´¨ï¼Œåœ¨è®­ç»ƒåæœŸè¡¨ç°è¿œä¸å¦‚LBFGSï¼‰

# 07-22

## åŒºåŸŸåé—®é¢˜

â€‹	æµä½“åŠ›å­¦é¢†åŸŸè¿˜å­˜åœ¨å„ç§å„æ ·çš„åé—®é¢˜ï¼Œæ¯”å¦‚ç‰©ç†æ¨¡å‹çš„åˆè¾¹å€¼ æ¡ä»¶æ˜¯æœªçŸ¥çš„ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯å·²çŸ¥å†…éƒ¨éƒ¨åˆ†åŒºåŸŸæˆ–éƒ¨åˆ†ç‰©ç†é‡çš„æ•°å€¼çœŸè§£ï¼Œä»¥æ­¤ åæ¨æ•´ä¸ªåŒºåŸŸçš„æµä½“è¿åŠ¨æƒ…å†µï¼›æˆ–è€…ï¼Œç‰©ç†æ¨¡å‹çš„æ–¹ç¨‹æœ¬èº«å…·æœ‰ä¸€äº›æœªçŸ¥å‚æ•°ï¼Œ éœ€è¦é€šè¿‡çœŸå®çš„æ•°å€¼ç»“æœè¿›è¡Œåæ¨ã€‚è¿™ç±»é—®é¢˜åœ¨å·¥ç¨‹åº”ç”¨ä¸­å…·æœ‰å¾ˆå¤§æ„ä¹‰ï¼Œç„¶è€Œ å„ç§ä¼ ç»Ÿæ–¹æ³•å¯¹æ­¤ç±»é—®é¢˜çš„æ±‚è§£å…·æœ‰ä¸€å®šçš„éš¾åº¦ï¼Œåœ¨æœ¬æ–‡ç¥ç»ç½‘ç»œæ±‚è§£çš„æ¡†æ¶ ä¸‹ï¼Œå´å¾ˆå®¹æ˜“å¯¹è¯¥ç±»åé—®é¢˜å°è¯•è¿›è¡Œæ±‚è§£ã€‚

â€‹	ä½¿ç”¨ä¹‹å‰çš„Parabolic è€¦åˆPDEæ¨¡å‹è¿›è¡ŒåŒºåŸŸåé—®é¢˜çš„å®éªŒã€‚

å¯¹**åŒºåŸŸ1{(x,y)|0<=x<=1,0<=y<=1}**çš„åˆ’åˆ†ä¸ºï¼š

â€‹								 regions_x = [ [0.10,0.30],[0.40,0.60],[0.70,0.90] ]

â€‹								 regions_y = [ [0.10,0.30],[0.40,0.60],[0.70,0.90] ]

å¯¹**åŒºåŸŸ2{(x,y)|0<=x<=1,-1<=y<=0}**çš„åˆ’åˆ†ä¸ºï¼š

â€‹									regions_x = [ [0.10,0.30],[0.40,0.60],[0.70,0.90] ]

â€‹									regions_y = [ [-0.10,-0.30],[-0.40,-0.60],[-0.70,-0.90] ]

3 * 3 = 9ï¼Œæ¯ä¸ªåŒºåŸŸè¢«åˆ†ä¸º9ä¸ªå­åŒºåŸŸ

**å°†åŸæ¥è¾¹ç•Œå¤„çš„X_u_train,u_trainæ›¿æ¢ä¼šè¿™äº›å­åŒºåŸŸå†…çš„ç‚¹å’Œå¯¹åº”çš„è§£ã€‚**

> åŒºåŸŸçš„åˆ’åˆ†æœ‰è®²ç©¶ï¼Œè‹¥åŒºåŸŸè¿‡äºå°æˆ–è€…è¦†ç›–ç‡ä½ç­‰ï¼Œå¯èƒ½å¯¼è‡´æ•´ä¸ªåŒºåŸŸå†…æ‹Ÿåˆæ•ˆæœå’ŒçœŸè§£å·®è·å¤§ï¼Œè§£å¯èƒ½ä¸å”¯ä¸€ã€‚

ä»£ç è§ **0722_åŒºåŸŸåé—®é¢˜ Parabolicè€¦åˆæ¨¡å‹.ipynb**

---

# 07-23 

## å‚æ•°åé—®é¢˜

â€‹	å‚æ•°åé—®é¢˜æ˜¯æŒ‡å·²çŸ¥éƒ¨åˆ†ã€ä¹ƒè‡³å…¨éƒ¨çœŸè§£ï¼Œåæ¨æ¨¡å‹çš„å‚æ•°ï¼Œä»¥ä¼ ç»Ÿæ–¹æ³•æ¥è¯´ï¼Œè¿™æ˜¯å¾ˆå›°éš¾çš„ï¼Œä½†åœ¨PINNæ¡†æ¶ä¸‹ï¼Œåªéœ€è¦å°†æ¨¡å‹å‚æ•°è®¾ä¸ºå˜é‡ï¼Œå¸¦å…¥çœŸè§£è®­ç»ƒæ¨¡å‹ï¼ˆåŒæ—¶è®­ç»ƒå‚æ•°ï¼‰ï¼Œå¯ä»¥åæ¨å‚æ•°ã€‚

ä»£ç è§ **0723_å‚æ•°åé—®é¢˜_Parabolicè€¦åˆæ¨¡å‹.ipynb**

---

# 07-26 

## 3d ç®—ä¾‹& n-d ç®—ä¾‹

> *ä¹‹å‰æ‰€æœ‰çš„ä»£ç éƒ½æ˜¯åŸºäºå¦‚ä¸‹çš„æ¨¡å‹å’Œç®—ä¾‹*ï¼Œå¯ä»¥çœ‹åˆ°æ˜¯2dçš„ï¼Œå³u(x,y,t)ã€‚

### **parabolic è€¦åˆPDEæ¨¡å‹**

 In this work, a simplified model of diffusion through two adjacent materials which are coupled across their shared and rigid interface $I$ through a jump condition is considered. This problem captures some of the time-stepping difficulties of the ocean-atmosphere problem described in 1.2. The domain consists of two subdomains $\Omega_{1}$ and $\Omega_{2}$ coupled across an interface $I$ (example in Figure $1.1$ below). The problem is: given $\nu_{i}>0, f_{i}:[0, T] \rightarrow H^{1}\left(\Omega_{i}\right), u_{i}(0) \in$ $H^{1}\left(\Omega_{i}\right)$ and $\kappa \in \mathbb{R}$, find (for $\left.i=1,2\right) u_{i}: \bar{\Omega}_{i} \times[0, T] \rightarrow \mathbb{R}$ satisfying
$$
\begin{aligned}
u_{i, t}-\nu_{i} \Delta u_{i} &=f_{i}, \quad \text { in } \Omega_{i}, &(1.1)\\
-\nu_{i} \nabla u_{i} \cdot \hat{n}_{i} &=\kappa\left(u_{i}-u_{j}\right), \quad \text { on } I, \quad i, j=1,2, i \neq j, &(1.2)\\
u_{i}(x, 0) &=u_{i}^{0}(x), \quad \text { in } \Omega_{i}, &(1.3)\\
u_{i} &=g_{i}, \quad \text { on } \Gamma_{i}=\partial \Omega_{i} \backslash I . &(1.4)
\end{aligned}
$$
### **2dç®—ä¾‹**

Assume $\Omega_{1}=[0,1] \times[0,1]$ and $\Omega_{2}=[0,1] \times[-1,0]$, so $I$ is the portion of the $x$-axis from 0 to 1 . Then $\mathbf{n}_{1}=[0,-1]^{T}$ and $\mathbf{n}_{2}=[0,1]^{T}$. For $a, \nu_{1}, \nu_{2}$, and $\kappa$ all arbitrary positive constants, the right hand side function $\mathbf{f}$ is chosen to ensure that
$$
\begin{aligned}
&u_{1}(t, x, y)=a x(1-x)(1-y) e^{-t} \\
&u_{2}(t, x, y)=a x(1-x)\left(c_{1}+c_{2} y+c_{3} y^{2}\right) e^{-t} .
\end{aligned}
$$

The constants $c_{1}, c_{2}, c_{3}$ are determined from the interface conditions (1.2) and the boundary conditions for $u_{2}$. One may verify that with the following choices for $c_{1}, c_{2}, c_{3}, u_{1}$ and $u_{2}$ will satisfy (1.1)-(1.4) with $g_{1}=g_{2}=0$, i. e. when $x \in\{0,1\}$ or $y \in\{-1,1\}$ :
$$
c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1} .
$$
The numerical analysis performed in Section 4 indicates that by choosing $\kappa$ to be no larger than $\nu_{1}, \nu_{2}$ the IMEX scheme should perform as well as the implicit scheme. Computational results comparing the performance of the two methods are listed for two test problems:
- Test Problem 1: $a=\nu_{1}=\nu_{2}=\kappa=1$.



 å®é™…ä¸Š `u1` å’Œ`u2`çš„æ„é€ æ˜¯åŸºäºä»¤$u_{i} =g_{i} =0, \quad \text { on } \Gamma_{i}=\partial \Omega_{i} \backslash I$, ä½ ä¼šå‘ç°åœ¨è¾¹ç•Œä¸Šu1 = 0 ã€‚

å†æ ¹æ®(1.2)å’Œ(1.4)æ¨å‡ºu2ä¸­å‚æ•°éœ€è¦æ»¡è¶³çš„å…³ç³»å¼ã€‚æœ€ååœ¨æ ¹æ®(1.1)æ±‚å‡ºf1å’Œf2ã€‚ç¡®å®šå‚æ•°çš„å€¼ï¼Œæœ€ç»ˆæ„æˆä¸€ä¸ªå®Œæ•´çš„ç®—ä¾‹ã€‚

> ä¸‹é¢æˆ‘å°†ç®—ä¾‹æ‹“å±•åˆ°3dï¼Œå®é™…ä¸Šå¯ä»¥æ¨å¹¿åˆ° n-dã€‚

### 3dç®—ä¾‹

Assume $\Omega_{1}=[0,1] \times[0,1] \times[0,1]$ and $\Omega_{2}=[0,1] \times[0,1]\times[-1,0]$, so $I$ is the plain of z=0 , the $x$-axis from 0 to 1 ,the $y$-axis from 0 to 1.Namely.  $I$ = $[0,1] \times[0,1] \times\{0\}$ . 

Then $\mathbf{n}_{1}=[0,0,-1]^{T}$ and $\mathbf{n}_{2}=[0,0,1]^{T}$. 

For $a, \nu_{1}, \nu_{2}$, and $\kappa$ all arbitrary positive constants, the right hand side function $\mathbf{f}$ is chosen to ensure that
$$
\begin{aligned}
&u_{1}(t, x, y, z)=a xy(1-x)(1-y)(1-z) e^{-t} \\
&u_{2}(t, x, y, z)=a xy(1-x)(1-y)\left(c_{1}+c_{2} z+c_{3} z^{2}\right) e^{-t} .
\end{aligned}
$$

The constants $c_{1}, c_{2}, c_{3}$ are determined from the interface conditions (1.2) and the boundary conditions for $u_{2}$. One may verify that with the following choices for $c_{1}, c_{2}, c_{3}, u_{1}$ and $u_{2}$ will satisfy (1.1)-(1.4) with $g_{1}=g_{2}=0$, i. e. when $(x,y,z) \in\{x=1,0\leq y \leq 1,0\leq z \leq 1\}, u_1(x,y,z,t) = 0$ :
$$
c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1}.
$$
The numerical analysis performed in Section 4 indicates that by choosing $\kappa$ to be no larger than $\nu_{1}, \nu_{2}$ the IMEX scheme should perform as well as the implicit scheme. Computational results comparing the performance of the two methods are listed for two test problems:

- Test Problem 1: $a=\nu_{1}=\nu_{2}=\kappa=1$.

### n-d ç®—ä¾‹

$\Omega_{1}=\underbrace{[0,1] \times... \times[0,1] }_{n-1}\times[0,1]$,$\Omega_{2}=\underbrace{[0,1] \times... \times[0,1] }_{n-1}\times[-1,0]$,  $I$ = $\underbrace{[0,1] \times... \times[0,1] }_{n-1}\times\{0\}$ .

Then $\mathbf{n}_{1}=\underbrace{[0,...,0}_{n-1},-1]^{T}$ and $\mathbf{n}_{2}=\underbrace{[0,...,0}_{n-1},1]^{T}$. 

For $a, \nu_{1}, \nu_{2}$, and $\kappa$ all arbitrary positive constants, the right hand side function $\mathbf{f}$ is chosen to ensure that
$$
\begin{aligned}&u_{1}(t, x_1, x_2, ...,x_n)=ae^{-t} \prod \limits_{i=1}^n
x_i(1-x_i)  \\
&u_{2}(t, x_1, x_2, ...,x_n)=ae^{-t} \left(c_{1}+c_{2} x_n+c_{3} x_n^{2}\right)\prod \limits_{i=1}^{n-1}
x_i(1-x_i) .
\end{aligned}
$$
The constants $c_{1}, c_{2}, c_{3}$ are determined from the interface conditions (1.2) and the boundary conditions for $u_{2}$. One may verify that with the following choices for $c_{1}, c_{2}, c_{3}, u_{1}$ and $u_{2}$ will satisfy (1.1)-(1.4) with $g_{1}=g_{2}=0$:
$$
c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1} .c_{1}=1+\frac{\nu_{1}}{\kappa}, c_{2}=\frac{-\nu_{1}}{\nu_{2}}, c_{3}=c_{2}-c_{1} .
$$

---

## DeepXDEåº“&TensorDiffEqåº“

DeepXDE&TensorDiffEqæ˜¯ç°æœ‰çš„PINNæ±‚è§£å™¨ã€‚

- [DeepXDE(å®˜æ–¹æ–‡æ¡£)](https://deepxde.readthedocs.io/en/latest/)ï¼Œå¸ƒæœ—å¤§å­¦ Lu åšå£«å¼€å‘çš„ï¼Œå°±æ˜¯ DeepONet é‚£ä½ Lu åšå£«ã€‚ä»–ä»¬ç»„æ˜¯æœ¬æ¬¡ PINN æ½®æµçš„å…ˆé©±ï¼Œåº”è¯¥ç®—æ˜¯ç¬¬ä¸€æ¬¾ä¹Ÿæ˜¯â€œå®˜æ–¹â€çš„ PINN æ±‚è§£å™¨ã€‚é›†æˆäº†åŸºäºæ®‹å·®çš„è‡ªé€‚åº”ç»†åŒ–ï¼ˆRARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åœ¨è®­ç»ƒé˜¶æ®µä¼˜åŒ–æ®‹å·®ç‚¹åˆ†å¸ƒçš„ç­–ç•¥ï¼Œå³åœ¨åå¾®åˆ†æ–¹ç¨‹æ®‹å·®è¾ƒå¤§çš„ä½ç½®æ·»åŠ æ›´å¤šç‚¹ã€‚è¿˜æ”¯æŒåŸºäºæ„é€ å®ä½“å‡ ä½• ï¼ˆCSGï¼‰ æŠ€æœ¯çš„å¤æ‚å‡ ä½•åŒºåŸŸå®šä¹‰ã€‚

> [DeepXDEè®ºæ–‡](../è®ºæ–‡èµ„æ–™/DeepXDE- A Deep Learning Library for Solving Differential Equations.pdf)

- [TensorDiffEq(å®˜æ–¹æ–‡æ¡£)](https://docs.tensordiffeq.io/)çœ‹åå­—å°±çŸ¥é“æ˜¯åŸºäº Tensorflowï¼Œç‰¹ç‚¹æ˜¯åšåˆ†å¸ƒå¼è®¡ç®—ã€‚ä¸»æ—¨æ˜¯é€šè¿‡å¯ä¼¸ç¼©ï¼ˆscalableï¼‰è®¡ç®—æ¡†æ¶æ¥æ±‚è§£ PINNï¼Œæ˜æ˜¾æ˜¯ä¸ºå¤§è§„æ¨¡å·¥ä¸šåº”ç”¨åšé“ºå«ã€‚

---

# 07-27 

## 3D-parabolicä»£ç ç¼–å†™

â€‹	åœ¨2dåŸºç¡€ä¸Šæ–°å¢ä¸€ä¸ªç»´åº¦å³å¯ï¼Œå³åœ¨ç¥ç»ç½‘ç»œçš„è¾“å…¥Inputå±‚æ–°å¢ä¸€ä¸ªç»´åº¦andå¢åŠ å¯¹zçš„åå¯¼ã€‚æ³¨æ„è®­ç»ƒæ•°æ®çš„ç”Ÿæˆä»¥åŠå›¾åƒç”Ÿæˆéœ€è¦ç•¥å¾®æ”¹åŠ¨ã€‚

â€‹	å®éªŒæ•ˆæœè§ **0727_3D_Parabolicè€¦åˆæ¨¡å‹.ipynb**

> å› ä¸ºå¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œè®­ç»ƒçš„éš¾åº¦æœ‰æ‰€ä¸Šå‡ï¼Œ
>
> - å¢åŠ ä¸€äº› hidden layers ä»¥åŠ hidden sizeã€‚
> - å¢åŠ åŒºåŸŸå†…é‡‡æ ·ç‚¹N_fçš„æ•°é‡ï¼Œå’Œåˆè¾¹å€¼æ¡ä»¶çš„è®­ç»ƒç‚¹ã€‚

