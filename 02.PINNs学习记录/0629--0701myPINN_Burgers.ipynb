{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc0a382",
   "metadata": {},
   "source": [
    "# 6-29 记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ba80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(layer):\n",
    "    if len(layer) < 2:\n",
    "        print(\"层数小于2！, 无法构建神经网络\")\n",
    "        return \n",
    "    model = keras.Sequential(name = \"PINN\")\n",
    "    model.add(keras.Input(shape=(layer[0],)))\n",
    "    for i in range(1,len(layer)-1):\n",
    "        model.add(layers.Dense(layer[i], activation=\"relu\", name=\"layer{}\".format(i)))\n",
    "    model.add(layers.Dense(layer[-1], name=\"outputs\"))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  测试\n",
    "layer = [2,4,4,3]\n",
    "model = createModel(layer)\n",
    "inputs = tf.random.normal([1,2])\n",
    "print(model(inputs))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ed250",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def myloss(y_predict,y_true):\n",
    "    return tf.reduce_sum(tf.square(y_predict-y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(),  # Optimizer\n",
    "    loss = myloss\n",
    "    )\n",
    "model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([10,2])\n",
    "y = tf.random.normal([10,3])\n",
    "model.fit(x,y,batch_size=2,epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307f646",
   "metadata": {},
   "source": [
    "## 后续要为model 添加 loss、metric、optimizer\n",
    "\n",
    "\n",
    "model.compile(\n",
    "\n",
    "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "    \n",
    "    # Loss function to minimize\n",
    "    \n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # List of metrics to monitor\n",
    "    \n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    \n",
    ")\n",
    "\n",
    "> 以上内容截止至 6-29 markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca7eda",
   "metadata": {},
   "source": [
    "# 6-30 记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9558e",
   "metadata": {},
   "source": [
    "train_step()是fit()将会调用的核心函数，\n",
    "\n",
    "而train_step()将会使用 我们之前调用compile(optimizer,loss,metrics)时传入的优化器、损失函数、指标。\n",
    "\n",
    "实际上compile()做的事情就是将这些部件加入模型，方便train_step()使用。\n",
    "\n",
    "故，当我们想要自定义loss、更精准地控制训练过程，同时又想要保留keras.Model或keras.Sequential的方便方法。\n",
    "\n",
    "我们可以自定义一个子类，继承keras.Model或keras.Sequential。\n",
    "\n",
    "由于PINN的复杂性，需要对预测值求导再组成loss函数，仅仅修改train_step()是不够的，这意味着需要重载fit()，实际上就是抛弃已有的fit()框架，自己定义训练循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPinn(keras.Sequential): ## 正在编写\n",
    "    def __init__(self,name = None):\n",
    "        \n",
    "        super(MyPinn, self).__init__(name=name)\n",
    "        self.nu = tf.constant(0.01/np.pi)\n",
    "    \n",
    "    @tf.function\n",
    "    def test_gradient(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)\n",
    "        u_x = tape.gradient(u,x)\n",
    "        tf.print(u_x)\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_U(self,X_u_train,u_train):\n",
    "        u_pred = self(X_u_train)\n",
    "        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))\n",
    "        return loss_u\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def loss_PDE(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)  \n",
    "            u_x = tape.gradient(u,x)         \n",
    "            \n",
    "        u_t = tape.gradient(u, t)     \n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx\n",
    "\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "\n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss_Total(self,X_u_train,u_train,X_f_train):\n",
    "        loss_u = self.loss_U(X_u_train,u_train)\n",
    "        loss_f = self.loss_PDE(X_f_train)\n",
    "        \n",
    "        loss_total = loss_u + loss_f\n",
    "        \n",
    "        return loss_total\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self,X_u_train,u_train,X_f_train):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)\n",
    "                   \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss_total, trainable_vars)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        return loss_total\n",
    "    \n",
    "    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):\n",
    "        for epoch in tf.range(1,epochs+1):\n",
    "            loss_total = self.train_step(X_u_train,u_train,X_f_train)\n",
    "            if epoch % 10 == 0:                \n",
    "                print(\n",
    "                    \"Training loss (for per 10 epoches) at epoch %d: %.4f\"\n",
    "                    % (epoch, float(loss_total))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(layer,Model):\n",
    "    if len(layer) < 2:\n",
    "        print(\"层数小于2！, 无法构建神经网络\")\n",
    "        return \n",
    "    model = Model(name = \"PINN\")\n",
    "    model.add(keras.Input(shape=(layer[0],)))\n",
    "    for i in range(1,len(layer)-1):\n",
    "        model.add(layers.Dense(layer[i], activation=\"relu\", name=\"layer{}\".format(i)))\n",
    "    model.add(layers.Dense(layer[-1], name=\"outputs\"))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train = tf.random.normal([1000,2])\n",
    "u_train = tf.random.normal([1000,1])\n",
    "X_f_train = tf.random.normal([1000,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [2,20,20,20,20,1]\n",
    "m1= createModel(layer,MyPinn)\n",
    "m1.compile(keras.optimizers.SGD(learning_rate=0.1))\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.train_model(X_u_train,u_train,X_f_train,epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试PINN (请无视)\n",
    "class MyPINN(keras.Sequential):\n",
    "    \n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x)\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss_u = tf.reduce_mean(tf.square(y-y_pred)) \n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            # loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            loss = loss_u\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736790d4",
   "metadata": {},
   "source": [
    "> 以上内容截止至 6-30 markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2e39f",
   "metadata": {},
   "source": [
    "# 7-1 记录\n",
    "\n",
    "### Adam优化器 & tfp中L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372f043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcf17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(layer,Model):\n",
    "    if len(layer) < 2:\n",
    "        print(\"层数小于2！, 无法构建神经网络\")\n",
    "        return \n",
    "    model = Model(name = \"PINN\")\n",
    "    model.add(keras.Input(shape=(layer[0],)))\n",
    "    for i in range(1,len(layer)-1):\n",
    "        model.add(layers.Dense(layer[i], activation=\"relu\", name=\"layer{}\".format(i)))\n",
    "    model.add(layers.Dense(layer[-1], name=\"outputs\"))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaaf232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPinn(keras.Sequential): ## 正在编写\n",
    "    def __init__(self,name = None):\n",
    "        \n",
    "        super(MyPinn, self).__init__(name=name)\n",
    "        self.nu = tf.constant(0.01/np.pi)\n",
    "    \n",
    "    @tf.function\n",
    "    def test_gradient(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)\n",
    "        u_x = tape.gradient(u,x)\n",
    "        tf.print(u_x)\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_U(self,X_u_train,u_train):\n",
    "        u_pred = self(X_u_train)\n",
    "        loss_u = tf.reduce_mean(tf.square(u_train - u_pred))\n",
    "        return loss_u\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def loss_PDE(self,X_f_train):\n",
    "        x = X_f_train[:,0]\n",
    "        t = X_f_train[:,1]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([x,t])\n",
    "            X = tf.stack([x,t],axis=-1)\n",
    "            u = self(X)  \n",
    "            u_x = tape.gradient(u,x)         \n",
    "            \n",
    "        u_t = tape.gradient(u, t)     \n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        f = u_t + (self(X_f_train))*(u_x) - (self.nu)*u_xx\n",
    "\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "\n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss_Total(self,X_u_train,u_train,X_f_train):\n",
    "        loss_u = self.loss_U(X_u_train,u_train)\n",
    "        loss_f = self.loss_PDE(X_f_train)\n",
    "        \n",
    "        loss_total = loss_u + loss_f\n",
    "        \n",
    "        return loss_total\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self,X_u_train,u_train,X_f_train):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss_total = self.loss_Total(X_u_train,u_train,X_f_train)\n",
    "                   \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss_total, trainable_vars)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        return loss_total\n",
    "    \n",
    "    def train_model(self, X_u_train,u_train,X_f_train, epochs=100):\n",
    "        for epoch in tf.range(1,epochs+1):\n",
    "            loss_total = self.train_step(X_u_train,u_train,X_f_train)\n",
    "            if epoch % 10 == 0:                \n",
    "                print(\n",
    "                    \"Training loss (for per 10 epoches) at epoch %d: %.4f\"\n",
    "                    % (epoch, float(loss_total))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5449d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train = tf.random.normal([1000,2])\n",
    "u_train = tf.random.normal([1000,1])\n",
    "X_f_train = tf.random.normal([1000,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6675b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Training loss (for per 10 epoches) at epoch 10: 1.1222\n",
      "Training loss (for per 10 epoches) at epoch 20: 1.0785\n",
      "Training loss (for per 10 epoches) at epoch 30: 1.0606\n",
      "Training loss (for per 10 epoches) at epoch 40: 1.0523\n",
      "Training loss (for per 10 epoches) at epoch 50: 1.0478\n",
      "Training loss (for per 10 epoches) at epoch 60: 1.0448\n",
      "Training loss (for per 10 epoches) at epoch 70: 1.0428\n",
      "Training loss (for per 10 epoches) at epoch 80: 1.0416\n",
      "Training loss (for per 10 epoches) at epoch 90: 1.0406\n",
      "Training loss (for per 10 epoches) at epoch 100: 1.0400\n"
     ]
    }
   ],
   "source": [
    "layer = [2,20,20,1]\n",
    "m1= createModel(layer,MyPinn)\n",
    "m1.compile(keras.optimizers.Adam())\n",
    "m1.train_model(X_u_train,u_train,X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2a94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_factory(model, loss, X_u_train,u_train,X_f_train):\n",
    "    \"\"\"A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    Args:\n",
    "        model [in]: an instance of `tf.keras.Model` or its subclasses.\n",
    "        loss [in]: a loss function in model\n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(model_parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "    # prepare required information first\n",
    "    count = 0\n",
    "    idx = [] # stitch indices\n",
    "    part = [] # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = np.product(shape)\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "\n",
    "    part = tf.constant(part)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # now create a function that will be returned by this factory\n",
    "    @tf.function\n",
    "    def f(params_1d):\n",
    "        \"\"\"A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "        This function is created by function_factory.\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "\n",
    "        # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            # update the parameters in the model\n",
    "            assign_new_model_parameters(params_1d)\n",
    "            # calculate the loss\n",
    "            loss_value = loss(X_u_train,u_train,X_f_train)\n",
    "\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "        # print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "        tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        # store loss value so we can retrieve later\n",
    "        tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "        return loss_value, grads\n",
    "\n",
    "    # store these information as members so we can use them outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_model_parameters = assign_new_model_parameters\n",
    "    f.history = []\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a721e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = function_factory(m1, m1.loss_Total, X_u_train,u_train,X_f_train)\n",
    "init_params = tf.dynamic_stitch(func.idx, m1.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0d7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 loss: 1.03994167\n",
      "Iter: 2 loss: 1.03919315\n",
      "Iter: 3 loss: 1.03913665\n",
      "Iter: 4 loss: 1.03857\n",
      "Iter: 5 loss: 1.03837156\n",
      "Iter: 6 loss: 1.03797746\n",
      "Iter: 7 loss: 1.03769338\n",
      "Iter: 8 loss: 1.03730607\n",
      "Iter: 9 loss: 1.03728044\n",
      "Iter: 10 loss: 1.0368849\n",
      "Iter: 11 loss: 1.03789437\n",
      "Iter: 12 loss: 1.03664911\n",
      "Iter: 13 loss: 1.03618467\n",
      "Iter: 14 loss: 1.03612936\n",
      "Iter: 15 loss: 1.03595841\n"
     ]
    }
   ],
   "source": [
    "results = tfp.optimizer.lbfgs_minimize(\n",
    "    value_and_gradients_function=func, initial_position=init_params, max_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65aaf03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0359584>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.loss_Total(X_u_train,u_train,X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33256101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
